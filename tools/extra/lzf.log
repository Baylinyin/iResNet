I0814 08:07:35.470998 28599 upgrade_proto.cpp:1044] Attempting to upgrade input file specified using deprecated 'solver_type' field (enum)': ../model_lzf_znet/solver_zoom_itr1.prototxt
I0814 08:07:35.486666 28599 upgrade_proto.cpp:1051] Successfully upgraded file specified using deprecated 'solver_type' field (enum) to 'type' field (string).
W0814 08:07:35.486676 28599 upgrade_proto.cpp:1053] Note that future Caffe releases will only support 'type' field (string) for a solver's type.
I0814 08:07:35.486744 28599 caffe.cpp:185] Using GPUs 0
I0814 08:07:35.542075 28599 caffe.cpp:190] GPU 0: GeForce GTX TITAN X
I0814 08:07:36.295966 28599 solver.cpp:48] Initializing solver from parameters: 
test_iter: 4334
test_interval: 5500
base_lr: 0.0001
display: 50
max_iter: 1200000
lr_policy: "multistep"
gamma: 0.5
momentum: 0.9
weight_decay: 0.0004
snapshot: 5500
snapshot_prefix: "norm_-0.01"
solver_mode: GPU
device_id: 0
net: "train_zoom_itr1.prototxt"
delta: 0.0001
stepvalue: 120000
stepvalue: 150000
stepvalue: 300000
stepvalue: 500000
stepvalue: 800000
momentum2: 0.999
type: "Adam"
I0814 08:07:36.296099 28599 solver.cpp:91] Creating training net from net file: train_zoom_itr1.prototxt
I0814 08:07:36.354771 28599 net.cpp:318] The NetState phase (0) differed from the phase (1) specified by a rule in layer CustomData2
I0814 08:07:36.354800 28599 net.cpp:318] The NetState phase (0) differed from the phase (1) specified by a rule in layer DummyData2
I0814 08:07:36.354807 28599 net.cpp:318] The NetState phase (0) differed from the phase (1) specified by a rule in layer ref_disp_scale
I0814 08:07:36.354812 28599 net.cpp:318] The NetState phase (0) differed from the phase (1) specified by a rule in layer ref_disp_scale_half
I0814 08:07:36.355129 28599 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "CustomData1"
  type: "CustomData"
  top: "blob0"
  top: "blob1"
  top: "blob2"
  include {
    phase: TRAIN
  }
  data_param {
    source: "/media/leo/game/dispflownet-release/data/FlyingThings3D_release_TRAIN_clean_lmdb"
    batch_size: 4
    backend: LMDB
    rand_permute: true
    rand_permute_seed: 77
    slice_point: 3
    slice_point: 6
    encoding: UINT8
    encoding: UINT8
    encoding: UINT16FLOW
    verbose: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "blob0"
  top: "blob3"
  eltwise_param {
    operation: SUM
    coeff: 0.0039215689
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "blob1"
  top: "blob4"
  eltwise_param {
    operation: SUM
    coeff: 0.0039215689
  }
}
layer {
  name: "img0s_aug"
  type: "DataAugmentation"
  bottom: "blob3"
  top: "img0_aug"
  top: "blob6"
  propagate_down: false
  augmentation_param {
    max_multiplier: 1
    augment_during_test: false
    recompute_mean: 1000
    mean_per_pixel: false
    translate {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0
      spread: 0.4
      prob: 1
    }
    zoom {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: 0.2
      spread: 0.4
      prob: 1
    }
    squeeze {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: 0
      spread: 0.3
      prob: 1
    }
    lmult_pow {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: -0.2
      spread: 0.4
      prob: 1
    }
    lmult_mult {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    lmult_add {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0
      spread: 0.03
      prob: 1
    }
    sat_pow {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    sat_mult {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: -0.3
      spread: 0.5
      prob: 1
    }
    sat_add {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0
      spread: 0.03
      prob: 1
    }
    col_pow {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    col_mult {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.2
      prob: 1
    }
    col_add {
      rand_type: "gaussian_bernoulli"
      exp: false
      mean: 0
      spread: 0.02
      prob: 1
    }
    ladd_pow {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    ladd_mult {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    ladd_add {
      rand_type: "gaussian_bernoulli"
      exp: false
      mean: 0
      spread: 0.04
      prob: 1
    }
    col_rotate {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0
      spread: 1
      prob: 1
    }
    crop_width: 768
    crop_height: 384
    chromatic_eigvec: 0.51
    chromatic_eigvec: 0.56
    chromatic_eigvec: 0.65
    chromatic_eigvec: 0.79
    chromatic_eigvec: 0.01
    chromatic_eigvec: -0.62
    chromatic_eigvec: 0.35
    chromatic_eigvec: -0.83
    chromatic_eigvec: 0.44
    noise {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0.03
      spread: 0.03
      prob: 1
    }
  }
}
layer {
  name: "aug_params1"
  type: "GenerateAugmentationParameters"
  bottom: "blob6"
  bottom: "blob3"
  bottom: "img0_aug"
  top: "blob7"
  coeff_schedule_param {
    half_life: 50000
    initial_coeff: 0.5
    final_coeff: 1
  }
  augmentation_param {
    augment_during_test: false
    gamma {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.01
      prob: 1
    }
    brightness {
      rand_type: "gaussian_bernoulli"
      exp: false
      mean: 0
      spread: 0.01
      prob: 1
    }
    contrast {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.01
      prob: 1
    }
    color {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.01
      prob: 1
    }
  }
}
layer {
  name: "img1s_aug"
  type: "DataAugmentation"
  bottom: "blob4"
  bottom: "blob7"
  top: "img1_aug"
  propagate_down: false
  propagate_down: false
  augmentation_param {
    max_multiplier: 1
    augment_during_test: false
    recompute_mean: 1000
    mean_per_pixel: false
    crop_width: 768
    crop_height: 384
    chromatic_eigvec: 0.51
    chromatic_eigvec: 0.56
    chromatic_eigvec: 0.65
    chromatic_eigvec: 0.79
    chromatic_eigvec: 0.01
    chromatic_eigvec: -0.62
    chromatic_eigvec: 0.35
    chromatic_eigvec: -0.83
    chromatic_eigvec: 0.44
  }
}
layer {
  name: "DummyData1"
  type: "DummyData"
  top: "blob9"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
      value: 0
    }
    num: 4
    channels: 1
    height: 540
    width: 960
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "blob2"
  bottom: "blob9"
  top: "blob10"
  propagate_down: false
  propagate_down: false
  concat_param {
    concat_dim: 1
  }
}
layer {
  name: "FlowAugmentation1"
  type: "FlowAugmentation"
  bottom: "blob10"
  bottom: "blob6"
  bottom: "blob7"
  top: "blob11"
  propagate_down: false
  propagate_down: false
  propagate_down: false
  augmentation_param {
    crop_width: 768
    crop_height: 384
  }
}
layer {
  name: "Slice1"
  type: "Slice"
  bottom: "blob11"
  top: "disp_gt_aug"
  top: "blob13"
  propagate_down: false
  slice_param {
    slice_point: 1
    axis: 1
  }
}
layer {
  name: "ref_disp_scale"
  type: "DummyData"
  top: "ref_disp_scale"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
      value: 0
    }
    num: 1
    channels: 4
    height: 96
    width: 192
  }
}
layer {
  name: "ref_disp_scale_half"
  type: "DummyData"
  top: "ref_disp_scale_half"
  include {
    phase: TRAIN
  }
  dummy_data_param {
    data_filler {
      type: "constant"
      value: 0
    }
    num: 1
    channels: 4
    height: 192
    width: 384
  }
}
layer {
  name: "disp_gt_aug_norm"
  type: "Eltwise"
  bottom: "disp_gt_aug"
  top: "disp_gt_aug_norm"
  eltwise_param {
    operation: SUM
    coeff: -0.1
  }
}
layer {
  name: "disp_gt_aug_norm_down"
  type: "Downsample"
  bottom: "disp_gt_aug_norm"
  bottom: "ref_disp_scale"
  top: "disp_gt_aug_norm_down"
  propagate_down: false
  propagate_down: false
}
layer {
  name: "disp_gt_aug_norm_down_half"
  type: "Downsample"
  bottom: "disp_gt_aug_norm"
  bottom: "ref_disp_scale_half"
  top: "disp_gt_aug_norm_down_half"
  propagate_down: false
  propagate_down: false
}
layer {
  name: "upsample_disparity_itr1_half"
  type: "Resample"
  bottom: "disp_gt_aug_norm_down_half"
  top: "upsampled_disparity_itr1_half"
  propagate_down: false
  resample_param {
    width: 768
    height: 384
    type: LINEAR
    antialias: true
  }
}
layer {
  name: "Silence1"
  type: "Silence"
  bottom: "blob0"
}
layer {
  name: "Silence2"
  type: "Silence"
  bottom: "blob1"
}
layer {
  name: "Silence3"
  type: "Silence"
  bottom: "blob2"
}
layer {
  name: "Silence4"
  type: "Silence"
  bottom: "blob9"
}
layer {
  name: "Silence5"
  type: "Silence"
  bottom: "blob13"
}
layer {
  name: "Silence6"
  type: "Silence"
  bottom: "img1_aug"
}
layer {
  name: "upsample_disparity_itr1"
  type: "Resample"
  bottom: "disp_gt_aug_norm_down"
  top: "upsampled_disparity_itr1"
  propagate_down: false
  resample_param {
    width: 768
    height: 384
    type: LINEAR
    antialias: true
  }
}
layer {
  name: "upsample_disparity2x_itr1"
  type: "Resample"
  bottom: "disp_gt_aug_norm_down"
  top: "upsampled_disparity2x_itr1"
  propagate_down: false
  resample_param {
    width: 384
    height: 192
    type: LINEAR
    antialias: true
  }
}
layer {
  name: "concat_input_itr1"
  type: "Concat"
  bottom: "img0_aug"
  bottom: "upsampled_disparity_itr1"
  top: "zoom_input_itr1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "zoom_conv0_itr1"
  type: "Convolution"
  bottom: "zoom_input_itr1"
  top: "zoom_conv0_itr1"
  param {
    name: "zoom_conv0_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv0_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU0_itr1"
  type: "ReLU"
  bottom: "zoom_conv0_itr1"
  top: "zoom_conv0_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_conv1_itr1"
  type: "Convolution"
  bottom: "zoom_conv0_itr1"
  top: "zoom_conv1_itr1"
  param {
    name: "zoom_conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv1_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU1_itr1"
  type: "ReLU"
  bottom: "zoom_conv1_itr1"
  top: "zoom_conv1_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_conv1b_itr1"
  type: "Convolution"
  bottom: "zoom_conv1_itr1"
  top: "zoom_conv1b_itr1"
  param {
    name: "zoom_conv1b_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv1b_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU1b_itr1"
  type: "ReLU"
  bottom: "zoom_conv1b_itr1"
  top: "zoom_conv1b_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_conv2_itr1"
  type: "Convolution"
  bottom: "zoom_conv1b_itr1"
  top: "zoom_conv2_itr1"
  param {
    name: "zoom_conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv2_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU2_itr1"
  type: "ReLU"
  bottom: "zoom_conv2_itr1"
  top: "zoom_conv2_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_conv2b_itr1"
  type: "Convolution"
  bottom: "zoom_conv2_itr1"
  top: "zoom_conv2b_itr1"
  param {
    name: "zoom_conv2b_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv2b_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU2b_itr1"
  type: "ReLU"
  bottom: "zoom_conv2b_itr1"
  top: "zoom_conv2b_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_deconv2_itr1"
  type: "Deconvolution"
  bottom: "zoom_conv2b_itr1"
  top: "zoom_deconv2_itr1"
  param {
    name: "zoom_deconv2_w"
    lr_mult: 1
    decay_mult: 0
  }
  param {
    name: "zoom_deconv2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 4
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU0p_itr1"
  type: "ReLU"
  bottom: "zoom_deconv2_itr1"
  top: "zoom_deconv2_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_Concat2_itr1"
  type: "Concat"
  bottom: "zoom_conv1b_itr1"
  bottom: "zoom_deconv2_itr1"
  bottom: "upsampled_disparity2x_itr1"
  top: "zoom_concat2_itr1"
}
layer {
  name: "zoom_fused1_itr1"
  type: "Convolution"
  bottom: "zoom_concat2_itr1"
  top: "zoom_fused1_itr1"
  param {
    name: "zoom_fused1_w"
    lr_mult: 1
    decay_mult: 0
  }
  param {
    name: "zoom_fused1_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_deconv1_itr1"
  type: "Deconvolution"
  bottom: "zoom_fused1_itr1"
  top: "zoom_deconv1_itr1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 4
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU1p_itr1"
  type: "ReLU"
  bottom: "zoom_deconv1_itr1"
  top: "zoom_deconv1_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_Concat1_itr1"
  type: "Concat"
  bottom: "zoom_conv0_itr1"
  bottom: "zoom_deconv1_itr1"
  bottom: "upsampled_disparity_itr1_half"
  top: "zoom_concat1_itr1"
}
layer {
  name: "zoom_fused0_itr1"
  type: "Convolution"
  bottom: "zoom_concat1_itr1"
  top: "zoom_fused0_itr1"
  param {
    name: "zoom_fused0_w"
    lr_mult: 1
    decay_mult: 0
  }
  param {
    name: "zoom_fused0_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_Convolution0_itr1"
  type: "Convolution"
  bottom: "zoom_fused0_itr1"
  top: "zoom_predict0_itr1"
  param {
    name: "zoom_predict0_w"
    lr_mult: 1
    decay_mult: 0
  }
  param {
    name: "zoom_predict0_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU_predict_itr1"
  type: "ReLU"
  bottom: "zoom_predict0_itr1"
  top: "zoom_predict0n_itr1"
}
layer {
  name: "zoom_disp_loss0"
  type: "L1Loss"
  bottom: "zoom_predict0n_itr1"
  bottom: "disp_gt_aug_norm"
  top: "zoom_disp_loss0"
  loss_weight: 1
  l1_loss_param {
    l2_per_location: false
    normalize_by_num_entries: true
  }
}
layer {
  name: "down_up_loss"
  type: "L1Loss"
  bottom: "upsampled_disparity_itr1"
  bottom: "disp_gt_aug_norm"
  top: "down_up_loss"
  loss_weight: 0
  propagate_down: false
  propagate_down: false
  l1_loss_param {
    l2_per_location: false
    normalize_by_num_entries: true
  }
}
layer {
  name: "down_up_half_loss"
  type: "L1Loss"
  bottom: "upsampled_disparity_itr1_half"
  bottom: "disp_gt_aug_norm"
  top: "down_up_half_loss"
  loss_weight: 0
  propagate_down: false
  propagate_down: false
  l1_loss_param {
    l2_per_location: false
    normalize_by_num_entries: true
  }
}
layer {
  name: "to_real_disp_loss"
  type: "Eltwise"
  bottom: "zoom_disp_loss0"
  top: "zoom_disp_loss0_real"
  propagate_down: false
  eltwise_param {
    operation: SUM
    coeff: 1
  }
}
layer {
  name: "to_real_downup_loss"
  type: "Eltwise"
  bottom: "down_up_loss"
  top: "down_up_loss_real"
  propagate_down: false
  eltwise_param {
    operation: SUM
    coeff: 1
  }
}
layer {
  name: "to_real_downup_loss"
  type: "Eltwise"
  bottom: "down_up_half_loss"
  top: "down_up_half_loss_real"
  propagate_down: false
  eltwise_param {
    operation: SUM
    coeff: 1
  }
}
I0814 08:07:36.355468 28599 layer_factory.hpp:77] Creating layer CustomData1
I0814 08:07:36.355484 28599 net.cpp:91] Creating Layer CustomData1
I0814 08:07:36.355490 28599 net.cpp:404] CustomData1 -> blob0
I0814 08:07:36.355509 28599 net.cpp:404] CustomData1 -> blob1
I0814 08:07:36.355518 28599 net.cpp:404] CustomData1 -> blob2
I0814 08:07:36.356041 28599 custom_data_layer.cpp:364] LMDB: Cleaned 0 stale readers.
I0814 08:07:36.356068 28599 custom_data_layer.cpp:373] Opening lmdb /media/leo/game/dispflownet-release/data/FlyingThings3D_release_TRAIN_clean_lmdb
I0814 08:07:36.356072 28599 custom_data_layer.cpp:379] DB entries: 22000
I0814 08:07:36.356189 28599 custom_data_layer.cpp:437] Data range: 0 to 21999 = 22000 entries.
Permutation:
13189 12718 11299 1121 8073 8624 21114 15836 14486 10970 3977 8380 6254 15357 14925 1617 15613 1671 17336 11292 11636 9813 21485 16134 10842 10067 3169 18261 8536 15314 5175 17410 19641 2600 13084 20502 10422 2839 6587 9858 20045 7418 13787 8442 20077 2366 710 18094 16118 1781 9865 5314 9812 3897 21141 6389 5009 2573 20664 11029 15785 12274 9030 12513 14076 12984 15224 19338 17674 3326 5689 9570 15750 10499 14452 8502 4988 19020 2828 3174 4676 13320 7698 6279 14750 21911 936 14045 11893 2435 79 4882 9530 16876 8905 9554 14127 15975 2096 1021 5931 21102 15046 16993 13722 1722 12329 21967 18145 8826 11406 10125 15126 2937 2837 13974 9281 17694 19335 7185 8621 5726 899 15817 13575 19238 8892 19876 760 17575 20133 21522 14624 8688 12055 11169 5905 16959 3538 5942 20011 19560 4945 14019 13050 7333 4057 12658 2784 1801 16344 2135 16223 1891 4507 2849 15452 7619 6788 5890 1436 2424 16717 15359 17239 2575 9655 8304 1064 16075 18894 10856 20528 11793 13416 1150 5431 7361 19140 5129 19069 13867 19042 17938 18285 1914 12951 19232 591 17828 19305 19410 5245 21256 13448 11733 13009 14561 3042 5774 14885 5401 18203 14502 19966 19495 5306 13637 2718 3450 573 16053 18327 19360 7870 13346 9502 20611 7477 17440 17964 15889 19932 10902 1985 18242 21138 8233 15997 7452 10320 3202 2970 11805 20302 21859 4765 21917 5723 12559 5737 14183 6938 13770 3073 3366 19062 16885 12776 13405 8967 16685 14460 11231 15803 13441 17512 11950 10522 17686 734 9767 1496 10225 2756 17409 2952 18419 14616 7428 12235 12661 8975 1417 9092 17354 19840 13930 4749 6213 13769 2064 19845 9549 18453 537 15164 4487 18110 2638 12554 10642 18878 6595 9631 10030 17167 17929 13290 16679 1545 9996 19669 13203 17549 14621 20626 6194 10908 18223 1106 3774 13303 19375 14060 4155 5319 2425 19074 11254 21470 13374 21033 10203 1697 10363 2321 6457 15647 6633 11660 10444 18103 9241 224 11828 9578 18208 13072 12842 3068 10507 10953 21336 19421 4462 8894 7883 18912 678 1339 5620 17906 13366 14471 1614 15019 18813 20105 19988 4772 2004 14806 3874 13097 19120 1742 12856 15304 13255 21818 18808 21603 10792 15590 12565 21577 11226 20538 4843 19105 17655 17671 9054 19284 12230 5864 15209 19671 12680 21845 18761 7808 438 5494 15883 13927 16832 7636 356 20240 6295 16902 11964 9606 1783 13540 8682 21250 11670 9218 1479 11996 9180 4921 7759 7246 15878 690 19595 2236 6142 10643 8940 18074 14704 15177 16854 5672 1332 8723 15143 5301 2616 12902 8976 11407 20800 14365 16752 17988 15586 12241 2362 2053 1721 14242 6494 15884 5696 1138 12965 6818 14954 13451 5264 21640 20860 10616 12710 2748 16628 16090 5239 14418 11911 17372 12514 15074 15752 9319 7055 11752 16701 682 21510 14605 6715 7354 4571 16148 19031 18091 14067 1531 2709 13917 20493 12720 8250 15715 16539 21748 12217 21108 6940 21902 11928 6618 13700 10179 3149 5867 18970 10737 11005 11337 18720 14562 12291 254 15680 18973 7086 19021 21040 17438 9522 19303 16237 21014 868 11410 10738 2415 6749 15720 7518 10158 6468 1883 5198 4070 19436 17421 9697 11611 130 16596 2456 4712 12179 19373 17971 10317 19461 14569 4629 14505 13935 529 8695 15640 12818 18178 12878 7090 5105 10962 8724 8261 12612 18881 325 11432 12505 287 8528 9909 14632 11993 3115 2197 12543 12290 16766 4142 8747 157 6410 5056 11557 6356 9177 11808 8817 15892 17332 2464 9664 15900 5667 5309 13614 11555 1126 8960 20176 8594 972 7160 20578 14805 10487 1580 19416 3018 17048 5296 15486 14002 17360 7431 9892 16283 1434 5622 11976 8302 1917 3828 9008 5116 11255 20732 10419 3480 4616 9711 20016 5559 21419 7042 4316 20650 17092 8207 8803 18272 5535 18139 5783 21376 20509 19488 17302 8331 21853 1907 18180 21865 4293 16097 3231 1861 18339 14241 6120 101 10708 10751 11773 3105 11948 11286 5114 6723 8271 11880 12336 21169 20552 20042 8694 13504 1405 8088 7006 12827 19546 15441 19205 12224 119 11173 7288 12810 5556 11061 4479 2862 12017 4674 10350 16545 4563 17579 2513 20914 9250 21841 7807 3360 5946 238 17080 12697 21442 14961 3541 12068 778 740 20893 13409 6732 17306 13238 14336 17781 10820 18798 8141 2411 8588 12099 12748 10535 13089 447 14056 18538 11649 11044 4532 2708 12077 18331 10430 17824 20836 2015 1593 14121 19881 7384 20490 18510 20177 12601 17839 14399 3216 6340 18429 16830 5450 5382 16655 18491 1450 17487 13630 19322 1650 15769 14578 352 14385 21678 15116 6269 2830 6637 19813 8372 413 7391 6883 7137 1110 14112 21385 2476 2982 4143 6320 4703 4783 9071 21594 8835 12461 2121 19705 6275 18705 21606 1626 20932 14221 3947 7972 5808 8504 17902 1262 5939 20383 11289 13385 5070 11959 16332 13856 17735 14273 11451 10614 10744 9140 2816 11707 12305 8496 818 7023 18233 10835 12267 10531 20540 3019 14715 21934 2517 7813 18962 2705 6117 8674 5858 17918 16494 8637 2808 10366 19848 20943 9022 19442 6318 20251 6482 14378 13131 139 16982 4084 10075 8559 2493 15171 9046 20361 15119 3428 19534 14261 16442 4879 9680 6227 21426 6939 19910 7792 2523 3417 21761 17185 6016 18404 12195 9066 4649 7569 208 20851 4623 12057 2558 1362 20769 12979 13983 6082 14001 18206 7573 10767 12889 17912 12155 1194 14678 4204 15287 16968 5340 16477 12500 6684 18582 19578 21073 19041 20263 188 634 7831 3724 7001 14928 14608 21930 2814 18853 18968 2122 16700 3283 20275 3786 7437 18163 21199 7100 8792 20571 3760 4550 17065 7497 10189 15842 15029 5332 8309 18852 3015 14913 15811 19206 18790 10983 11104 15382 5569 18668 12664 5213 16253 11149 429 14674 3422 627 21212 12688 15982 5713 5369 18698 5244 10889 2107 4864 8400 13692 11453 1621 16234 8237 10477 13418 236 13311 19773 12619 14026 3351 5722 1776 6496 16073 7123 1406 19058 4384 10146 8484 10413 6744 15678 11495 21747 8490 13458 15179 3111 20583 19356 14498 5465 7544 15421 901 536 1364 6741 11369 14648 16424 13058 1371 6349 6383 21900 19076 4689 3718 5013 5758 11602 10334 2695 4515 14521 16869 11712 15679 20923 12887 12207 14033 7351 8589 16371 11297 6375 225 13609 859 11819 8880 11316 11579 17765 16470 17739 10411 21416 5982 252 19389 16353 11560 7837 14122 17471 19890 3726 11642 18167 9878 3327 5435 2547 14240 6790 7080 20525 19957 705 759 18221 4929 4910 3156 19243 18036 20365 17329 2167 3966 5780 5271 11751 6343 3854 8963 8573 18048 21620 12738 18717 10640 20181 2213 11375 16372 7879 2404 16259 10161 8719 154 8667 8687 21960 19175 1923 9424 1930 1529 3573 3961 13302 3233 8219 21185 19605 12635 16064 13592 14669 8403 16571 9794 6150 6290 8926 4152 3853 12349 5132 13923 20600 19693 2975 14312 4512 10947 12125 3139 2093 15896 8301 15241 498 17213 4154 19225 6289 2059 15470 18307 20261 6717 2256 21922 7221 21096 6958 4045 12782 17667 394 7818 2900 21991 17247 15219 17762 5983 199 21512 11634 13143 20658 4564 213 14946 9026 19529 6477 10205 1467 13925 12873 13709 9122 3873 11831 1978 19222 10052 15472 20644 17936 17901 10828 5259 5700 16812 15601 12681 18748 9521 8920 8853 19383 1901 2448 20233 5384 7799 19295 17366 1202 10808 18778 21417 15503 19471 1209 14236 512 15728 5664 13949 19324 12767 19154 3031 4834 9907 14787 3732 4388 6407 21862 21932 17424 17618 7133 11787 14701 7641 1498 1947 8213 14692 9325 13834 8518 10778 4401 15985 13129 12415 21631 18106 16690 12314 11955 13631 14835 12164 19923 949 18238 19814 19072 5180 2182 17030 14809 6151 11206 8222 7232 3647 9977 16359 15334 7282 8971 1225 10513 9574 15342 5439 4716 7103 17939 3900 14442 12826 6110 3446 8914 6576 11604 5960 15225 17996 4939 7828 1595 14292 15588 11387 9732 2355 10866 21770 2519 9023 3695 19203 8855 15629 6437 19475 18589 10802 18264 16594 18341 996 10396 8031 3193 18010 7000 8661 2793 17648 12714 6857 18824 1784 14682 13188 15047 3001 20501 9243 6076 18436 16230 18692 1163 18105 827 11236 14587 18794 8591 14611 18137 4517 5378 16339 21276 10296 12904 7555 21161 12754 20064 17022 11662 6124 14951 20495 8012 3383 3932 15442 427 14397 19883 8918 21897 19769 15397 15736 3809 7144 3296 12800 8678 6626 21236 16043 21106 2889 19358 15392 1060 20849 3308 6889 18188 12353 14179 11981 13929 7678 5208 20336 10753 19497 8434 17643 15552 18987 1324 1579 4366 10433 9087 977 2840 15910 15390 14941 5816 16365 7606 11544 552 12024 13271 5076 17107 13420 14057 19995 15154 17196 1168 8411 18078 10636 8805 2682 11910 12617 15593 9805 8090 1274 307 8706 2275 568 1040 18532 6460 732 7173 6984 16205 6802 16226 20790 1646 3834 12128 9309 10641 16418 10627 4347 12824 1898 4617 13305 17191 17596 19404 18869 5199 11564 5006 8587 14393 841 16310 11103 12796 5472 7215 4023 16962 7350 5442 9458 6247 4451 226 8664 12744 3675 3638 12597 16626 15764 6779 21816 88 21240 16575 17454 17071 8615 4527 2753 20835 5897 4408 12893 1031 21400 4187 17873 4789 21375 19345 18637 5283 13193 6841 13288 9690 14232 19121 5235 8140 17775 11202 8775 17877 15622 4361 17847 7302 15425 5373 13617 8966 5860 4986 20824 18445 13496 10910 8631 14398 2769 4640 15988 8293 5601 18864 10238 10971 15182 6276 15062 16461 2934 11208 4454 1185 5611 19196 21691 4787 2155 932 10328 17786 17177 5815 17627 7744 132 10806 4477 19129 7317 2295 13388 16301 6322 20727 15794 16641 4450 4014 8542 2324 14883 3814 8909 1935 10080 3388 14219 15080 21633 15479 18980 13597 9226 11408 14497 9884 18320 21495 9271 912 8361 16916 16303 16560 3348 638 17351 5610 6167 19330 1773 3820 20831 20554 19763 3178 2333 16918 9 18234 875 939 2164 15600 14748 1082 17385 9016 9133 8382 1267 9870 3904 18593 17602 3980 20248 13551 15410 13356 8666 12572 6171 19941 5859 11195 10850 16159 3158 12030 9410 12616 8991 16840 402 2865 21001 16721 7534 18337 2302 6003 19450 6232 21716 15123 13357 17060 14153 6811 2332 21725 20274 8119 6328 11298 21585 18923 7596 11967 10917 620 16245 14119 8159 11821 3841 13121 12670 7131 18737 3848 21769 16989 12341 18150 16969 19859 21086 9700 3415 21343 11937 2351 18014 8734 4672 20527 3736 10050 6085 10972 4296 12048 4953 6409 16086 15466 14347 9482 10160 11865 13978 21892 19429 1380 9617 15986 15234 10417 4267 8463 865 1409 12460 9192 8700 18457 10871 12168 13651 1905 14007 2140 2556 17250 20592 20050 3758 21291 13866 7955 5747 7087 10250 1642 3934 9070 21987 3453 16145 3958 10926 3350 19277 7233 11210 16414 21596 8464 19130 12733 2870 570 7908 4064 12376 2490 1869 3379 19195 193 7856 15594 12088 16676 6310 20335 626 21402 2886 4253 18020 18972 17476 2577 17049 1696 18628 5637 11603 9729 7249 16015 6116 5624 50 17361 11978 4116 10618 8650 4937 13571 4397 18695 16889 3467 21935 15583 19340 19347 17425 513 7229 12429 12716 10954 11203 15035 21261 142 4359 9740 14472 14660 20924 18460 8493 9921 10103 6927 16010 14316 313 10248 392 17344 20772 4549 18929 4685 19977 14810 9959 1748 18576 4514 18001 5555 3412 10471 790 11280 2533 10230 4263 11287 9132 21198 8193 15558 15527 11069 13716 5173 6070 19262 10975 13112 2035 16175 9821 7992 8929 15946 17106 11373 6819 9923 10986 21420 12604 18730 8819 8402 19819 21811 10948 17310 11892 1687 21767 7568 2578 11665 7716 19485 6999 14921 18825 6346 12154 18342 14094 387 12568 14287 12839 15926 11736 13734 11561 6398 10176 9079 19598 8950 13491 11362 19528 1143 21351 5413 13532 19261 20574 20624 6009 13498 13897 13220 7436 8514 6613 3843 19040 4572 14902 16841 17962 9486 9315 12864 12407 21020 7659 12886 4327 19289 8342 15455 10725 577 8779 12915 25 5608 14144 1077 15406 12575 11987 14468 20287 6528 17669 6704 3817 6620 16126 20662 10615 3997 17217 7184 10743 4661 1087 6689 8569 3398 11266 18739 7165 17161 16377 14981 18529 8608 20048 161 7830 11033 4984 18240 21762 10670 16917 21889 7021 20749 14419 17166 20433 9927 4867 11975 12334 9879 461 8646 8143 19012 14705 6514 14924 11182 1851 11011 14999 10383 1775 12403 7868 3769 3059 9163 4524 11814 12536 376 12874 4994 16157 2303 17559 10800 15475 18100 1310 14825 9804 8114 3507 2147 6539 21730 6394 10482 14180 20326 12645 13174 14829 2626 8036 19742 13914 542 13299 19849 14952 381 4003 15833 21110 20785 8906 3010 6192 9737 3103 15921 6219 20257 7125 9372 1782 1470 3486 111 21847 3759 8038 2021 20352 12892 9751 6444 10969 11305 14542 6144 15800 14997 4762 19000 10520 17365 18235 6221 10548 13674 2692 1659 15513 19569 3403 15660 10032 9796 4407 177 21100 10870 16315 12701 2018 12421 3981 10053 6127 4088 4244 20582 13191 13423 1123 125 13144 5751 5903 842 21142 11509 21908 11157 3289 10436 2491 12108 7188 7692 14512 1798 10924 4608 5354 17171 11192 21076 8858 9503 3007 861 17931 15501 15671 8633 19938 3122 21550 8795 9766 19656 16837 3645 13812 19149 19785 12715 13452 4175 17234 11264 745 2262 1586 13404 5670 2385 250 8338 12978 3200 3025 441 832 6666 16769 3207 19581 5916 17712 1598 19100 4677 6316 7439 3392 793 20992 10475 14314 6973 17704 19516 15254 8422 15659 10198 20487 13040 10464 7283 6414 791 4266 5554 14965 635 21191 16499 1336 5653 16995 5591 6593 17000 8189 7551 1988 13335 12400 18381 2515 16933 19176 11102 8139 14638 16174 1174 8236 10055 11473 11317 18917 3089 17371 15974 56 7663 7947 7819 6546 7306 9002 3831 18086 6325 3065 21563 20665 13735 3972 6152 2143 8610 2278 16051 20284 1563 6217 1765 14934 9322 7231 2619 13206 19135 20725 10655 4312 12740 7381 15560 11997 3585 11134 12431 10825 3967 2072 113 12076 109 20289 2358 5140 15979 1739 359 13519 9190 11262 19122 720 14877 970 3458 17736 10380 17358 20366 12548 9345 822 7667 16328 9409 10890 16031 6913 9367 21850 21186 3819 3261 2842 1916 3589 4534 4989 4095 1971 18230 19235 18985 10369 20969 5307 9170 5938 10887 448 21357 11933 2339 20889 7083 19670 8874 20918 9285 8737 8208 15157 10542 11138 15690 4087 5612 19363 14152 3307 4457 14532 650 18109 10712 4709 16886 20217 52 16202 5908 19146 15886 21078 7558 7862 3912 12009 11282 731 15612 20536 6438 15804 15648 12662 12840 6059 14509 6361 18893 7433 12633 14871 7404 12091 3132 9484 12113 20517 8446 14463 9208 12387 13183 16987 11471 6577 10843 1720 378 1245 8017 10445 7403 18146 19001 21820 185 9487 8531 1729 14636 12498 17907 15395 3487 12056 13514 4853 8174 8636 5957 6840 15131 16945 18248 17088 18525 5317 181 21480 6839 19183 19879 12606 8814 20839 19654 6443 9384 14047 1992 20044 13486 20141 4100 14812 18645 9886 7523 17740 7344 21095 18741 4132 13825 18953 3642 21505 4854 19240 7537 13200 21600 471 11838 687 20798 8844 8409 4252 21611 10071 10604 21077 10904 9868 18898 13521 21193 12908 9265 11505 7631 12749 2922 5915 76 17430 18412 1954 5519 11534 15625 2268 1427 20982 21204 11461 3924 16806 17621 2628 2193 14649 12270 10895 21757 6991 10768 10141 1826 20673 170 20020 9473 16321 13300 3080 16415 11458 1003 1059 4466 4983 10127 1001 3138 12656 6462 11757 14626 16149 16814 13751 17911 8707 5872 4306 4916 21264 13223 18265 4169 833 5551 7056 20271 12474 5913 11107 2323 17258 11622 10573 15934 13788 8815 8054 13808 3593 21435 4223 2012 9047 5491 20478 8098 13952 18499 13225 3254 12148 20481 15231 11701 586 21299 13634 3669 13484 3763 4972 18029 14973 10781 18027 13118 19378 16624 21273 18299 1638 6464 5069 3849 7068 1665 19911 21411 2640 5480 15476 12081 11597 17316 13869 5625 9701 20521 15473 21367 4111 10680 21719 1807 19212 10635 15000 8558 15547 6177 149 7960 6994 9989 13553 14586 19729 5212 14190 7694 693 13563 18102 4335 3499 11628 7272 15789 10110 18976 13459 6886 9704 1475 9703 12988 12947 13179 12396 17807 8458 19415 11452 746 15212 9037 21697 13559 489 10171 1925 2850 15525 18623 6828 15782 18832 14639 5371 7124 6500 21496 318 2746 15098 7029 18255 13281 18827 19039 13789 12618 15911 372 1643 13410 17889 1169 18787 14407 19047 18947 8748 13873 13229 9405 8753 13899 3237 4288 4991 1945 18837 13782 17097 21324 16590 753 17680 8321 15825 13001 4775 3930 8520 1288 17006 458 17200 9791 4183 1994 7644 12130 7614 2159 15861 10625 14282 17728 17267 12298 16369 17029 17626 17320 10113 14443 850 13342 12725 5560 16643 13826 8437 6557 9416 8969 11571 15608 16648 2166 5674 4163 14079 18115 12562 14808 925 1599 9783 21645 8557 19918 3539 9589 3530 8428 1437 13371 9692 2909 4038 16603 15439 3679 20397 10474 8897 15275 9793 4820 16215 7301 9069 16743 8910 14132 9869 9928 12541 15875 14338 20039 12593 12404 16820 9035 9803 20615 14826 12745 14210 1885 20961 3410 15541 6222 6670 1231 7793 3320 7290 21446 5023 5702 16914 17699 8158 2609 4437 2770 12354 15814 5933 14279 18021 1396 11965 3583 16882 19218 9948 3808 12075 13331 17810 3186 14857 13169 13642 14430 15941 679 7093 7925 3962 17830 1843 12654 11743 16200 19920 16336 17826 10404 702 8439 20477 19976 15336 11605 7467 8516 1926 464 4330 4509 14141 9975 20019 15687 21295 18527 18572 6825 12854 15285 16698 6545 19749 9364 12943 20290 1156 11786 3134 7164 11874 17192 18474 2691 492 5630 704 17536 13250 15780 19414 16530 358 1368 6591 5531 4825 16749 12819 19580 20620 10428 5883 16255 20750 4476 2735 14916 1098 15945 4261 20575 5710 19282 19321 16725 17156 18760 8020 17408 1132 10181 538 5549 6797 2536 21280 19946 10260 7634 19152 17966 19753 1160 7041 13919 6095 17986 21233 17151 16980 5695 15487 1416 21831 6240 12803 156 10371 20255 17075 4320 148 10093 8641 68 13339 20773 20541 485 7753 4919 4318 14004 7236 7511 20670 20305 10241 2145 2624 18336 18060 11830 11017 1264 16152 12769 17851 16709 14304 3280 2111 11059 1605 9388 973 7901 7322 9817 3599 14296 9356 13074 7579 19245 3118 7463 14149 15303 2469 18747 3154 7275 2570 15927 6850 19996 9745 6669 7514 8995 15518 6428 17184 18126 19454 17203 16114 12771 14485 21353 11974 16171 21913 18859 7814 15276 17840 11118 15027 20985 1506 2856 5238 11338 15688 2169 5154 18185 15203 6391 632 16838 4998 7836 583 9024 7944 12897 2194 10997 11212 2601 6654 8708 5839 17074 7747 5025 15905 17062 16651 18977 4159 9155 15280 2396 11348 7074 15505 10401 7758 2946 7460 2592 506 17374 7309 17182 2660 21964 9505 17697 521 1991 20770 8407 1919 15468 4893 15761 21661 8936 4121 938 11178 2668 4601 2760 5529 13456 4498 2201 259 11306 9966 13901 3106 17242 15003 14920 3069 7208 12809 7140 21011 16340 11341 1164 12248 4793 20692 17732 3029 21372 2868 17379 1669 4607 14845 624 6552 5693 1724 18396 11333 18156 1246 10015 15523 4724 3026 21277 14058 7182 9318 18937 9174 6974 16514 15637 20494 10714 13081 20150 11122 3810 18540 7843 16746 19870 5558 10735 8373 5680 12219 6366 20688 21513 9061 18547 16070 19170 2153 2887 9951 5299 8837 18452 20222 3235 8116 1881 6239 15125 13231 3728 5988 12277 13291 9199 13373 13282 17789 2441 781 750 22 21959 20910 14315 17727 13208 17076 18507 16754 17248 19199 2639 8684 11855 18913 4654 3778 15705 19401 6876 12124 11535 20438 6498 14017 8710 15295 14230 11051 14465 3757 5884 1397 7270 13654 18035 10306 17138 19119 15916 9525 10994 11423 15851 17812 12846 16970 10765 17760 2588 5073 17439 3858 21431 9210 14184 9702 12667 16777 5714 20726 1372 2389 16853 13017 10013 21704 12262 8701 6776 5900 20545 9681 4906 9341 5125 8840 15760 5881 20073 17624 16756 12558 18129 2317 10014 14870 16317 17950 17794 11906 18805 19613 9015 2440 9040 15746 6092 3429 16474 18575 19160 6837 12020 20212 11078 12639 18776 6043 2963 20544 1075 12515 10163 16456 14735 10845 16723 5671 13426 9148 3633 15388 18099 346 10732 2516 12169 21282 19073 10929 4667 21241 9246 11970 3461 19585 269 12708 6952 13268 17205 46 8154 17915 3253 8566 6234 3108 18495 6122 3496 4077 20272 9186 13718 9771 6858 5626 13242 4684 20621 20999 19359 18549 19198 3481 2374 10764 12894 14014 9398 19828 17208 2370 13660 6553 10995 2230 4886 13568 16965 16619 18118 21734 6112 6474 16161 17288 18390 6990 11881 19740 15207 11466 2807 12898 15028 21162 8462 11380 2284 19056 20950 584 11949 18232 14652 10677 478 8506 4053 20434 12969 7262 14258 2003 17748 5654 5576 18370 7147 19319 3243 21263 15063 2104 19649 2123 3619 4248 271 1756 6242 5054 12532 6159 19137 893 9727 12520 21576 361 13779 7981 12012 10763 4791 15137 2113 10325 12483 14247 19308 13545 17895 15121 4599 14178 15269 8182 8166 8221 8683 6280 6955 3265 14996 12909 15272 12062 13858 18408 7611 10810 20752 10461 18449 1924 19915 20003 19271 241 8419 19380 2505 957 8016 12786 21824 8517 10166 9544 21794 15758 18533 4756 13691 5831 9552 4780 8556 15282 21494 20928 1171 3643 3870 5474 5365 16952 21021 4842 11685 5058 3973 2294 11198 19508 7572 9369 15807 17348 2025 7082 1879 5809 6515 14188 21423 5391 19244 15707 2933 3899 19053 3285 8632 15738 9492 7458 14173 751 2223 9839 18440 8946 2485 8466 7902 12410 8253 7499 20768 8307 7101 21584 13116 13876 13921 8318 15330 4734 314 4174 6606 9183 21428 14911 9672 2761 9834 9253 1980 5833 19368 20456 6856 15401 1223 11963 14849 13980 17046 1882 12628 19774 15823 9476 2383 622 7348 10097 4566 15843 15490 5918 12210 18647 13175 8607 1153 4660 2190 13507 19044 888 4395 17388 11314 437 10609 21721 16672 11320 1305 3215 21074 15729 21676 3951 9465 14228 9310 14372 1306 3050 21893 15627 13864 6284 1162 9260 19651 3241 12360 20637 8130 8256 18999 14320 10437 21080 21251 6967 12528 6495 12875 510 10214 7702 445 15450 13595 14349 14205 12851 6682 6487 14071 3821 17702 21613 14329 5287 20166 11003 17679 3800 12923 39 19775 17450 11619 20454 8257 2422 14215 11748 404 20782 20818 12950 7411 10059 12063 19477 21708 16481 4909 14672 12253 13367 1036 795 17389 8647 6565 13460 21928 9564 656 881 1634 18907 16453 15904 6745 431 13831 12338 13462 17562 15538 6650 4031 10134 531 20239 18309 17190 19640 14390 5045 10492 839 14657 3359 2851 18615 9489 9961 7962 9527 12356 9165 5574 9298 9368 11067 13907 3475 19336 16720 20667 7376 15992 20408 9168 12065 13474 14104 1326 2943 12173 5367 15335 11888 2904 17601 10156 3358 8788 12023 6448 10818 12414 3012 3567 7915 2750 1893 1265 775 14011 21379 16657 7530 9478 7557 11985 2367 4927 2759 10797 194 83 4434 3569 94 12453 7618 21482 5172 18887 12919 12166 19185 12602 3884 20723 1908 5418 4813 13885 19634 843 17714 14030 13589 9831 9532 10044 7841 18011 12910 10718 20374 16851 20793 8420 21168 17100 9067 10692 10392 4219 14459 13194 5048 10435 21025 20436 3965 2363 4818 18316 388 16130 1740 557 10958 554 12991 15376 5814 9543 5191 7220 20604 8124 9425 15925 15199 11238 2594 12825 15345 20220 2580 20439 13249 9053 11114 14440 8769 17722 575 14584 2551 1063 12328 10170 1737 12995 3626 7347 18177 14890 16506 9076 12905 15864 5766 1325 11409 20766 4028 403 102 19855 10089 15025 16740 2428 7407 6919 2183 6195 17920 10333 16696 5375 12263 4125 20645 15820 10572 2863 6336 12820 5923 3302 4303 11343 7138 4516 5344 13960 15991 17798 8359 20814 9141 6989 140 1644 2090 12489 8822 11540 1639 370 15953 15426 70 18215 19050 18660 10440 19639 11042 11082 1028 3617 20106 16272 21986 11526 4956 6691 12256 8714 5266 18493 11807 9206 11405 5115 4400 12192 19991 16069 2095 14564 6260 14421 9534 386 2345 15591 16779 10848 13761 16197 6126 6709 5233 18004 4274 14513 19268 5159 10649 16208 9540 2541 16468 9730 20109 2156 10556 8485 18886 3061 17309 21466 14580 210 15371 21783 4009 12553 17163 769 7682 3519 15631 12659 12393 13477 7872 16108 11503 7712 19381 17855 17558 13986 3531 10456 18688 5286 13702 14601 4048 8316 58 935 14062 7199 3199 4958 19004 8538 17899 6377 3931 14832 11384 1743 19765 8964 18997 1734 16528 3064 17452 4933 2687 13883 477 9267 18384 3136 7656 10485 4299 9630 643 4165 1033 15106 8324 1490 13865 13364 16702 703 16102 6737 4033 11494 1764 1862 13940 16185 4580 3147 19884 19320 10210 16390 19934 14434 21469 7774 10083 15284 11155 12477 10720 15414 13470 4621 19564 12163 19405 21454 4352 15653 13309 6040 16000 11921 15398 3888 9357 17638 2951 12316 16238 16588 17825 10629 10726 10285 5342 12525 1451 9744 10066 5106 765 1066 4622 11045 13520 4605 499 1670 7890 4235 1959 21612 18049 19898 15071 1348 10416 16057 7189 16146 21727 671 14391 15698 21172 15695 5137 19003 3893 16842 10414 12493 21388 17241 2910 15511 1713 55 15381 10008 13671 16531 13912 13668 18246 1083 13693 6639 3096 18642 2733 3779 9790 7709 8580 14425 9142 5570 10415 589 4137 20876 19049 4585 19695 8838 8732 10400 4442 11442 17909 20744 11468 159 19189 8697 20603 6789 5020 17068 13161 17838 20280 13119 5241 19467 16873 1010 11979 12857 17174 21441 4917 21209 8334 8512 2409 3144 2688 16291 7210 14796 15965 118 3903 16708 10373 494 2043 16565 11737 7201 16467 11484 9029 17831 9433 8106 12531 8362 16761 17792 5303 13379 9344 3035 13059 4321 8665 9414 7766 20922 133 6905 4717 6202 8954 5147 2949 4807 9806 8698 3337 2823 6359 2884 8085 10391 11845 5039 21057 8042 16048 14828 9366 9932 20299 815 16421 19481 4105 16342 997 6237 1635 19413 15672 501 5420 18053 4040 7114 3000 21234 19952 10945 6785 8949 9881 15661 3224 3525 17871 9392 15813 18842 7486 20534 10619 14953 12394 11854 20140 13814 1065 2633 3533 853 21534 10459 2105 19862 13725 20127 11615 7653 1358 7823 3826 4719 7639 21863 4708 20651 10862 8223 15866 5542 8369 15655 11887 12357 3889 10009 12233 6854 6992 11899 16870 3510 8523 18239 7098 2054 3907 3892 16861 9080 8216 8729 3527 7603 16940 4981 145 17461 17942 17709 21511 1401 20115 1920 13906 10697 31 3847 8487 11914 2459 9833 10301 12378 15459 601 6014 8829 16440 1386 17522 19476 14791 15605 3352 4839 2477 17300 20058 1547 12907 19352 12157 14869 475 9596 18573 17567 10921 4410 11549 13564 4862 20063 2698 7687 5861 4403 8957 6220 8577 9011 19803 15628 19269 6154 4637 13523 3230 6942 866 6794 974 21705 5850 8709 8454 11945 1645 8417 4876 2195 14753 12159 3860 18691 18066 5196 6645 7495 15366 15485 18608 9184 4069 16012 2568 18249 4135 17984 11023 19449 11599 11111 761 9555 19839 19577 17129 945 14565 13210 19832 20007 9623 6649 11220 20356 8459 11230 21836 10011 7597 4242 14106 11170 6792 7106 10772 11585 21969 3659 1741 17404 17744 9337 2250 17690 4046 16550 6590 2582 5206 16081 19799 18518 4345 15332 8195 20964 11848 20685 11537 11368 7658 428 17485 4233 9479 1026 6869 6722 10956 9523 2058 8021 10974 18936 13164 13355 691 12170 13422 10554 10685 3640 8985 9779 15917 18335 6894 13056 4844 14130 4892 3631 5729 6911 3032 20114 15294 16745 10183 15874 3236 4715 10354 3078 11758 21146 3459 13321 13403 19676 14738 18563 12478 3218 1936 6426 2822 1841 6885 15247 17229 10243 11354 9263 5862 3653 10686 14072 14326 8860 11132 17903 12752 14560 13763 21041 5779 10048 9248 16516 3082 14567 21604 6146 9750 18745 9646 5817 5267 18362 6616 6412 17353 2068 8770 5592 8148 3717 15536 3421 4567 19342 17773 3086 7770 20868 12666 4115 19265 10367 1147 10074 12067 8889 7713 20559 15322 11551 11992 14968 12877 21158 8942 4301 21284 2272 3812 18334 5636 7120 9462 10191 19492 6384 1281 1097 1431 12226 21910 12136 4521 17721 19929 13257 2364 19686 8848 12 844 19748 4800 5613 17442 16845 4819 2273 8686 19158 16654 20370 11389 19482 3339 21782 1419 14198 21955 8843 9962 3813 18446 6907 4124 956 5181 20747 5711 17474 8934 14708 333 15676 19555 20369 18365 12518 12693 3815 17286 15093 519 17312 13695 2292 6838 8634 2706 3846 16617 6160 10533 16547 7545 658 8128 9678 768 16112 21147 1717 19587 1109 1412 12522 4467 8882 14976 3104 4226 7079 20215 6868 17216 12792 12944 12331 11826 16522 18297 5164 12545 21130 11639 7242 11547 3709 16946 14937 16729 6187 2098 11088 10253 12876 16326 16433 6455 12652 8781 18420 8146 1118 16578 4396 234 20038 9238 8168 7011 2971 985 1463 14150 20996 2712 10740 20942 8473 10698 19104 13800 21281 16589 3546 7379 4645 13136 18598 13837 6415 14568 21927 5285 14158 725 15877 10273 19396 21119 15696 1609 4951 3754 4427 8878 10122 19732 5037 15572 11823 5393 19108 18696 7325 2976 520 3654 10511 12419 18147 13604 17380 2137 15617 19018 8679 16686 17617 13258 5976 1404 10877 13510 20965 2847 18584 13616 16319 8390 13892 5187 14903 7945 13120 21731 5910 5419 7952 7250 5315 16142 20279 14151 3608 13570 9840 10029 4208 69 279 1960 17238 20709 10951 13450 2772 17641 14422 21415 19301 1846 16355 1633 3532 2613 8481 4458 10076 13500 2901 20986 15602 267 15 7298 697 18325 12560 18885 5414 927 12181 19837 3928 981 10525 12469 13673 3698 11775 18719 6451 5337 11515 15539 12655 7296 9802 16379 891 7307 10733 14118 1903 21242 290 2749 19951 4415 12880 6344 11346 9987 1366 476 17738 6497 7096 10356 1340 5609 14597 11923 7737 15132 3053 6916 3880 11159 20508 18775 5227 7755 4013 15919 16684 14234 14335 21164 3380 99 11609 10387 21326 9708 1271 21798 13534 979 4196 7948 16076 5345 15436 13262 6060 16640 19134 2625 2470 17983 13330 391 8188 12236 13620 235 15734 19868 17096 21562 6998 1255 776 15469 14677 10526 19128 1039 6644 1615 4638 484 10132 19734 15841 19029 4245 14129 6754 7838 20939 10123 20422 4530 4615 1557 17662 6049 16389 12698 21950 17954 7832 17896 21175 6373 20228 7846 21468 1797 6778 8290 20909 17843 1965 5661 15990 1904 13778 4421 4761 13629 2767 5237 11741 15689 13100 5052 14744 2853 14333 19452 16025 15067 13094 17276 16553 21954 2521 19339 13506 919 3768 13762 18229 15957 11897 18149 7507 15724 11356 4777 17968 21363 21427 4426 2045 15460 4740 82 3036 17529 6053 895 15220 19080 19168 21272 6987 9837 18166 20329 5527 8057 10652 2489 5928 15787 5034 21031 1032 5163 3123 5648 19157 2202 4354 20563 968 14464 13307 17221 14404 4900 19880 20000 13809 1549 15266 19444 3297 652 13114 2522 21531 1619 21225 1497 8246 7865 947 18924 16198 16824 9752 14538 12092 10747 12051 3167 4443 320 12938 4948 13731 4018 18015 8749 4494 16613 19781 6369 14249 21422 7320 14286 11768 2130 11196 3431 15461 200 20094 2099 14016 7241 7988 18639 18065 12284 5537 18740 162 766 13400 6432 7492 4180 20129 17916 12637 11645 10374 21035 12198 21311 17152 3083 17400 4182 12213 17520 18382 19738 582 2013 17314 12622 17345 17102 4488 8980 491 19426 8640 7152 11058 16480 2430 18795 12631 14147 8284 37 522 8996 15635 16649 12033 9792 16312 3637 15984 21112 13893 12773 8277 17644 6688 12442 3249 2078 17764 4098 7633 87 1108 1192 20591 6698 3242 2168 692 11310 18611 9336 11025 6345 1407 665 11024 3550 15801 18535 907 4668 1342 20609 8627 11713 13644 8039 14068 3255 17635 8696 21094 11131 13977 12199 9466 21047 8859 9999 13784 10811 8703 11467 17197 15870 21377 21390 1847 19252 20295 4720 16307 1081 17956 10583 13567 16599 17813 17426 9124 16878 1079 11565 699 10779 1280 2701 3040 16021 10120 4760 680 9144 13819 4340 5764 9352 8861 19747 6872 7359 3468 14789 5209 9613 9050 13214 11153 8047 5150 6920 19562 13992 20898 8612 9463 1448 5868 18521 19667 20131 9284 15338 4827 15368 20753 19800 5790 580 12985 4773 20938 9242 14707 19357 19970 15767 20214 4840 14504 20774 10590 6933 5488 7252 5316 8200 2261 5297 1961 9665 21195 11922 1184 18075 21771 12933 1041 17237 7773 20853 1408 2916 15253 17282 2047 14553 9235 10664 2906 15128 2306 15929 18559 10331 11378 9720 7805 6600 3954 6612 13318 16839 19190 16002 15311 9379 13797 19713 7622 20388 12932 21449 21985 6864 10907 13211 967 16459 2434 17034 2744 16782 15245 15170 3716 21171 835 3989 5769 11225 10124 16718 353 10130 6291 1105 11302 14526 14525 7535 10865 3020 17869 17291 15173 11694 14751 11753 9429 1867 11567 18680 20947 20027 16403 4946 1519 13031 3941 11012 17664 3391 21553 5618 15210 19594 6812 13269 15307 2685 21268 12903 7266 11095 20533 18629 4781 8005 5473 5324 1689 19064 13587 4650 2812 13726 11989 16135 1166 7672 2931 16586 942 5782 11459 20613 14222 19681 6863 13202 21418 7146 14466 13370 8095 13438 10821 20979 7999 20162 11243 17821 9828 9699 11073 18846 11508 8186 1950 3338 8807 14625 551 5183 9314 4847 6899 9257 9636 2314 19893 8044 4832 13669 14285 2244 7769 3349 12335 4218 5968 21809 18432 20476 3993 5995 14770 11415 13068 13406 10567 14018 3602 17220 4192 20312 14779 13037 3375 630 15902 17576 11810 2587 7447 2255 19297 20238 17926 13387 12238 18367 12201 16934 20869 19112 17793 11667 21359 18281 4581 16194 7179 16189 21088 20049 9432 15784 20980 3109 20087 11635 21178 21524 1323 15667 6574 12059 11179 168 9508 1822 9611 7091 16677 3902 4258 7510 4104 19054 9471 16880 4001 11695 12812 17136 16645 9946 17232 12678 12193 5274 3264 15983 549 11381 2329 824 20002 20792 11431 1854 14157 6814 13055 14323 13071 16797 6393 4222 20367 6521 3998 18083 5952 21099 5533 10558 16583 10332 13794 8862 3267 11879 15281 6308 16491 5433 291 12074 4559 116 7777 14537 5119 15829 5441 8299 17723 15358 2966 14154 10881 4190 4420 19610 19596 17503 5425 17103 20172 9607 15238 16484 5863 10282 4485 18465 14881 18774 21453 15042 21477 6847 17653 20848 18722 9056 3347 6659 18393 18038 8651 11835 13806 8242 16495 1235 662 18469 18157 13328 17844 13658 15070 4109 4376 3777 2528 18818 5130 20925 10950 17311 3664 9955 7723 6758 13434 19846 6781 14778 12486 1849 3950 11513 15014 8584 21221 15493 5469 922 2382 13998 1008 534 16256 1751 7617 9624 16595 1948 13773 10719 449 14431 21392 8019 13852 13508 9650 11079 1454 15550 958 18323 9565 4902 7157 5080 7343 5502 21682 12340 20710 11798 1273 17743 20567 14607 14617 16361 13880 18964 7218 1101 6760 15341 16864 2962 10517 4990 10749 10965 15540 1022 20951 4499 15084 19626 641 15775 16083 1410 4200 15967 14559 8570 19680 4894 3840 20682 11321 3256 2777 10498 15652 11920 7293 11219 19679 19202 7685 5516 3021 4251 13221 16143 3929 7548 13177 14051 19329 5511 9583 19097 2472 9084 19085 10605 1218 8043 20393 11488 9866 18462 19947 657 14066 1899 9025 20427 7942 19142 12671 14520 11924 20165 21907 8396 18207 19756 13264 4381 1422 13938 7303 8656 1080 396 15136 20834 13971 14134 21803 562 15781 20265 16224 3701 10329 14676 16983 10458 19220 7408 5279 11086 21944 19079 1188 11837 21456 12750 11692 6536 3540 5144 21006 524 5291 2342 10016 20069 11633 20416 1818 5432 10375 13088 6810 15202 1298 15117 2481 6071 9275 18387 4331 18844 17897 12054 21323 20136 7014 7488 10838 8 12132 11961 10752 17870 20921 8395 4106 15321 9943 20601 19408 13115 9871 10174 14742 2634 12035 14799 5997 20462 311 13306 8965 12097 6136 13933 2532 10886 19033 10360 6890 15048 10469 12764 18200 19570 11982 20453 11393 7213 1248 4162 1725 16943 10874 20813 16512 5270 11180 686 21179 9988 16757 6560 8202 11766 18512 5120 12821 16277 18838 9862 12070 2451 12706 2526 13554 7649 19432 8183 15585 2479 4754 18371 4641 13857 5880 18994 13478 14884 7849 17494 2219 15007 3137 7878 3385 15100 21685 21019 7362 20944 7941 10222 10279 21988 11616 2337 15806 5152 14752 8986 5343 15300 7835 8551 14271 14653 20459 4682 10854 13070 21555 5752 16141 812 7255 11681 17057 8790 13155 9272 20340 5825 3204 712 2070 20195 7806 13727 4899 398 12450 17953 10054 18351 7738 9651 14620 10017 16329 6544 10831 6740 2269 17862 13075 13649 8877 10406 4816 19836 10541 10091 14176 21589 12712 8260 13750 17551 14646 8800 9039 4417 9764 143 6492 3534 11962 3665 15354 1552 14305 16314 16618 11497 6132 2418 20151 15474 18655 18558 17665 17787 21523 19802 585 164 12964 1523 11218 12427 16439 8467 17502 18991 8286 15457 17449 8851 17183 11099 13218 18205 12386 12695 879 17725 19472 1334 18425 20152 18560 21945 4589 5581 4535 366 13959 9618 18974 5566 666 6216 19386 8785 127 9105 17041 11738 17791 20389 21872 11677 19948 1973 10112 15250 12914 11358 20789 7978 14161 16323 20577 15933 2296 931 11864 6729 1667 5336 1232 18127 10509 592 5352 18193 17315 15869 8923 17105 8398 17264 15095 21812 12552 528 17771 9929 2942 17997 4997 4390 11258 18347 17630 21903 18275 2857 19817 5788 3416 6326 20775 5138 17023 18617 2179 19478 21586 3396 2965 12529 10443 8863 15115 20595 15768 12008 16770 13051 14718 433 17005 7798 19037 15320 9251 19648 20499 20866 13990 258 10276 9620 16500 9200 2988 21220 17295 14918 12901 20857 16125 20886 19395 6772 7421 7110 8468 9686 20771 12251 6441 9185 21618 10177 3727 1458 21474 13354 3623 7378 547 18996 3508 14259 8997 8375 11054 21484 16990 12428 16996 11648 4656 9965 1933 1176 5633 16426 1918 12111 4094 1432 873 7417 218 8330 21091 19084 9816 6822 17431 20375 3079 2394 9827 12595 9152 18900 1915 8268 309 3330 13950 7022 7676 12294 18026 80 10623 11820 9428 4548 6430 18176 14944 4264 11663 4642 19958 7788 5935 14518 3097 17919 20988 14604 14927 9761 15349 17178 16263 20926 7043 21825 3433 5468 15471 6184 2992 4234 18946 18502 6887 20402 4385 906 3194 3374 9656 10717 1727 20573 9311 19968 6313 6025 21478 273 11020 19312 2267 4373 18043 10872 4411 1524 17268 10251 17352 14114 1140 20282 7469 11844 7223 17856 2187 783 2475 13153 16229 1602 18312 12728 1790 11464 19325 13380 9883 15147 17518 5018 21806 16017 2089 13274 11608 14163 6592 16435 12212 17751 11242 1824 2622 20111 19504 8103 9814 17313 12805 4555 7356 5280 13711 8052 12140 10346 15839 3598 4940 559 21579 1863 16018 13803 18266 21002 136 15615 15860 20788 15056 3949 147 17044 3470 10876 12449 7339 18313 994 18399 12412 14919 5445 17073 15356 15548 18744 5975 2040 4085 9621 4934 20138 12534 13752 11485 5174 6075 12582 2629 15915 12280 10129 3203 18039 12116 10982 3240 4220 2487 10600 17778 13158 15350 14821 15632 3933 13025 15389 17098 17301 14720 13777 14896 16370 17989 21583 19853 7664 14164 20593 4711 19550 13485 5407 8774 5095 20971 5359 16398 9349 6406 21463 19690 2806 17755 18204 6134 18438 7170 17127 2905 15033 8252 8987 18596 4181 12072 17688 13096 13613 13397 5775 21174 5621 13851 9983 17054 9402 20742 6804 8162 6827 1266 21896 5084 9495 9855 3145 15740 4446 242 20855 738 3444 11696 17833 2773 6656 16549 19565 2798 3914 18008 15331 2301 4081 19889 7623 17115 10246 2928 4633 17632 229 8693 17586 8320 10295 16151 6629 15083 5647 18013 8167 5225 19912 17254 9743 7516 9038 20888 9765 18051 19334 3451 8363 1 10896 10448 7334 20815 17572 11521 4247 3183 20184 3803 11659 1889 5947 12325 7560 16240 6032 1139 3838 12784 7415 1259 4804 20258 1608 11313 11257 12324 12366 13359 20264 1628 8107 1291 15592 3494 16270 21472 15351 4145 2689 14514 10785 15969 1268 12399 13576 13577 1260 7314 1896 9954 1514 20931 18899 13076 6993 6947 6140 20953 1158 11278 21046 18769 20473 12260 3882 8505 17153 4126 6720 3473 7426 9514 18077 20652 15987 9679 10505 21 1440 6303 20157 1137 14530 12734 6569 382 21055 19462 19102 8702 10898 2200 4610 20323 20196 4995 2258 3182 18613 13961 12502 17077 15375 12255 7994 11759 12937 7772 11589 15313 11275 10137 8598 11396 18888 988 3764 14495 12939 11050 10527 757 821 1385 7987 12397 21444 11209 9776 20716 10470 8911 16800 14107 8582 8638 11524 1686 20878 7224 7503 1987 20519 15428 612 9501 18954 14531 6314 9108 16941 19807 12906 7565 2826 9234 9212 4372 89 18226 5381 19806 14776 17455 9663 19602 16563 4062 7247 14477 14109 6763 21139 18514 5824 21391 9717 1018 8771 11324 15684 7483 3009 19326 21646 2912 15907 14794 11511 798 11232 7741 17888 15624 12098 18107 13042 11856 72 8522 10051 20672 741 2186 13399 817 8886 18171 8026 21348 8093 10358 8758 13341 6268 12175 11478 4735 20429 10840 928 17569 18537 13888 14618 20683 12282 9972 12180 15002 9042 19227 21953 2016 9107 3043 6201 6026 257 9498 742 12891 15162 4178 19861 20359 20755 11726 825 21216 4413 16356 21578 15508 15148 12992 13226 12110 14189 16362 11538 3309 9283 14364 13908 5970 1395 18089 14680 1821 12147 1205 11791 21127 19691 16683 233 7722 19776 10087 11785 12799 19234 15865 14590 20163 10068 15139 3520 5089 295 11788 12011 5429 5573 3790 21507 7345 15367 6472 21203 18133 755 3187 5171 14685 14681 14352 4770 11986 17134 15101 10937 12101 13141 11163 15418 9573 21399 15976 12221 4369 9736 6022 17376 18635 137 2134 779 21653 7637 14196 3179 18751 11335 9538 15512 18738 2827 2124 6169 13162 10538 18201 1864 446 9237 21092 19099 4943 20706 21683 3092 4284 12526 21007 16402 17849 21626 19913 17349 14361 17428 4582 2433 5517 11916 14281 1415 16724 7387 18693 10497 8289 17401 1684 15135 17356 3299 3332 9838 5434 14429 905 12010 18836 7472 1295 2650 16963 7900 3521 1852 18358 14185 8477 6556 10224 8645 5295 163 18657 19530 13381 8956 15160 6061 4307 1977 8546 4481 7442 15449 9204 21992 21754 14864 20417 5278 3722 12036 5634 5169 6874 10429 8711 7365 3580 14733 6641 17025 15733 19566 21844 8488 20760 18295 3837 2055 15939 3172 19537 13855 17063 9045 7212 275 10377 13099 3306 2128 13363 15692 13093 107 8337 18280 7983 3039 7928 4418 9021 12948 4538 17878 2030 18504 5871 18391 14131 14170 13464 2227 16540 15315 11941 47 17050 8292 11443 16350 4782 15399 13350 19350 2152 13676 20321 10049 20384 18777 2463 1597 13767 7661 18315 9899 1688 9944 36 2843 13512 21946 13139 7331 12250 2216 13128 849 15799 15030 7632 13721 1868 6401 17279 20643 10944 20503 18134 16158 1827 6285 7112 21644 8002 4206 13811 19682 20028 11065 10042 16994 2392 3877 17027 19780 21465 1703 2174 19443 2406 20192 16165 4613 11731 15947 1568 21968 13234 1361 9639 20231 11009 4113 3049 11676 525 17650 1800 12828 7366 5265 20428 937 12220 11586 9328 6738 649 10739 6751 6845 4058 17290 17811 14878 20693 21181 4744 503 6385 2941 8534 628 410 8303 21148 944 5128 21919 17985 18179 18054 13973 1808 15881 14712 12137 11706 4858 3226 10432 18501 21180 7044 20130 5094 11607 4300 17545 17724 5843 4652 9089 9515 9539 6551 1931 8451 15700 19673 12368 2467 12363 18935 11932 7532 15158 14319 20505 8385 11834 9249 20717 3658 13445 2071 19333 19653 4440 190 4334 19272 8092 10987 20872 7867 9295 19211 21448 15081 14223 7993 14517 19647 7172 2555 10555 11878 17026 18950 8535 3129 13479 8927 13799 11440 2917 20598 2117 1425 18276 1803 7464 1319 3946 7971 21648 17622 7178 6379 3110 1037 20894 5214 11143 17187 15530 16978 18063 735 17817 3151 17499 5631 19668 2585 5792 11966 12684 11098 12149 17875 20316 2162 1676 5725 21290 20300 13065 11072 122 6091 17656 10034 8765 9082 16488 7825 18709 10348 21009 20991 20421 19650 19699 6928 11251 18820 8102 6962 6107 17066 10196 8313 9117 14675 14400 6371 21885 18482 8079 17754 11638 4416 4587 5530 7757 18017 16242 6218 141 19760 9891 20656 14318 3263 21652 5178 17396 9134 11991 21313 2866 20134 7264 11469 4999 90 8919 3248 14533 2563 14276 2636 2969 13606 14901 20568 28 17318 285 6898 10816 18806 10338 7984 5540 8825 19543 21445 18388 16682 2452 9421 11331 2525 14663 10826 19006 222 4122 9276 14389 15196 18456 17039 19620 6413 66 17061 1249 8541 15411 12003 12798 11749 2981 1011 12663 1337 20570 20158 5463 6640 12418 13840 8820 4389 2762 4976 20724 15477 15786 16466 18847 5901 9460 5081 19843 2507 1787 5866 17280 7882 10745 16308 13516 10064 21413 3883 7352 6578 14187 20526 10303 8597 17123 17137 4074 16572 6762 12881 6668 2882 21016 2410 18373 11394 11438 14522 16920 4570 10788 8097 220 9739 2832 18488 21712 8763 2612 11973 15677 5454 15822 15141 4870 14950 21024 20055 15230 5999 19600 15092 10892 2395 457 6948 14042 8055 13724 17796 17433 20320 598 18407 16168 20026 3590 1953 21735 8447 5575 17135 7367 1308 9853 18807 12884 1747 6065 6931 7640 20092 3146 5139 14065 19895 847 10578 8173 13511 5229 16907 9156 21989 18592 16647 1726 8691 20676 3102 16062 16542 16155 11824 808 14097 7026 4310 9836 5963 19101 9644 18780 7648 3284 17610 17146 5567 16404 19575 16117 13205 6420 21048 128 15262 8205 12144 2699 7505 7175 8364 10095 20418 1234 61 7127 2871 4798 12788 6680 18988 15065 9201 6873 6281 20200 8508 5802 21628 12228 9254 7595 11345 830 21575 2178 14698 20520 12920 9735 8671 17018 16664 3479 13414 1094 7116 20085 16287 2620 2811 11076 20700 3343 11927 975 2771 21231 16569 18963 10703 2740 10571 754 12372 527 19816 9598 5449 9553 12317 17042 5690 8315 6198 14488 21286 21544 2539 1533 3624 21827 20090 14412 1668 17373 607 14436 20762 2936 15856 5377 788 18762 16441 5996 21774 3529 17978 16937 8291 18860 1486 8875 10335 16710 8953 11953 14074 16938 4329 17255 20981 15051 4805 10197 9746 6529 3595 19411 6589 8170 8421 18111 5192 17357 12177 11294 19096 6046 17415 20173 6467 3340 21548 13730 21222 13816 20805 8904 21657 1251 18064 11968 4736 15251 20807 7602 13887 10989 10540 2538 21937 17364 10244 4505 10894 20678 1161 3910 7493 18433 20812 13542 17143 8754 10217 19551 7429 17130 7979 192 13775 18870 3205 14096 21467 6256 5964 12834 20854 8265 6429 17211 18326 4552 5012 17599 21636 4634 9619 2703 13835 17998 18033 15326 11496 1462 6960 12867 17802 19009 21309 14709 13047 7536 8075 8623 4957 21714 1357 20213 4714 14992 4978 21750 18354 2384 5205 21368 18140 7392 7346 17173 3101 15329 3685 14145 11080 9602 19901 5629 7646 3228 18220 20224 11671 14091 11259 18052 3399 5184 9350 21183 8525 9939 960 21855 20148 16901 2757 6504 13720 3741 12490 4360 4562 18435 15075 10185 6572 10424 20311 9586 4086 1424 3742 2488 14342 16052 439 16629 18154 11047 20956 2081 19694 550 16660 7998 18685 15400 20377 17155 6701 5452 12184 11563 16546 16250 12675 21475 12016 6786 4412 10326 7957 4332 18903 3382 18199 1511 18759 3890 4108 9824 19371 4229 15072 6245 9091 13368 4214 10775 15862 7517 588 4722 978 2061 6651 3784 4837 15152 18641 2203 15514 20962 5108 11687 19458 9256 11618 19117 20174 10165 1966 14724 2657 2344 4959 7660 12026 13148 12391 14658 9062 7666 5451 4669 13544 10115 9438 5103 12613 5088 9882 1692 8149 17154 7528 7946 7369 13965 19627 10220 20394 3439 6299 8931 19567 18627 14248 11224 9970 7547 13771 13493 19254 18466 13033 12739 3762 4911 12596 244 14837 4072 9485 1735 19317 13431 8132 3027 4776 14474 16699 19124 20430 10516 13749 13389 17243 20331 6897 11789 21671 11328 1946 4289 8027 11957 20008 530 19267 10368 11783 1254 7395 863 21320 14035 17189 6726 14055 20341 2060 15956 6674 1937 18940 3321 17737 21854 11233 17014 10552 16921 9547 12430 19095 1131 19459 21952 18601 6759 13753 17715 19637 17283 14980 5955 5743 8476 21745 5329 2738 5203 13199 2233 1648 21753 15853 20103 18715 14044 19083 5232 7227 3971 2254 7158 9121 18779 14876 3686 20802 12516 14054 13377 3448 9324 5724 20930 18114 20827 7487 14461 21775 2903 21554 8890 5619 7861 18471 10242 5167 6064 11664 15113 3436 18707 17464 19094 15553 2888 7652 16054 15004 12259 20998 19801 17521 9472 10247 12683 5438 1532 16391 1119 15373 20975 1972 11350 7590 1625 6706 21914 14541 21248 2589 18915 16534 1649 4808 19187 15670 11199 19148 19318 17742 18658 17537 9593 10794 14256 4096 647 10515 16567 4579 6607 8387 10721 12223 21541 5261 20983 2861 11803 6111 16028 20510 9494 16762 7710 6411 472 7459 3547 17501 7136 15044 4078 10893 5658 3189 11947 16358 13638 17453 9716 18649 7466 15208 2869 10960 21619 21404 12986 8405 7128 19106 450 10454 20431 4194 13005 17770 20471 4281 7877 10148 10980 19399 2707 2983 20250 21122 7717 19507 21709 1731 6178 13465 18152 16518 8879 18889 2591 16063 10347 18417 20786 21906 16736 20225 15024 15308 20410 14453 1963 12094 15808 13028 19313 21621 20343 417 3995 21609 14933 11700 10147 16341 13552 20884 16294 2697 16906 9179 5051 6200 20219 16047 7234 1522 3201 19939 18967 17001 2607 6834 14536 16219 2899 18062 13781 5548 1562 3188 9687 9363 10107 266 13903 21354 13184 16678 14039 10047 20933 16469 16795 8581 4272 9073 898 21597 9399 10035 18183 14977 14963 6327 20904 14330 1774 10822 758 3373 1705 20535 7092 19332 16856 4928 2142 12365 19809 21749 14301 5142 17973 5236 4391 5346 21681 1100 1250 17214 3041 5812 13449 16177 21797 15868 10631 11746 17420 1938 3175 8414 11576 13612 14734 15510 198 11559 15872 14447 5832 11161 7704 14686 21237 3681 17122 16862 16661 106 12385 14405 2308 14830 6499 8074 7781 4035 17892 5294 2429 300 4265 21028 17654 13038 21036 15010 1542 6925 5348 14355 14515 11064 18022 18938 12299 7430 2959 2734 4968 3793 12142 3087 20585 4857 16333 11318 18742 19250 15703 15528 20927 20179 20293 7305 3921 7679 1981 21888 6550 19860 12279 13569 5978 19675 15053 10094 16067 8126 7066 10827 6826 3133 5635 16281 21826 11070 5461 2390 3153 21218 5349 2693 10195 21027 7438 11416 16443 1084 20954 18416 2129 21383 1092 2902 17806 14905 4199 15252 16555 9493 8281 13846 8053 12717 1685 3472 2091 7851 13101 21629 12615 20444 11668 2914 3474 2955 21124 15731 21153 10004 10913 19114 16078 15319 6137 19707 5898 18182 15749 7034 993 3656 3465 1866 3017 3596 18930 7743 1167 15110 16034 103 16180 16430 18561 11689 10977 13886 10154 16410 7605 13945 2510 5166 12249 1758 3996 16444 14237 20071 6527 14386 16955 5330 16774 13319 15205 16525 21707 3476 7267 6418 6783 16831 17292 8715 21072 11998 7062 18990 19965 5762 910 17423 20349 17970 18663 5427 10085 12086 8501 13647 5984 11252 1753 21327 14032 2027 8386 14659 16378 6667 4006 17541 17369 18785 19791 21332 1122 5311 12598 19858 21443 17207 2318 10111 264 4123 19355 21409 19628 14964 5902 7269 21751 5563 9362 18262 16046 15912 13588 19527 7539 19608 10268 11482 3822 18034 20797 4560 2818 19438 15286 10121 12050 792 5092 16741 16566 14523 8022 13015 3313 10002 12580 10814 12538 6024 12495 6771 13953 16787 19841 16322 19904 5778 21842 7313 14353 21109 17822 7970 9535 3703 696 13337 2855 14218 16630 9562 10426 5443 15039 8782 8613 13818 9129 19625 21686 7446 408 14991 21166 6658 10653 16535 16794 4309 4197 13 15497 8973 15710 18346 4215 18090 19642 16521 6530 10857 9671 16337 15723 19246 20636 14387 4795 9905 15068 17055 15968 3357 2420 20032 14755 283 17550 19502 17590 19556 2097 11699 4596 19164 4510 13421 21406 2001 11583 12900 3806 15571 12511 4931 8609 10853 19155 11426 16929 6244 4342 16666 20990 178 13719 18773 16280 21819 14637 6795 13682 13326 9797 19723 12172 17486 16273 4706 2603 9491 19771 16249 15993 14792 11066 3404 1540 4119 19201 5027 7506 11158 19455 18746 14133 1484 1473 5284 17588 749 7594 17589 16178 16999 20446 15365 1381 2026 16419 6047 4766 12641 12579 11776 17513 5338 13467 17444 7826 5478 20068 20515 305 20318 17861 4439 15962 1200 4898 8135 2540 7670 10546 6757 13461 17959 21933 20958 12347 4156 3799 19714 17119 17917 18961 13685 1979 9787 21680 5937 1588 9775 19794 18237 7281 2896 20994 17339 15712 615 15363 19635 5992 18526 12785 9450 4489 16935 8498 1318 4348 12793 14070 6798 10455 11370 7470 1287 8294 261 11520 21984 3252 8937 18245 19520 15580 18828 14781 2674 7084 1050 5308 12648 1279 15755 21455 2319 7858 5242 8445 12446 12375 6479 21451 10291 18666 14115 6836 20780 19348 18310 904 19664 6703 2608 3746 18517 715 13572 4950 17566 2158 21317 21514 5107 21829 6149 12100 8540 20977 20061 21625 3572 8051 18247 18002 3587 7279 3098 6842 2637 5467 21258 20781 11790 19275 8014 14694 20317 15638 816 6425 3673 2743 11164 13706 6088 14298 2335 21701 20246 7390 4370 5177 5298 16268 18330 13125 4822 19702 609 6 17359 16895 16748 11672 2561 14224 8356 21549 8961 14855 11137 14491 15569 11117 19770 6403 18721 8018 8475 5111 9592 4568 4725 8736 10500 15453 20337 4565 20660 4932 4543 19914 1673 5098 17491 18684 4721 15299 5112 19397 1528 3714 14962 14610 8482 15964 14582 19755 3405 14233 4067 18061 21294 12623 17500 5368 6138 18464 19789 18213 3956 12755 10039 8727 10037 17305 11004 20205 20226 9811 9941 20597 11053 13103 17534 13713 16985 3825 18485 19390 20549 8218 14197 12757 8521 19854 17647 19882 14684 1376 7821 1815 953 12626 12373 16104 10941 18956 17126 655 9675 21285 18044 12444 5701 14761 13020 20913 17834 14024 18951 19239 20088 5887 10025 14417 21957 16730 1307 21023 19519 576 19891 9340 19783 18441 14136 3683 18605 3743 5688 21654 17064 14550 14793 15445 11411 11090 10611 19391 15557 10314 21054 2560 11545 21830 8059 20669 5497 16085 15802 2779 16186 16713 20086 14469 15831 9403 5886 16455 4294 12053 2326 7575 1944 268 13957 13201 483 3191 10327 14623 9020 20023 19959 10847 19715 4771 10998 717 20294 20463 13785 17451 13127 6505 9856 2512 3836 6774 6718 6230 9569 6098 13067 3014 20816 16858 2527 16513 10155 16751 16309 13259 4639 16816 1536 13848 13565 10140 4651 4883 15607 9231 6900 14416 8492 4082 17028 6855 17951 11876 16973 10462 7108 3435 2005 4136 6881 12323 9255 8924 11136 10418 9034 4817 8515 5907 21739 10078 138 9448 6871 4794 623 3797 5007 590 4849 12382 11705 14499 17466 1483 6005 3752 8881 12696 8153 8935 15918 10836 7562 21322 15232 3464 18729 13392 9979 12232 19869 1096 1902 5755 3871 2118 9247 10447 18918 7929 5415 4339 7684 1812 7054 12187 19022 1564 3636 15899 20867 21247 7540 15791 2365 19035 11477 11197 2376 11750 10671 1792 14766 5837 16796 8061 6943 19905 18098 18516 15333 2330 610 15675 19725 15122 3117 9860 4764 19558 8756 5038 2446 1290 8606 19606 9843 12174 15273 8617 9842 17900 4727 12591 8011 5456 6165 11926 11626 15741 12246 11430 327 11784 13157 16503 7050 1976 21593 13678 17707 1487 1660 8034 18711 21235 15259 8532 8461 12957 19636 62 8080 19171 19464 13533 19544 13185 7238 15192 4836 1622 12879 642 11797 7312 13764 17236 17547 14959 19703 10388 14229 16476 15674 10385 17116 15796 11049 5380 10570 19629 15657 13087 1755 13077 11092 9202 14651 19827 11165 8064 12000 17089 14328 12578 8725 7202 20555 19568 12870 4829 347 10919 21397 13611 7768 13946 21864 4392 8151 6427 7397 3390 20252 18845 20432 13463 19851 16087 158 9287 17609 18363 13182 9688 8955 13261 4779 5325 343 7906 8187 762 15507 17827 10278 1148 5514 6625 5134 14449 9274 6719 20646 21407 13729 13501 12374 7311 5250 17443 3335 6100 131 5847 12533 3047 5536 7468 7222 10529 21667 3378 8533 2645 17470 16881 10024 18643 14023 6930 2754 5600 10823 5495 21229 6206 20415 339 16674 74 9470 9709 1623 3751 6875 20666 7815 4788 8510 15673 20610 1292 6265 14546 4232 10727 20614 11376 9291 2484 17719 10603 10955 2222 21647 20740 2643 4129 4216 10985 20249 3148 5899 3362 8999 21784 5683 5607 2790 9377 7484 15180 6210 1438 1572 19885 2247 20467 20070 1712 16738 17675 1256 7363 11570 16976 4473 16004 2237 20253 5217 6395 913 18690 443 16801 16867 13080 1278 12830 3604 13798 10062 8204 14661 3943 18590 20307 17188 20267 21436 20823 13924 10676 7933 4701 21873 6963 10912 14402 18119 17325 14548 9684 9447 15291 5483 217 11714 15483 15844 15310 7010 11154 12058 14765 15437 19538 15279 9777 16140 1078 555 5339 13524 19643 12783 21371 11558 19798 16413 14802 2921 12952 21713 17405 20803 9873 12753 1151 10644 21679 10046 16446 15151 13558 5109 7995 12571 19364 12321 13574 3095 9389 1844 17990 9057 8602 232 11717 6262 8669 15480 14594 3772 18158 8596 4597 574 6312 20879 11566 21547 6164 14473 19930 21303 4755 2062 15937 20972 20504 13896 14415 6768 17752 9332 1054 8440 15089 2915 6229 6465 17118 10807 19490 3935 6337 12523 12896 1956 7130 2926 8367 1677 20221 962 17531 1471 20882 20447 18916 4694 18 17769 10715 20206 17224 7966 9044 5087 2534 14506 20125 15797 1350 15467 13428 8433 15236 19509 19584 18372 14257 4079 911 451 6582 12807 4402 15903 12743 6368 18700 3706 15544 9353 3782 5676 418 9789 11168 15770 11276 13993 13401 12134 20787 12624 3795 3371 8994 20414 16213 15958 19972 7009 1129 286 15666 20168 12507 20579 15197 2102 12038 11125 6755 9910 545 18087 21140 16233 12189 7550 19182 3556 5397 2791 1107 13694 16407 5066 4993 16536 3239 533 3196 21437 10476 9452 9845 9666 10012 6523 5078 20005 10701 14811 12653 3402 3273 20566 7734 10022 17829 15603 5922 7122 4292 18007 2066 15526 4394 14105 19615 3887 6502 20907 2348 4198 17463 15041 397 14938 13473 3588 19255 16485 9261 6402 4139 5740 19327 9749 15584 4750 644 13237 3456 3517 11517 3876 9957 16243 17608 11490 3832 9517 20681 12929 6914 7875 12216 10199 6419 9286 14356 16653 10809 18170 18644 16557 21799 2731 7373 9097 4092 13918 2177 10167 19825 8764 21962 6008 9302 12916 6006 2666 11248 18194 1375 351 7559 3219 1447 9561 1771 13060 17604 2831 11763 2964 20895 6538 19015 7400 14940 20820 2285 14059 17530 12268 15443 1152 3314 17634 16896 4221 16910 14696 6069 3811 10239 7647 2499 11189 13962 6614 11850 7177 7615 18791 17949 19023 5458 2215 8381 3864 1719 21838 12787 19589 3060 5099 10829 2742 918 16695 1501 212 14124 17859 12047 12467 9483 1967 12126 6168 20518 13566 7662 260 15261 9748 12850 1478 9474 6354 1402 9294 21675 2359 16386 3493 15105 1793 18523 20334 14040 17535 20810 17716 20647 9528 5185 7 2293 18444 1217 20967 16290 18687 4237 6608 20920 4471 10370 11367 17642 14690 7243 1370 15554 16789 18712 18681 885 7587 6611 9289 9426 6627 19903 21956 10656 17109 20385 10984 13312 12713 3165 17820 17091 13988 21737 3610 10259 20191 6319 19609 6007 5032 19294 3748 4729 11896 4863 10784 2719 17741 19346 19767 20411 4260 1293 11144 1199 3271 21301 7604 9252 12117 1384 14929 20099 12959 6605 4227 5771 19162 7783 6573 12083 18151 10131 2080 771 6978 9568 12535 14824 1363 21746 16041 7866 19805 12044 8713 16538 12566 18565 21104 11089 3310 3119 21483 18196 3246 19210 24 2923 9937 21486 9579 16206 8319 8457 3141 17698 13176 21044 16275 20995 9963 7889 3287 7304 3710 7940 1173 8389 7489 7524 3220 13981 18890 13165 8272 7434 19875 18683 19177 611 5096 8883 19174 11441 20795 21703 6563 3162 3937 11186 17008 9455 176 5060 4034 14556 2799 4949 7751 3801 17819 17193 14664 7752 14456 1840 6724 166 13475 614 7064 7546 14519 19672 1149 15094 14930 8629 5135 2985 2369 19382 5660 14544 17479 10632 6752 3676 11904 2503 9434 6334 6387 17766 13044 14470 12362 9577 1429 9269 12922 13034 21500 10234 10263 12409 20371 11002 20605 8644 7715 16925 9599 5704 5262 14362 6520 19698 727 17416 21882 6466 18654 424 1761 20677 20811 1492 5707 7930 7329 20551 4436 21634 14061 6849 3865 19484 9375 11057 12320 20348 3974 8797 12272 18041 13593 3668 10439 16605 16262 2661 9488 11119 19572 7109 21034 14955 11344 10688 4891 13951 14947 14990 1472 15064 18634 17893 2867 21254 6950 18383 8699 21061 13618 633 12998 2810 7191 9331 7107 14357 8988 12071 17682 20738 7707 17564 4673 10968 3497 5948 13509 15718 5728 1982 7630 15012 3756 12636 6296 16727 7072 7696 10518 1046 2189 21238 2087 13824 11211 5980 12227 3489 16138 20332 10978 3905 6351 6251 9293 10588 20399 15181 13723 16416 18784 6331 18536 19291 3052 605 21966 2711 4128 16511 18626 17542 6731 804 2947 18716 3071 14914 2565 14246 16066 16244 3615 16811 3484 19559 8432 423 20161 20025 19831 3923 9245 2412 9153 3979 14348 7935 11270 15066 18633 3646 14350 20881 12266 7522 11101 7554 19194 19795 4118 16137 11419 18332 14723 17860 20829 7811 21983 17059 21666 7453 18403 17321 6643 11977 11541 9442 10106 6902 4042 7412 6959 6586 19463 15793 8105 6571 14736 9933 2474 6524 5059 9506 1203 8896 7427 19532 13530 13645 10602 20074 6010 4966 5840 20516 10607 12300 7910 17244 11591 20696 19833 1870 20247 18557 2958 14396 7943 3045 15108 16001 7950 3354 15651 11742 3783 15402 20351 8269 21103 10139 8030 17233 19110 6884 5466 334 13881 4785 5709 763 13582 19447 4492 544 2728 14135 14689 13135 108 16105 3038 14374 18442 73 10575 13743 10283 21981 571 10988 13807 10593 2443 11260 6157 18057 9841 3512 1042 6399 8676 14635 14966 13376 9136 4256 17445 15444 21627 14159 18108 16598 12691 13528 876 6910 14629 11735 11213 20842 16318 20844 10169 19057 16697 18854 11946 15422 19032 9648 814 11918 19514 5684 2313 17594 12704 15863 5462 16429 12564 5464 1820 5945 20121 11437 12766 3067 943 14998 2022 13538 20531 7977 7804 2737 12586 7651 6646 17761 17256 9211 12642 2204 11632 20730 3738 17132 20919 21624 16193 3393 18165 18682 2997 4908 8150 1304 8917 12974 20405 11852 18659 5944 8592 20885 3387 19724 1465 1157 5492 9657 14827 13573 15150 18552 9725 21998 11935 1762 13549 8082 5777 9626 820 11529 14078 4356 1701 16432 14307 10859 20387 5998 18975 8899 17965 17441 8203 11397 13833 12822 19118 363 17289 14049 17980 7956 21561 19384 18602 18018 151 16502 7859 1525 7276 11550 11194 963 17776 10622 13224 21630 16604 6057 12440 5186 2114 14932 13340 4557 20694 19077 1799 17932 9627 8406 17245 406 7462 16044 8259 9343 7207 16493 20764 15498 8989 5503 2908 17910 12405 5870 16865 11128 4869 5043 11988 9900 902 4609 1085 4796 17328 12927 16427 9278 19306 10001 7584 16942 7299 1507 13369 11326 2300 13045 14895 8083 7337 12447 10782 13863 4975 20808 12966 14982 16564 12141 4365 19425 11527 5394 4575 19982 7496 364 17532 20902 7677 12289 8339 8368 10879 12960 19655 20223 3733 19743 13375 16464 15931 7613 9647 6454 4250 9068 2161 13095 12303 17670 20873 2032 2402 15772 3269 17934 1637 13113 8266 7007 17251 8601 8901 541 21649 3142 12760 4453 12046 966 21491 20116 7019 19917 3173 6001 11849 3955 5561 13041 13427 13347 5578 4147 7078 16744 11421 6677 17053 3771 7907 4573 4276 6767 16944 11492 13046 730 9374 15339 3863 16471 3788 12837 21177 20929 8240 18564 3176 362 17837 5399 7724 16261 2836 20671 546 20043 14490 9036 5748 4496 15895 19751 20406 13117 6522 10668 12222 4531 3214 8611 11816 21916 3311 4980 13872 20903 13707 21815 5402 11291 18599 2028 8115 9864 1962 9643 10869 1940 19067 1355 21601 13022 3250 21271 10227 16220 18546 3324 21342 2792 15185 12858 19193 5852 10510 2939 18284 1804 20210 19392 17767 5795 917 3984 2239 4386 12247 1201 2149 11940 16716 15725 13241 1190 18195 1934 6912 13599 3576 5165 18394 3356 18190 10882 17003 7690 17780 10324 17874 21733 11457 12245 10313 10496 9436 8118 5292 2961 16644 17261 21244 13605 5959 5550 17392 21768 4230 2343 9612 14 17651 21792 18771 8077 4778 6004 7531 14034 21049 21358 12156 21545 11060 13013 6370 11542 3488 14643 14177 3785 10090 13828 10159 3744 14634 21660 20937 12527 17472 9660 2286 18288 862 15683 14046 6250 6672 21211 4027 13026 6470 2184 6870 17623 3244 511 16892 14373 6924 8444 3063 21305 19900 10710 21503 21764 21736 3229 1556 2652 1912 7762 16712 5953 13256 16127 19385 274 7665 2739 12672 719 5603 1909 6628 21037 21152 6501 13853 13333 12293 6209 20562 5956 11087 4895 2336 20391 54 4153 16366 18443 17625 6243 7626 716 13435 12115 12731 16214 5222 969 16609 20634 17381 19758 20602 20539 20372 1009 11857 20612 10003 8438 2144 17758 1238 20639 18478 11077 6851 15337 6431 3737 4506 19950 3845 8659 13154 11237 7228 9406 2554 1630 9685 5597 16016 2897 10257 14020 5366 256 18257 20675 9313 3851 5961 12610 18727 21292 13032 8206 6817 10184 9048 8509 813 7355 15766 20169 9317 16791 17659 4344 14841 15824 7629 17007 19980 12151 4681 8172 13968 123 14807 3823 12590 9908 6491 13871 9645 16658 9173 16843 13677 15719 10136 9981 17507 263 7424 11142 1535 3983 19777 9451 6901 17206 10596 15278 16877 1130 9137 16343 19510 2704 2874 14879 21176 7521 13315 11288 10923 8058 6660 2165 12770 1613 11191 16136 9992 8453 1187 11481 12928 2056 6125 9292 20633 19288 6050 206 4802 10757 7881 8831 2011 13168 12517 18796 11832 17343 11877 6548 12802 9726 4821 1221 16758 12869 2885 5160 6632 17753 0 2031 15506 6803 4350 4387 97 7364 17145 21214 5971 21564 20381 10815 14583 14009 19907 9308 20741 4518 2019 17777 10534 1537 5146 13999 6891 4304 11395 3781 20865 21382 13004 18244 15144 672 15867 8010 19299 2658 9516 21786 9632 11493 19036 9417 1212 5646 15840 2000 18786 17803 13345 12747 19457 13675 9531 9390 1636 718 12045 16286 4614 14000 12283 17186 3570 14206 572 16162 21763 5362 17383 6447 1182 18578 6801 4486 11349 3975 8806 13891 7474 18210 3553 20465 19362 19465 7129 7336 9633 6248 5617 19008 13759 19582 21895 13325 6904 2322 19374 9169 9710 9096 13829 9481 9945 18173 16515 14268 1730 18566 6224 21851 11818 10305 2675 3682 5719 6982 12794 9768 1858 16759 16133 21492 17876 7134 5586 5958 16836 21132 13394 11806 13430 13455 12275 21226 1044 11641 523 16131 21948 882 2872 7842 14943 2287 6292 19412 18650 9055 9216 8096 8554 4987 3652 17585 1857 15765 3959 6675 16826 16807 12358 19409 1631 19229 11062 14769 14064 5669 1691 10361 19169 13499 21883 18135 11265 2500 5126 5718 21051 19554 13942 13772 15031 21706 16311 8833 17857 11414 16324 17940 1559 6119 18231 10000 6681 5302 7197 35 21039 19962 10211 10 12668 13527 13628 16504 10233 11148 21728 15691 16972 5460 1620 20596 21246 2341 2375 1718 13601 19821 6567 3159 16568 444 15978 17399 12629 11339 6156 12039 5731 13732 11325 2458 10888 9139 12301 9264 19500 6580 3441 12420 19975 10231 12700 1002 13126 21694 3270 453 13439 3559 12476 115 5228 7888 3953 3099 3765 20846 5685 14375 10594 581 11267 829 17012 1237 8063 5218 21805 12138 19316 13662 2627 16387 8907 15451 10269 900 6287 15385 20104 15521 5801 10100 18782 4648 18697 6493 19517 9404 14462 20616 2569 18472 982 3970 379 16897 2891 7204 16357 12962 20746 2774 1344 19711 17275 12729 5598 18397 5985 21659 6286 2715 15008 7810 19247 18606 3301 11084 5215 6027 4845 2110 10182 14957 14777 18467 12436 18941 1285 15104 21450 65 15996 8423 10812 1601 7383 19231 8308 9500 18801 17447 21565 13967 1594 11510 16113 5768 5028 7599 18849 14552 15876 16966 15268 425 2778 18686 21003 5321 8089 858 9857 2279 13639 16271 19612 3704 16728 16719 15598 12862 17331 20841 14025 4594 9580 733 13860 8267 21990 17473 14749 8804 21245 3315 11465 9013 16449 3394 8310 17323 19945 1997 946 21262 18600 20156 2781 9902 8605 14277 9378 14516 16991 8145 9112 10460 15082 14381 11530 17412 20062 21591 3563 6073 7913 12182 3259 15355 21030 10215 13343 6011 21915 18674 504 5763 7985 4311 13733 11762 19718 13296 7444 764 7323 11528 16384 6083 5304 5408 1051 1833 7847 18812 12288 1135 13267 16667 14840 16473 18904 17246 18562 15954 20900 12302 19535 8399 18079 12573 14847 16448 19270 15174 14716 987 10018 14213 11140 10398 6067 17195 2854 18758 18515 19961 18097 4865 21526 1754 13440 12457 2309 3455 6648 16616 21434 6988 14819 18891 3074 9371 16045 20273 15130 20170 16487 14555 17663 20948 4016 19979 21912 19949 12424 1943 1300 5400 15159 3662 14500 5627 19967 15257 5767 16673 15748 16088 18671 5398 4954 18553 14551 8586 20564 430 11703 13298 231 6362 785 12488 4519 11382 12243 9001 1181 19874 10236 2338 14886 18984 17338 12996 16106 214 20407 16096 13008 17804 6712 282 21598 12031 19166 4203 1442 5692 6231 20606 11739 6683 18757 12829 11942 13280 15248 323 377 9431 15246 1067 6671 20072 20618 9747 11683 20207 3630 17511 17120 7500 9563 10345 17886 15413 6598 20059 16704 1485 13543 14721 19045 5828 9695 16834 100 663 7162 2340 8456 11176 21546 2083 11200 15830 4144 11536 11250 14540 20194 5973 4936 20288 7552 17113 19216 470 2088 19823 14703 736 4884 6949 19273 13956 2825 6547 6617 15302 11612 10007 8110 16400 9682 10265 5029 3260 9113 2171 12084 17933 2310 13173 288 8161 17943 14670 1544 13105 13424 8333 8112 2858 1694 15166 5921 20756 19697 14043 726 13541 3290 21926 479 12705 1090 9386 18993 14662 16573 3985 12504 6364 16883 18919 15189 921 19403 13944 21228 12408 14585 1071 11655 10299 17946 4747 13939 407 15009 20569 14300 7168 13502 18955 6355 14969 2172 1229 13297 8658 17040 16181 14441 7549 8555 2919 221 21807 19456 7797 12350 16849 6696 17592 5356 16335 19863 14266 4430 17924 5149 15509 11402 8084 21459 15726 20187 11952 16438 18333 6835 5765 10138 2346 14798 3918 8958 18949 5036 19298 17362 10309 4881 3552 2630 11647 18873 5657 15717 1333 19439 12746 20852 3206 10480 21670 4351 9504 9759 2263 9135 13997 10728 2763 11554 18385 3363 321 2020 4423 3852 19677 18665 13849 20029 2397 2391 4969 9903 1983 15138 15377 21123 14740 18348 21129 11960 3395 9304 16304 21075 7873 3999 3100 16003 1957 3612 5121 6966 21356 16785 9588 13697 21839 13408 13958 9327 12835 3938 7271 8675 5912 1439 18251 14331 5079 14344 7742 19422 13252 17981 12866 5288 15898 5892 11347 21587 16855 3770 11273 20100 17017 20202 18198 12730 21894 326 14960 17478 4020 1604 21740 11582 4448 21329 17979 12079 15169 20828 20946 14444 1068 9890 5614 1347 13636 9854 1984 8067 19192 18050 18767 20266 8780 13122 1331 1749 357 7117 17818 6461 16584 7370 1678 16670 773 11246 6935 18965 13982 855 13358 19093 7360 7501 10678 16037 19188 20004 9942 8626 3066 14528 1072 9616 16602 14052 17510 308 1640 332 15054 17393 3046 11094 2291 3024 20804 18344 18545 17333 17394 16984 15156 5 3635 17086 12465 9807 17488 10275 21439 18811 7065 2529 13780 18282 3868 5493 20619 4482 11507 16257 14303 10040 3170 12935 17556 19733 1495 2610 16183 6503 9125 11151 19256 1353 20822 2357 13228 16872 17210 17577 6267 9420 1311 7822 10716 21995 9950 2531 15454 15240 14843 11869 10065 9397 21013 8350 19131 4500 14700 8008 9622 12563 15488 14252 15091 8142 5616 7791 15664 2462 3798 17782 20987 1073 14687 13149 15529 21684 19103 12472 18897 11653 15909 17509 10465 12239 4285 21931 21698 17684 9978 19263 1838 20135 6636 18728 13503 4287 13270 29 15380 18701 9130 16241 345 9119 21361 4127 4068 1204 11010 11860 19224 13947 15431 11951 14602 12961 16246 13795 2524 19486 18675 17422 16954 10153 18305 7681 12359 9615 10875 3719 10438 2393 12470 14647 14041 10777 3331 9461 19138 14193 12095 10319 1974 17414 18803 15423 20376 14719 18983 7728 292 16173 12735 1144 17850 15609 18752 17384 473 1275 12972 17713 8787 12982 15952 1296 11584 4110 20572 3909 17011 18045 17705 11116 9559 2471 20153 21608 17935 4209 18528 9051 637 11556 18153 8228 17591 18651 16656 11862 11990 11183 18496 16302 1043 1387 12934 21822 2271 20758 8579 2635 2722 21527 5246 21107 7541 1312 890 4742 5565 12082 3661 21381 17367 21149 7761 17480 19678 18585 11640 4533 4889 16201 18169 17543 9556 14985 8471 18848 6212 10617 5659 20911 15217 7739 13954 17854 6078 5224 4362 1878 12344 10421 14346 16267 6796 13344 3616 4944 4926 2844 16058 14859 1508 14995 7095 19027 19622 20777 16911 13069 12183 18736 8282 15852 19034 6944 11068 13987 1482 16818 3445 1481 20767 153 219 6189 11475 20154 8731 3034 1113 19954 15614 57 18927 3048 15191 21879 3922 16164 1538 4688 2789 3414 18283 10491 6197 354 13163 15751 14172 19788 14211 4044 12022 11039 9102 2654 3707 18302 7589 17639 9877 6264 3866 19337 21201 3792 4445 419 4133 11867 4478 9373 19125 19908 6400 20901 10423 3344 1860 9413 1794 10315 11172 11498 8383 7735 8857 6506 21160 13696 8037 205 5424 18431 19181 14865 14795 19812 9116 21665 12028 3936 10586 5734 1014 1503 19089 5481 7027 3796 6753 5417 7297 14816 4146 21306 3896 17043 3731 689 13289 9430 13915 3916 15849 7478 16266 10532 3549 2050 9157 4099 33 20010 14900 1329 8045 10957 1102 20949 2684 8393 7583 6679 16297 9181 11760 15665 14006 12411 12524 4590 9924 480 11215 17527 15763 14524 12888 8993 481 19524 4632 17363 17294 505 16953 6223 211 19441 4032 21196 12398 21430 17231 2994 2069 18377 1834 15972 3613 13190 12389 9145 15097 6161 7787 18925 10662 14702 1606 5289 5594 8871 19754 20627 18831 8120 11898 16204 8761 1534 21389 16331 10638 12603 18555 556 6702 14539 3875 9382 10918 10897 13913 5787 11295 17687 21410 16638 5083 17756 10290 16601 20081 19393 9958 18809 10341 18609 7643 20674 846 13284 15109 15383 306 18867 13905 18031 19071 13820 7706 14545 8343 4663 20745 5949 12537 13382 6342 9394 16922 20188 3003 18879 18375 5482 14688 12364 14527 6624 11613 4925 21804 17010 13690 18841 2702 21333 14225 15663 9444 1301 18463 17616 20589 2873 8572 2606 21364 3528 8900 4051 2347 4061 13916 17757 4833 19328 19999 6665 2445 9526 21568 17035 18236 20580 17660 16543 11504 18506 15226 7578 17685 3457 18084 21566 2876 16396 3761 14368 6186 5376 19136 11846 20236 10311 20833 20328 1541 20425 7884 6456 16986 18694 6867 21053 8842 11614 12118 18461 13027 3463 9326 4757 9196 13187 2672 19865 19260 13494 2929 16300 18104 3055 20426 5793 7911 21860 7593 9509 6422 2198 669 21743 1848 9847 5927 728 21869 6906 1283 15146 1142 16278 5063 16773 1516 5523 3406 1261 7194 15531 7592 14986 1007 11811 1600 11429 15386 12093 11506 2502 16082 8117 19376 1680 14595 5223 16705 4678 21206 16827 9440 4164 14254 9835 19419 21346 469 1494 13622 4647 11840 19759 14529 17720 21918 20144 1675 6311 13830 13562 7620 2501 10837 9453 19872 1757 7409 18068 20412 7180 539 4326 4970 13489 17307 2835 18861 4671 15604 11684 6440 16726 4503 10213 11669 14880 14243 18556 7809 14401 209 17845 17652 16212 2243 9217 10804 558 10145 21904 18804 117 18019 17879 9986 19989 4831 10530 13432 16207 869 20498 4630 17204 3944 8545 1738 17009 17523 10504 8129 12848 17603 18082 11130 10579 11374 3515 12206 8201 3408 2930 7785 5757 4502 11460 18830 4066 12630 9072 3666 6562 19998 8968 18120 13679 2893 3155 6489 19274 8783 16347 4625 3011 1155 18291 13920 11268 17930 17703 16032 10061 460 9027 19428 10108 10519 1509 7024 8071 8137 12781 8068 20897 14593 995 9520 15826 7169 21777 9757 2417 20119 7778 10254 9065 7886 15533 19926 18095 6033 6302 2918 7571 13322 5379 14691 13870 13329 19574 10647 11450 4546 11108 16163 17519 5736 8336 9032 14209 281 10427 21828 1955 18895 7482 9885 12052 3582 5000 16912 5204 21396 13758 4210 13740 9419 19354 15198 18520 3305 20009 18880 4866 8332 10300 1859 1887 9901 15416 5248 16490 18604 1004 7151 18454 4493 20056 19369 15949 8136 10180 11235 6756 19078 4897 16191 13078 13454 6036 1672 15178 7393 110 20908 4922 3222 19844 15387 20547 1752 12607 13073 9229 19400 19888 20193 6610 2427 2683 4695 11651 11859 11392 10834 251 11385 13646 6549 20118 7869 2151 10219 4992 3294 12551 10353 21876 767 14403 3915 16600 18971 8190 14327 15145 5072 14251 6143 872 5821 21528 11447 17490 5290 7039 10665 4759 13791 19017 13704 9984 8214 20344 13147 10780 20208 20458 11000 13338 5698 2960 18945 15118 4363 20450 12669 6208 11652 6710 1805 10633 13838 21438 1906 19518 13550 20346 20862 4431 19997 8562 15085 14673 10741 1570 5678 8443 2996 11293 7251 5844 20277 9085 1491 7465 6791 12439 636 3281 21255 13909 71 2924 17475 8384 14589 880 5272 14446 7102 14484 18580 11115 203 13150 13686 10207 10858 14747 11379 21571 11034 21658 18624 15549 4043 9385 7760 16027 6621 7230 14577 2796 8279 9160 18487 459 360 18928 21082 14113 21493 13776 12976 5193 12037 14310 11630 9469 10949 5504 5791 19719 5507 3072 14099 20763 15277 4713 18621 20625 19662 12567 2447 13402 11691 19549 4860 15727 16218 15519 11340 14614 6554 6784 15102 21857 9214 8430 12768 3426 14117 1302 19515 6555 7176 20819 1392 10372 18905 4324 1780 495 10266 2694 1842 20945 4941 2864 1244 14803 11174 75 6622 3537 923 13102 15175 14894 2797 18437 12160 14949 11016 9997 239 17150 9244 17069 20661 369 14574 21052 2046 11693 13213 9100 1845 20079 9940 11792 21898 3597 17570 2813 857 19236 3054 16868 19786 11873 15193 5383 12242 3729 13984 13547 4308 13104 14295 5803 18123 5358 9320 10801 6211 1520 9906 18959 908 5444 989 15298 14195 8635 4470 5305 16681 2597 17657 13890 93 12032 3911 810 19873 18189 14984 3622 4588 6446 7736 14893 17370 16436 4428 9872 1578 21572 10287 20976 21084 9707 4282 9290 2277 15795 4850 10105 19539 21929 13253 9931 2614 20523 179 14667 21143 19088 17347 6473 21133 19617 20189 13525 10933 14785 5136 21726 18718 18567 20078 7650 8410 1320 3574 2115 3247 13850 10711 21373 9075 17140 9063 9875 20363 2514 12178 1489 4010 18906 12627 2276 3257 19161 18835 7502 8192 1853 6283 5666 3657 933 389 12310 4718 16863 13683 6017 6129 317 20912 12665 2659 13989 3157 1354 3342 16958 6390 13006 5756 10175 8574 16659 17265 14437 14650 2405 20537 15938 5085 21165 14989 16808 17898 18503 13235 15296 9762 12257 15270 4732 19909 4848 16170 12482 13011 2721 17015 9449 16694 8040 3245 9012 7931 10582 3920 9571 564 6205 20640 21321 12122 18081 14899 19257 20065 8507 17298 674 16478 15407 17598 18713 17148 13083 6272 20303 12843 18046 2038 5322 17823 7318 20400 17692 16475 6396 5796 3413 11006 14394 9104 13895 15942 18037 5471 10033 8155 12600 4008 21337 8511 18278 13024 20512 13064 15730 16451 18910 422 1360 5148 19834 8830 10883 2979 4212 1811 8652 2307 12611 16852 21120 9513 465 13134 1413 11938 86 21208 20712 15894 16092 6012 18468 3143 19614 2883 12481 11449 10939 15616 9560 5585 14439 14283 21297 5062 3723 10289 5313 9800 436 600 13482 4491 19736 16707 12484 4812 886 8220 18524 19804 18766 13086 20035 19081 6773 20137 13774 15537 14936 5510 15845 17747 2119 6909 16510 1421 3180 13994 12339 19466 6063 16788 3641 15693 10293 2954 20890 21790 6508 18620 11456 16606 14408 19710 6450 6104 16077 15005 7008 8078 1589 14260 2029 6338 9851 16124 15021 608 4698 10547 12416 12127 16874 10563 10186 2720 5528 15218 5986 11519 6175 15955 10674 5423 15213 15227 7903 15901 1172 6090 19571 2331 7892 2480 20216 19059 13928 15776 8755 3816 11522 17894 18317 1736 19818 13436 4593 3611 11386 20128 8497 21328 19984 15347 16625 4425 5335 21087 8513 18253 1744 19310 3603 1655 5281 19311 7389 6266 20832 7289 272 10539 10766 6035 11894 19750 16967 9077 10468 14722 14854 19233 13178 10723 16013 20778 2109 10023 7319 4730 340 8673 3016 8076 12041 5717 5254 13180 19963 2041 19943 21408 17324 9696 1682 9683 3490 20254 1802 21835 14496 6844 91 16412 8179 8962 12625 10932 10501 5579 17689 11531 20308 6015 17335 12027 6917 20466 19025 11353 3364 4872 14912 19132 11240 10660 13061 189 11190 15818 10722 5334 17605 7779 10557 19448 19377 604 10967 7829 9439 3818 19479 496 19942 9147 7621 3168 5977 15559 2100 19935 7731 9911 2751 19752 10010 16819 12229 10028 14875 1289 4030 7449 18588 3827 6976 17215 3957 9422 5855 11543 4195 5077 3452 7479 373 15577 7775 17121 10316 12506 11718 7419 4536 4838 10104 10151 3651 13323 19838 11446 20843 739 9323 1428 4050 20877 1865 6511 3114 1488 17718 5827 19241 20015 19933 2986 10292 15306 204 16376 4036 262 15928 639 15244 4225 980 18490 6981 21093 7975 350 21090 16005 17382 64 16260 15821 9693 15656 12727 8733 2465 8452 667 20765 1809 20126 9718 20880 1795 16844 12455 5645 20268 6185 5197 19928 15597 18866 17975 10899 2264 11308 4523 8743 10734 18338 20237 774 9299 14972 17375 21899 11454 16607 7475 4456 14038 7324 4913 16349 10762 18480 13145 3833 5746 16150 10337 507 18101 16061 6588 4433 15773 18059 16465 5361 15634 19473 20543 14217 4060 16981 8379 4355 3506 20067 645 16011 14120 5989 13066 13877 2567 17976 4577 172 9952 13670 21145 13137 9953 1975 7413 3690 7012 77 14337 2736 8824 514 6253 10512 5750 15520 9524 18016 3562 3684 6228 10342 13652 13932 6770 12561 21595 6597 21347 11063 20941 16447 15168 19113 2007 6537 20560 9149 16156 15229 21113 3437 6561 6692 8165 9131 19660 14199 19066 13007 2217 13627 8081 17560 18378 7642 11886 21656 20524 10925 20892 10880 15167 6193 14450 17058 10991 19230 8549 19746 18981 21582 11361 683 1049 3318 6730 18591 17020 17797 2416 4811 2073 5243 16663 21403 17209 10787 16792 8503 2473 12737 17175 20858 18224 11678 5202 21773 4279 12343 14892 10964 15888 15129 6329 16805 9307 3735 21535 1070 18638 12001 7256 1575 5347 18583 5875 4885 6946 487 10911 10756 16775 1338 8766 3711 9741 6945 9219 16823 2386 16691 1258 5385 17628 4768 5188 3919 13239 8394 7782 16932 20080 13843 9694 2782 10019 17072 14612 1414 14547 18676 4005 9019 1746 4211 19611 12165 5605 18714 18028 17227 21518 11166 15297 13468 4463 17319 17637 16160 4704 18047 20301 11722 5179 4382 4168 4377 19341 5941 9774 5929 20102 20859 18076 10746 175 11486 14558 14226 20485 9010 4874 12090 21884 297 14239 8868 5406 8374 17992 5732 9518 5479 19453 15948 3926 13362 21173 6929 13442 21891 14090 14021 7498 16614 18653 7625 1272 19778 11919 16098 11377 11858 13747 12295 1890 4103 21362 15576 12994 21017 6081 18130 12297 11085 17587 15940 12413 4188 7245 7423 1322 21504 16305 12034 21270 18389 15415 8486 17368 8741 1177 21791 15879 5041 19251 9348 8704 12234 10863 7456 3680 7380 13293 1446 1723 11162 19692 18689 17745 15951 18732 8944 7991 2998 889 3485 7473 21098 11216 2221 5355 17710 4409 10682 5712 17913 18136 9203 2419 21849 13386 9964 15249 12158 3609 1759 16731 12190 1257 10451 7895 10502 4313 2354 6799 5437 15819 9188 13832 8941 21315 8371 1596 9974 14359 15242 14048 777 1180 17070 15364 4011 18202 6174 13332 21802 16815 63 18755 4290 5571 17083 13836 17225 18005 5981 2717 395 7708 13648 8784 12795 3491 14311 5447 13140 2678 2048 17406 1573 4554 7118 21374 20550 9808 17691 15188 2662 10813 18096 13934 15499 20357 21921 1585 13513 21834 7853 17013 13884 2173 20353 6158 13650 18640 1116 16559 11501 21224 14783 20970 3536 4341 3869 14782 19068 8244 10770 16930 7763 3575 11274 10592 15646 13336 12225 4809 12646 7353 6408 9273 8169 8465 8777 603 9477 16225 10841 21334 21832 20483 2229 21766 2360 2106 14644 11110 11398 10041 5403 10689 4076 11272 17885 20690 8069 21089 18268 1062 11675 21144 6478 19043 5811 3212 2125 13632 9949 1927 17872 19852 6404 16434 4371 13517 20297 21943 3835 12763 9423 16373 8619 14003 17114 12849 13092 17403 1091 10549 14250 5577 3894 12015 20373 10628 4690 14923 12287 3625 16671 7850 10102 1382 186 14143 20776 16979 18754 5091 1214 11234 4739 13029 2845 19864 16866 20203 6333 2559 3766 20396 3425 14656 16833 8013 5715 21366 20479 2136 7374 5794 17696 3535 7037 4815 9346 11472 13518 240 15379 10760 21982 14767 17159 5323 14727 3121 16239 13623 12209 13755 18418 4262 17546 2259 4367 10599 10321 17947 9667 894 14910 4569 2437 16123 9784 4873 6903 18451 7308 11187 20330 6765 2192 8870 14379 7058 3504 6921 19435 11774 2453 11822 15023 7855 9396 10830 21559 16780 5055 14324 16621 7671 18817 16144 7028 20482 7240 15290 21194 4901 16338 14906 19219 11100 8895 2037 12689 3462 5979 13488 18610 8921 15935 954 800 12936 1728 20871 3322 6510 16585 7450 6038 16742 1711 5686 9756 926 13123 21452 4166 2181 11546 2361 2403 17436 20565 13285 18530 7273 5845 11728 14191 21081 16408 342 2795 12687 3829 18252 12987 1706 19797 17952 20817 15642 10232 2821 4091 6860 20883 5706 18944 21668 19279 12707 13030 6853 16552 5326 722 3419 4746 10240 2414 8241 9395 11223 629 16771 5161 19237 20096 5219 4399 1681 8568 16327 6445 316 474 12765 21765 18958 19739 18212 4692 7891 924 4475 7451 20190 14383 20333 6123 9713 20180 21335 15079 6961 20185 3586 12855 6288 10696 13765 4799 3492 21182 9197 1576 5075 18301 6080 15372 2460 7989 16887 9850 2991 7174 1175 6365 16382 8752 2132 10201 14245 20089 14467 17411 5730 13349 3855 3091 1830 7937 8585 5470 17270 13672 1095 6233 724 4012 4841 17467 10379 8620 13107 5490 19993 3397 14557 18509 14358 11127 20957 5810 17884 8023 20623 19302 9758 3298 3160 6813 613 16251 2598 10322 13701 8211 18921 11152 9742 9763 14867 14967 21787 4741 19394 14480 4021 7280 2457 6972 4728 18839 20256 21257 10705 15618 11403 5145 18699 14773 2266 1029 5410 21440 2387 19728 4786 8094 15194 8006 16579 16036 5093 215 21529 5705 14627 2210 10981 16904 8992 20095 7490 15006 17226 7693 8378 9146 1460 7071 7816 1530 19964 16458 4380 18148 2724 11228 7580 15798 3430 16248 4114 5776 15243 19434 1053 19144 10580 11229 19592 21702 5895 2196 9728 18361 5487 10595 19856 16804 16988 10340 851 3369 337 12726 5721 17253 19552 6273 13492 310 12491 16033 2572 14194 515 17284 16399 5003 7013 4679 15127 19474 7045 14088 16071 13216 6782 15362 1872 12724 12406 9863 2188 21010 19525 6780 14503 2497 8548 1276 1835 13265 8495 10803 14186 3288 10574 10754 11912 17563 2065 13584 17402 16195 415 155 17170 202 2232 2311 13626 18508 9935 8436 10645 7720 15908 9536 8065 20713 8828 12942 13236 756 2655 18306 7796 15859 11903 6084 3353 19433 4784 15478 4015 18256 14413 14146 20784 4024 11480 19145 5615 11296 7586 17864 540 12327 19446 7980 3861 21308 3401 18840 4049 7969 19936 16997 7616 12377 5064 6225 6518 4228 990 2944 18434 16292 21779 19706 20101 20680 6512 648 1263 10885 18258 9268 21639 3434 21502 9225 3325 19300 11644 845 4213 20327 10088 4435 11332 13505 17961 6843 17322 10648 11944 18124 15827 983 17963 1230 21187 16633 13786 4644 11601 15809 6806 9300 21866 20733 5826 18622 1855 21197 18470 2206 4172 20464 10790 9468 16231 5411 15701 3505 7601 1055 11539 4173 6274 8757 1874 21296 6130 5505 10466 16524 5632 17808 1733 14888 13894 7824 5917 8232 14834 17498 4871 8441 8563 2496 12150 18909 19684 14202 2238 18896 9205 9982 4723 12384 6321 9809 3081 21696 8887 16008 8974 11201 19987 7561 10651 8915 18871 16715 16784 13212 2423 2242 11383 14334 10204 15714 18519 12547 20561 7394 10351 7187 20759 3581 5374 19561 9074 16871 15922 20720 18369 2225 7003 20728 3787 20110 13091 9832 14887 7004 17172 9151 11487 14455 13286 18360 18259 1510 5386 11885 10446 6278 270 13653 10690 18753 20617 17600 2801 19940 6293 1056 14931 17274 13841 3672 21032 3411 6062 16351 21718 1892 10914 12685 7268 2282 6097 18128 21476 1813 616 12309 13472 12508 12152 11048 20171 6099 21840 6821 10057 3209 26 5110 14370 14979 10072 8125 12422 2325 17571 20243 3177 670 13714 18254 5100 8248 21154 17548 18489 17285 1832 17144 10026 8845 21398 5475 10905 3879 1518 21405 5972 9266 6170 2968 1136 17249 17649 11794 6196 15959 4090 167 1179 17717 13931 6180 6236 16928 19226 13684 14510 11221 8229 2671 7414 9529 6358 685 1500 2127 6507 12434 20041 20629 10142 9629 20863 1373 787 20197 2506 19440 18353 1023 12882 18070 12772 16636 21056 3988 19793 12989 20132 6031 2252 2631 19470 8215 4591 14711 7961 5485 6558 11708 17198 3791 13227 3333 20204 367 3514 13411 11514 6051 13251 13854 16915 1243 14479 15038 9086 12085 10312 11241 4357 10381 12106 1210 21501 9239 6397 2057 19744 7032 18427 5749 6434 20648 6861 2379 7020 2595 13222 7075 2819 371 17842 12466 19172 5220 2824 811 2509 17484 160 6687 7754 3734 9662 14360 20874 20342 18492 12494 3400 1477 8424 8348 11939 14713 5639 9095 10846 3842 10514 9956 2989 18833 9584 6000 9973 7899 13681 1061 16007 14371 10409 4748 12971 15140 15111 2248 10495 4920 2249 6263 19002 8990 15566 18113 10868 11021 10472 7089 13384 5993 18088 5310 18366 17465 12096 16080 20298 2984 16631 9441 7727 8258 14363 17891 20608 10323 4511 16014 6888 3377 12487 7538 19830 7203 11184 7673 14909 11596 21097 6030 5446 21674 1779 20149 8234 2967 18570 2881 784 3058 16786 11767 11462 3513 4107 20735 3699 16620 18797 1277 16615 13378 3460 4576 12342 16385 5328 11724 12774 19683 9316 7771 1571 7396 11577 7574 14889 21499 11245 10943 20934 13156 12542 8948 12171 11121 13062 852 1394 14915 4117 11905 17341 1282 13594 16420 4697 9920 18821 13889 4861 18826 21059 13487 16760 312 2101 6108 14005 10336 4700 17977 19091 14069 8816 18056 16960 12817 21530 5865 2786 4810 17350 2092 2679 12657 18356 7167 1574 12066 19735 17002 15448 10736 17157 13444 12890 14169 10006 19264 17340 19197 10384 1315 4602 19712 15759 20556 15233 660 21858 9282 11427 3268 10620 21742 14111 16642 13790 11917 13555 12860 20936 14863 21623 4480 18386 21560 3372 2350 5157 9215 3195 1941 5546 11365 8925 1345 3370 20576 8401 17446 13407 1590 4452 1704 11631 14341 19 9638 17327 805 11323 15573 4824 2664 9120 6141 14956 13689 13736 9109 7225 4751 18012 15960 21339 5805 10761 3789 19638 7048 6769 12643 11417 4378 7287 10133 11022 16334 11606 15630 21793 7898 11015 1030 8854 16374 8003 9305 14771 9161 2356 13844 14860 6664 8133 44 14063 20529 8832 20198 11772 11301 6977 7834 19931 19016 15043 15438 8377 9753 21489 20737 15049 17693 15309 2647 15096 13160 7986 3277 2483 2371 18672 676 5019 1452 3389 1159 9889 18783 18723 14818 15871 17104 17948 12940 21066 3600 6114 19469 4025 15222 17506 15433 174 3674 9654 6304 20486 4868 1216 17032 19632 21515 14756 3482 15626 18209 3942 3591 3120 17180 661 12797 17801 14836 707 7310 10634 13793 4504 12556 2848 7628 16581 7887 4619 10152 15316 9724 10587 10157 5589 10308 7445 3705 1013 1206 1750 5395 19179 2023 10545 13868 18225 17477 8274 4556 21457 11046 2520 11156 11842 14207 11290 2584 21414 18144 11666 3323 5681 21227 8341 2809 3509 6246 12296 12105 17230 5513 12742 21977 8181 13979 20358 3341 14746 7291 10537 8550 8856 11041 19533 18448 12025 18379 16527 7422 8947 17169 14410 2034 2680 13360 4177 6519 15274 197 21350 7286 9005 5930 21116 9859 14801 3124 20160 20319 18772 1512 1555 1088 2297 7963 8705 9919 7705 10565 17037 4586 3644 17308 15971 2401 15353 4257 15403 9391 12838 18834 18080 201 3217 16646 10931 6094 19531 14786 1796 16176 17355 12007 13130 7916 19937 14008 13310 6072 3584 11129 9222 6805 10934 10966 18085 6002 16817 21316 11745 19727 6707 17607 5869 16505 965 16829 8499 6739 1591 4291 807 7909 15099 5263 9166 7701 10359 2644 2288 5457 3466 1418 8415 20721 18932 16798 6816 10020 17568 7216 8932 7904 6374 12592 15779 951 9786 13535 11476 4317 2074 2766 19290 13052 19116 2116 21537 13760 21552 10523 3077 3278 4102 6564 7190 19451 18131 15722 9118 19156 18673 14613 16023 9846 4249 18263 18571 17272 4960 5888 21999 19011 11779 18398 20796 3190 20034 2596 10773 1553 16733 18355 16040 20339 8416 7327 9652 20984 2776 14012 8197 19955 9519 2377 5500 6249 10564 9400 18006 19726 6694 10973 675 16153 21925 13324 6575 4191 2494 17914 9387 16368 9976 383 2741 10493 10669 18874 2067 10503 4120 3649 21487 5564 8347 12104 11782 16634 18856 5820 53 16498 2892 1690 4877 3721 7454 14426 2398 18308 7386 3850 18865 15239 1189 15434 18500 10613 15556 16479 9303 12574 19126 11330 18702 19583 21669 3211 3750 2372 9507 8912 8392 21150 12621 2920 3862 1651 8346 19046 19013 20245 19522 21870 10959 17517 7402 10434 21519 13240 21663 1459 11915 1468 6846 14482 16544 8224 1099 5016 21688 8812 12694 2051 6087 14411 19070 17673 7927 1989 8740 20112 19296 16523 12202 21380 5255 11756 13053 2723 4269 21699 15611 20507 1389 7277 14917 1928 3131 11715 21205 6488 17729 7052 3712 11931 19293 3471 6647 5498 11624 15016 8888 18032 17540 2036 20183 12946 21338 19010 13805 21551 21424 21068 16417 1888 13372 7668 9172 3454 21837 12426 411 13457 8846 14631 4202 5797 9365 11839 18303 14137 5974 5733 13248 21395 13395 1103 1582 16669 13822 6685 21887 11861 12318 2802 13276 17944 10867 12107 896 4547 8639 10092 21843 5489 17795 1624 20731 4383 5760 18494 10884 14725 619 5035 6604 11253 462 1213 5885 4346 13802 848 18277 2226 11303 8263 21817 17377 7183 3618 15906 9772 5176 6423 11038 10484 9581 13443 11113 12844 7259 4406 15641 5926 21878 3516 677 15686 3282 13823 7244 700 2787 14780 10833 8212 4157 7854 14606 1445 20124 15045 13219 8156 502 1561 6596 8335 21677 1400 14596 8185 2747 17346 368 19696 20809 16899 2938 6378 18260 6215 21580 14168 16961 8345 18734 19973 17779 3070 8772 8455 21314 8567 14414 293 18843 11680 6699 16360 4022 4801 5894 5440 4138 12949 7214 12258 7031 11463 14293 9849 6880 15317 1328 11590 15516 19547 18155 6021 4193 14445 15378 12002 19483 8593 7295 18768 3386 11909 12355 20659 11227 2995 20628 3238 105 4063 16055 11637 8600 18770 8298 8296 3334 13715 19406 20281 5770 3033 452 3857 2880 16289 14645 21101 21781 17925 17133 16409 7864 8576 12423 7016 18579 12129 8712 19060 2673 18175 21861 20309 7059 16793 8245 12895 14671 15561 19276 1856 12145 13578 20973 4054 2002 10608 11175 13283 13936 6471 8970 13049 11439 1616 4075 3037 4905 7965 12640 18222 15570 14619 2800 16711 5047 7669 1341 21429 21778 11690 17462 1241 15396 19700 10481 16428 4888 5962 18511 4611 14376 2834 1369 15857 17763 1612 3291 6162 867 6543 10683 21965 13754 6833 11474 9228 19367 4073 4635 14156 5557 18160 18364 5162 7104 10405 11035 535 16541 8908 12676 15855 13948 19822 19541 2604 8123 8866 4314 6517 6135 20345 7440 16232 6585 4217 17701 2710 2557 2466 7936 6103 2084 10079 10528 20211 19768 19661 20030 19150 6655 17429 12546 2052 4769 14138 14212 20586 7046 8327 18473 1145 4665 7341 19969 4578 2651 920 17526 10441 3802 2860 12970 7526 20955 5562 8864 13603 5994 10208 5741 19258 2298 4924 567 10479 10190 5282 385 6388 13393 9512 5189 4026 15155 19281 19215 16668 2974 8571 3991 7485 10364 10407 5641 18822 17958 4961 7635 14015 11578 11327 9781 19956 3258 4065 7300 21156 19082 13665 13390 14762 7733 8811 13172 3969 9408 17666 17260 10610 18661 14022 13317 11600 12539 16397 21780 17619 21655 2133 16551 21729 3543 9339 11954 3948 16848 12307 870 11277 594 11704 6435 5708 21973 11096 5606 6020 3113 18884 4830 18003 13619 8726 16689 13526 20017 19496 16282 4424 6941 11969 5848 2877 21975 3345 20350 14628 20066 802 14409 14908 11214 13245 9754 14699 3720 13666 10839 20905 19111 243 15058 13036 9033 6332 13437 10193 21259 10376 11360 2224 4733 959 11470 10117 11007 21252 5360 12269 9780 18267 7976 1837 4207 4161 9798 6121 17273 8176 6341 16635 1127 1548 12931 2328 12719 5800 7504 15619 6932 1949 16909 16196 21045 6630 3346 9052 18073 2508 19919 1027 19349 1017 15891 1228 15187 21880 21432 17557 12679 12721 12119 11516 19762 2727 187 5327 18995 15702 16462 15565 2454 21283 8735 8648 15721 13313 743 20959 13010 4037 16128 13557 2498 4710 11181 1895 15216 13167 7802 7284 13741 9896 883 19924 6172 13351 15589 14489 9191 14581 13275 9733 7067 11872 9603 2240 16747 20296 6601 144 16074 4508 8850 17417 18401 19151 18159 21433 10394 8351 15504 3030 12459 6214 3745 4298 21219 15037 5891 10920 17432 6176 21207 1788 18405 12417 14037 2841 10550 21344 3606 8544 6115 8916 553 1074 3432 4797 4305 11056 10271 11193 17750 12711 11925 9162 10901 13641 20649 21759 5141 14108 9719 14853 17287 3730 4353 21170 18162 18357 6449 19983 19028 19407 8762 18270 1057 10759 19877 17330 15059 10993 15790 15887 8599 4912 9014 20997 6013 7683 11532 6509 4979 9115 2993 6297 21638 6957 6452 15258 13515 8056 9895 18298 9904 405 9143 903 12509 3384 7425 17957 10560 19379 18855 17646 17085 7718 18710 21310 16284 21115 19266 12584 8448 15643 19810 16931 4414 10126 10486 19286 18933 3107 17437 21509 19075 6728 19087 8175 4856 14476 16753 2444 11093 15214 3891 21715 4083 10650 2082 18030 21341 5842 4835 16188 20891 10637 6486 9000 1314 10172 7581 21539 6964 14238 15649 4890 20679 4160 11754 1772 21497 7726 16908 21650 16431 14730 15340 7441 7730 17544 21223 8355 6526 701 19790 11817 16364 16893 247 19971 19207 2160 14275 13583 17772 13556 2251 21760 19716 21848 8519 11673 10663 13209 15361 8328 10304 14831 15708 20722 2783 19200 13810 7051 12326 2076 16517 8722 12133 13197 12674 13396 9230 12292 14366 7248 1019 7748 10750 4790 1469 20409 1390 5568 19892 13186 10561 20455 1880 17528 13621 2566 11141 17731 2008 8739 4189 12588 7142 4628 11147 1836 7035 7863 13878 11351 8152 20707 11401 1913 1695 319 12200 8799 1377 9971 1768 12060 5101 20511 6997 11244 836 9658 11747 21393 19173 420 7077 19505 3023 18484 13596 5786 9312 21425 7332 3312 21856 17583 11851 10038 8791 18505 6055 9058 11889 21540 4254 18763 18400 10776 13656 12778 9980 3780 14028 8305 8024 991 10188 6525 4923 8972 8048 7253 18475 1502 5773 3234 5544 18544 17419 12741 9383 5799 2245 9914 18191 11721 15812 8113 14267 21026 14575 18882 10704 1663 11205 12069 21996 4497 14433 21215 15440 19599 11900 21690 15369 13995 19052 10298 7949 860 14576 11595 5318 8663 4130 4540 10264 16107 16209 2283 1227 12120 21592 16265 11032 16776 5428 11764 8226 8628 49 13963 18243 7953 11043 12367 5889 8391 5525 9407 9874 1207 2953 126 14573 17788 3739 20315 3135 7171 20701 12614 12196 5015 12549 19902 15494 11884 7934 8759 20398 1666 13659 16091 3776 7725 21387 17848 7860 7588 5260 15792 3839 6074 11725 18948 16103 16509 18368 8939 828 20830 1551 21963 10713 7382 1767 15913 11573 11372 838 11580 11399 2290 21821 19630 2042 6676 11901 12872 3939 16582 19309 12815 5853 560 6866 11075 8050 4349 1627 2063 21312 16346 17084 5122 19366 4551 15327 6583 95 6386 569 12868 13497 8865 4464 12351 8100 5276 1020 16765 9810 19978 8603 20917 17835 19353 5194 9445 21939 12999 17299 5584 18349 12968 3251 992 12512 10935 7894 10397 15464 15184 17890 12993 6793 7533 10355 694 19857 17266 16463 7520 15568 16216 19666 11091 12211 16381 11809 19896 12167 3978 15393 11825 20864 18554 7852 21610 3578 8198 15582 20262 4544 17317 5735 9376 6380 6954 16857 9590 12756 16875 20155 12703 21833 1565 296 695 7278 14155 4537 18851 15052 18072 8357 17164 19143 15815 1117 15090 1700 8681 13090 1346 11499 18092 11908 21243 21200 6820 20838 20488 3856 3872 2765 7081 13273 7005 16592 11120 11870 7113 11363 6191 15744 301 9829 5675 5628 6056 4606 11027 16482 4627 8489 5806 13904 12555 6179 14325 9158 14813 18116 18447 18218 4176 8134 8938 5761 4004 11593 14475 7509 4903 4171 5846 18172 21616 2700 2120 13898 21062 9788 12632 6934 12203 12146 7876 5050 1441 3925 14424 12557 20821 4056 14182 19792 14351 5547 13661 21365 16903 6096 20887 21293 18227 7513 18877 12801 16306 20232 10928 9542 9334 1045 19090 3940 18423 13295 13480 8322 12899 21532 14731 8680 16964 13159 9670 15271 19701 4246 15555 18000 17056 14339 11734 11319 13233 11342 7525 18542 21230 1240 18410 7491 13635 15435 10878 2945 20435 9443 12448 7512 2641 12955 5919 9347 14695 5552 4059 14591 21213 1708 20292 9090 12499 12954 5857 13417 19424 20993 2676 17784 1921 6163 11489 6721 4379 19536 18161 21738 1365 7732 10938 2112 13490 5484 20799 10707 6484 7258 6376 21464 20175 19417 10612 4041 17110 15201 17606 196 6764 10286 6975 3293 11013 1398 5950 349 20916 11674 7132 4495 18409 4737 6953 5001 14693 14508 8210 11907 12087 14763 19402 12319 13768 12381 21136 21004 5649 15017 8614 19351 8983 20364 21772 4158 16122 21710 6381 15391 16236 8314 10976 9411 998 40 17524 21961 17799 3275 5011 10589 1270 11643 8630 5234 14162 9496 7833 19604 4315 1069 15200 384 17565 9705 5789 2785 16095 9649 2241 19618 7612 2542 9597 10729 5422 20697 5772 2677 15745 9187 4595 14264 6652 2681 16279 7990 7750 17278 7126 9114 20314 6918 15114 12650 16591 20496 10927 6865 4141 11833 13279 21521 19063 15757 1654 6475 5421 12859 8836 11445 14942 21979 11160 11249 20703 10109 18322 11483 11146 19593 9566 19899 21788 12814 15639 16035 10302 9418 8604 3601 823 10805 6459 7958 11658 18607 11719 5515 21473 16597 338 8062 5230 17711 12073 7553 426 10228 8728 13581 3990 7608 16129 10731 20051 3564 9595 1970 15405 4541 12441 1664 6534 1058 48 12281 20631 11853 13845 12402 21370 3438 6352 17387 409 10425 21538 5026 16492 223 9637 4738 1839 10408 18125 15352 4278 3469 6579 7745 19548 3859 3084 18978 14868 20230 20076 13522 684 8275 11133 17434 399 11617 4542 9342 16799 5216 19372 21278 19217 3627 18497 5590 17495 12599 11001 17852 21871 14563 9457 1766 12605 14800 14922 5906 20091 9355 15610 4584 10819 15832 2231 20699 10702 10626 16486 2663 16445 4726 13792 9537 856 12501 500 17149 9969 15704 8239 12677 6054 16405 20097 13138 15920 6657 4942 21972 10709 6693 20123 7340 13085 14926 19953 20117 245 5496 12330 8903 1367 11413 17259 5699 3945 8689 401 21533 5268 8618 4148 13667 9567 14737 10073 16950 4686 20654 16352 6686 14511 12462 92 9296 11765 17908 20014 14317 13744 19992 13314 14093 12583 18522 7135 18430 4224 13663 14306 6733 21808 15595 12806 19259 1900 17263 16755 10849 10576 16392 3901 5909 5252 14872 5014 15897 8872 6271 4938 9912 13847 20146 9926 11081 5954 15417 2648 8007 5604 21070 9233 1641 18612 599 3028 4952 8590 8312 16891 1093 10906 7591 3440 21557 16422 9164 6862 14309 6516 14142 9401 19147 15994 2562 16009 3127 11518 284 5526 3908 9721 7839 15404 10450 5155 11139 13937 15133 2163 20 1141 248 17128 13000 19314 14815 10687 16169 1996 15190 2139 5539 15581 11311 14862 20542 4598 1457 16423 16093 3648 10952 20159 664 12980 16345 10318 7801 9609 16632 20084 7563 8087 2381 10553 3895 2024 14284 18340 16042 5602 7794 10568 17573 12112 21498 7627 1294 14084 21949 440 294 20107 4375 21942 1929 6166 7448 10900 4767 8979 20386 3621 18625 13996 19437 16732 2185 14622 16772 6969 2690 16822 7399 12832 3085 14089 10187 4935 7154 13902 13057 7765 20235 12345 21693 2907 15032 7897 7893 1035 11436 18214 13688 11279 19906 10070 1012 7600 11729 18677 909 8625 18286 5249 15447 1662 20113 9653 12468 20989 17094 11271 19024 13003 7342 16947 8450 7776 13412 14427 7721 8527 12883 2820 2157 13111 9887 21641 17493 19542 8813 9221 5823 11420 16110 7398 17448 13738 17620 18631 16703 11698 15542 2207 6971 7689 19430 1699 21643 17955 20557 18345 21190 14588 10223 1133 12361 19540 2803 6282 4758 14457 18343 6603 5501 21525 19387 1969 19086 11972 18112 8101 6118 18374 8086 2274 15036 6153 14838 19591 3605 1683 14128 17038 17019 19214 5459 8306 17087 8773 14420 9993 21128 16139 16951 6705 237 10212 6609 16030 5925 6106 11804 4472 12395 15834 6638 2006 18816 6034 15681 16050 15567 4814 9918 12709 6199 16923 6131 20704 21063 11239 9329 9446 15596 12497 14255 5640 15408 9277 8426 15620 4561 11 12492 3634 12918 7680 1581 7061 14851 20060 9738 19921 5990 20837 9830 15743 2211 9195 9991 20714 16956 3663 13600 12608 11999 3692 2108 16821 15015 19784 19887 12208 4185 16939 16971 7921 15153 5436 13246 2859 11875 14566 984 10226 9123 11428 14640 18934 6951 16722 2667 8317 12458 2017 16222 135 335 20952 18595 13710 2413 6824 6173 1170 13910 6323 5650 2649 3316 6058 5104 16508 21384 20283 13152 2537 20227 8235 14814 4743 16111 5333 2432 13529 6831 20751 1618 42 15713 7099 668 3232 21543 4465 3844 1786 4971 752 1770 8873 18181 15943 17683 16049 13039 15412 10942 2948 3594 3824 18551 8418 1505 21814 9947 11657 5682 7416 6148 10173 21517 1128 5987 10891 19704 17615 8742 9815 15324 532 8767 14148 20354 21253 10961 3006 2758 2014 13266 799 16936 21479 673 3687 17841 18597 9755 21300 2103 1876 7840 8287 20522 11553 4241 10056 184 19499 6642 11207 7211 1661 13215 2180 2495 18726 434 12977 18733 2044 21700 7148 6259 15255 596 8616 16627 2552 14340 9213 17636 17785 9801 1239 5320 9770 280 2564 4583 12924 15348 12013 265 11863 299 7905 17142 5932 6442 19283 2980 11575 10431 21905 8251 14272 831 13146 8744 19051 20861 14448 8885 1829 18862 2852 21064 21508 1016 14263 934 12333 60 4539 9604 11697 9175 8776 3022 8655 15644 7570 14332 7328 7049 11124 14842 16948 19141 940 13839 19624 216 19418 14031 17483 11702 1114 3128 4859 2246 2804 5053 16737 7119 16395 780 13204 6335 18892 16957 20711 19687 10081 13539 3755 1444 8529 9088 10378 13537 884 15578 7638 6823 5856 12089 8365 12736 16401 9867 9848 14081 3577 18765 12123 4520 20276 1456 20451 5251 18426 1566 8839 486 21852 13110 182 877 21974 21867 9333 10128 8945 13783 9475 6360 7749 15873 8184 17124 17597 12194 7567 13624 16006 8049 1789 11435 929 13278 16274 3171 9551 13019 10851 15658 17390 13292 13063 1964 20896 8643 19019 3181 21202 17561 7338 1343 11433 5004 18872 1297 8295 6725 14116 12816 15430 4985 708 20530 18829 7053 8952 14354 19249 16860 9661 10706 7598 7610 1698 19652 7848 5541 6895 3749 4826 20497 2833 8718 5738 10118 16675 653 19603 10543 6736 8288 3443 21355 18581 21642 19512 17468 9723 9541 15263 1435 18902 14760 18802 15742 5002 12852 7688 12390 10252 10101 7200 4846 19721 15711 3592 18164 1745 104 11890 13742 3628 12847 12254 9994 16132 5662 20847 14101 10116 6714 2126 10658 7764 18058 5874 4019 8408 14220 14029 19221 8746 12006 543 7880 6023 20622 7141 12153 18187 6742 711 15186 16115 5754 15011 10992 7254 10261 17816 19894 9064 18271 7217 9017 6203 15966 6226 16024 20448 165 15534 14080 21722 4000 2478 3140 12530 4852 7163 1330 18966 625 5587 10675 9043 19960 18669 18292 2875 16612 19178 3542 5477 11284 4982 11568 2838 6041 1647 17304 21151 1449 4343 7914 19730 4875 6257 10789 23 6634 3166 4002 11610 17131 16900 1321 21789 112 11014 4151 16325 4140 14683 16556 16059 18428 15735 15981 19974 17158 9081 5965 1817 3418 17708 13969 21460 18534 7461 16203 3376 12865 7697 21776 7002 2788 1550 5372 20355 9600 2581 21574 16847 12956 12443 8539 20663 14098 5364 17533 2583 19601 493 207 9698 20779 5936 17508 1569 20845 6777 16313 17672 10235 17458 8479 13272 21117 19005 8104 4513 5273 13640 11771 12103 19811 15221 7576 8649 10442 3773 15228 21401 19191 21111 2270 17582 11623 5893 12473 18876 11562 19709 526 19153 10277 18376 15546 16497 18219 16116 8285 17201 11388 8738 10783 3881 9270 20143 12885 12276 9359 17082 597 8583 19427 15223 6018 21232 17658 14075 9614 19576 11799 4558 7435 9915 19944 15970 2549 9358 15535 16247 6028 32 2623 7695 8255 21060 21386 3449 17746 18619 7097 13633 6392 19925 21412 8238 17853 7508 14935 3088 2299 4093 14543 20145 9083 2646 8108 21689 20653 13708 15078 14395 14367 9060 18042 15835 2373 8494 13598 7800 16072 19674 5781 18279 14534 13643 8654 17021 15771 17342 13348 4618 16296 10852 13021 16803 5836 8913 11074 15854 748 6424 930 14743 9676 864 3365 6995 10393 18568 7455 10963 15846 20470 19204 19491 5745 14958 17887 1086 4806 5156 21785 6743 10661 3295 16258 14125 17553 4620 17031 21319 3571 17492 3878 16734 621 11502 8131 20588 565 17181 5834 6926 8283 21105 20036 21711 16298 1707 5818 7480 21029 13698 17125 19597 4659 17435 14369 20052 10272 12699 3650 20581 5545 12252 14866 3655 16813 13655 5143 8353 10786 15394 2925 10162 16056 8959 9677 15574 7624 18402 17413 16383 173 4007 8404 9820 11984 19494 1995 4405 3994 7030 9689 16184 3423 2745 8157 16068 14665 20474 3192 21615 4170 21890 9628 2191 15211 13585 2220 2380 17391 2665 12040 14454 2199 18750 4525 1089 14804 5873 17202 11958 5943 7406 2730 11930 714 5687 1146 8273 10581 18290 17303 4604 11422 834 5082 20489 14598 9259 19826 13546 1196 8164 21360 7219 14265 7357 5742 15142 10036 5652 4904 6463 16496 8847 5124 11329 14817 8121 227 9510 9223 5877 12186 11391 7145 6985 3125 11956 1284 21605 5258 4907 5798 4828 17257 21732 5521 10330 11036 7789 3526 21614 579 11744 20736 14201 152 723 871 17928 12775 12823 8750 10936 14535 20960 5694 3867 10270 16147 9934 5409 17645 11983 19871 5668 18321 6540 20532 19772 6339 3483 9898 16926 19331 2725 16764 14270 21118 18586 10916 14572 17262 5010 8480 14994 21542 2764 10667 20708 16767 14406 8922 7922 18819 21588 2407 20178 20164 15446 7556 20362 1313 3511 19886 13415 14322 2935 5269 10681 1871 442 12589 2141 12306 14204 11866 19586 15088 1411 11448 9826 19657 8483 9101 10844 21042 19835 13109 12369 5453 14974 10521 10344 5966 517 16975 14579 6532 20108 20053 1715 5389 7094 789 20702 5623 508 19361 11656 11418 21340 14784 19109 20719 12912 4364 17999 18055 5924 4273 15562 19133 21569 4474 21810 9819 6711 6727 13334 8553 10060 1186 17921 5882 18411 2214 9227 19503 11829 2154 5896 19764 14253 17866 3518 20761 1657 6635 2544 5643 21249 5538 8032 1763 5207 10192 15265 20445 14603 8358 21599 19722 9459 10551 10403 19658 14987 21083 14478 10149 13254 9006 17514 365 16576 15732 19186 19986 96 587 3304 19248 13277 15484 4297 20420 4657 13018 14820 20285 1527 21058 9059 18652 3670 14630 20082 744 7645 12311 5841 17868 1252 12479 2504 15424 20269 6877 16166 15754 5663 10288 14077 9381 78 1993 747 15492 4368 18618 17969 6893 21188 20475 4047 17814 10666 20182 21909 6980 11491 5508 13991 2768 10832 21520 3678 4574 9818 10099 16650 15176 18069 1825 2400 12831 13413 3561 2353 15783 1115 9126 9894 10999 8264 16688 124 11730 12191 17078 5182 15346 15995 4680 10284 797 8491 1567 13548 12682 4792 12161 21796 20338 8928 12214 12014 1222 11112 20513 16790 3767 11679 11723 17222 9825 14160 2894 7349 6734 19616 8869 6700 9094 7655 13705 17790 322 8524 12692 20655 17067 15500 13054 2957 13972 17228 5655 11994 20468 6848 12917 16561 15323 9150 9913 4624 14100 13746 19487 3126 4528 5499 14758 18656 12438 8111 11359 8225 12471 18662 4101 6307 12475 17580 16859 4977 1986 18311 20826 9548 18289 17176 4592 19388 6716 18908 14874 4239 13495 15837 355 12029 2669 17497 14299 21672 6809 4955 18792 12577 17676 14897 20139 497 10077 11315 3361 5582 20423 14858 14833 1656 3986 14103 11028 2898 17974 9262 10771 2815 20553 16227 14975 19685 10903 13875 6662 10362 9497 7780 7088 3775 654 21307 14073 18192 706 3381 13586 19423 16580 7156 2085 14278 20705 2281 10043 6105 15778 16622 3503 5390 9499 18287 16533 9279 1433 14641 971 3713 16846 20286 9306 17079 16179 12435 2618 1197 7974 4878 20801 10114 16285 8388 3367 16293 6915 6513 5370 121 12433 468 18455 17922 10457 12521 21581 10402 8670 67 16437 13712 19167 18211 12271 17036 21005 12383 8595 17809 1383 59 15669 15163 3056 19163 3044 6815 7582 4447 4702 3976 19757 21936 15103 336 16100 2461 20469 16221 19420 3568 13476 12043 6469 10673 15260 15890 9731 819 1233 3555 17212 13976 13196 17552 11281 21941 2209 15699 18548 6145 2260 17595 18704 467 14235 7714 6533 488 2932 20691 6109 9338 11815 19180 14288 14053 8311 7069 7292 10307 12480 14087 12496 20404 1104 11620 9111 8810 21692 9691 12833 18477 14013 9007 20584 19815 15739 9546 7111 11171 8930 6490 12273 20186 10249 1195 15885 1134 11843 7150 9922 19292 6155 3409 13263 7143 5022 12813 20915 19644 16288 455 21622 21901 8370 21067 11404 8760 9031 6019 1193 3885 8786 15944 13016 13391 390 18318 7018 4752 7932 9437 12380 7885 5405 3565 7047 18724 5118 14050 12114 3208 18543 15204 9545 6829 9778 18422 12634 19717 303 5293 1539 1191 13900 15924 20147 2972 3002 1875 17945 4167 19981 5042 13294 709 20500 6039 941 12647 20360 4179 15482 14739 1309 20047 18952 12463 12218 6042 14126 4855 18023 3917 10267 854 1850 15685 8326 5673 5040 5331 11847 3227 18800 5350 15882 328 2713 16172 17882 12185 16778 2548 21940 11936 3094 14392 10672 20021 15788 2304 13580 20791 15456 2131 10096 8270 16450 18850 1939 2039 21287 15936 12018 837 18731 7196 19159 8751 1526 482 6559 7740 13796 2726 5532 15668 19287 20278 6483 3557 9585 17863 7335 17101 15328 21330 3292 5158 15001 6879 4131 16316 10659 14732 134 7410 19623 1990 8478 12379 3197 11710 18756 7239 7330 18141 1430 7997 18138 2305 5044 8891 11412 15495 16608 18646 8060 5727 6485 10798 12585 9049 2049 20022 19343 19842 11283 12388 16554 4325 12346 14377 11247 3697 4887 21756 17160 7609 9822 2602 16020 12609 17051 17 14384 17633 8033 11123 13433 1814 1006 12261 3303 13124 15650 10144 14797 20966 13195 14289 11285 6315 12265 2205 18168 5596 11222 19123 16835 17386 17982 6619 1732 18143 15599 16276 8657 18969 14458 4962 12109 20940 18216 15805 15989 13911 7261 10639 8028 14321 21266 3560 12204 8543 3620 18294 8893 1499 2611 15838 20325 10791 20460 7195 4150 14852 5509 17111 276 15980 15848 432 14313 20743 20229 16687 772 20259 8354 21298 6298 8660 14904 17700 10382 15283 21349 16548 4915 20657 19506 961 1493 2486 10646 10630 12049 16529 18574 9194 11688 3805 15777 14945 21978 15774 8247 10483 4455 13970 13002 6986 6077 16235 14262 1480 15013 14754 11312 19055 12779 5720 9171 2670 9844 17846 2388 8231 19619 183 9936 15055 13014 21516 14717 10584 6996 11895 17271 16060 10280 15288 21617 15040 8821 721 98 16022 13181 10194 6594 18614 20379 8552 228 2228 19521 12240 8575 17093 11334 3075 6348 11135 13815 15325 1877 12176 5133 3639 16375 18550 5168 2956 11269 4323 13608 12732 3691 10463 16879 9587 2553 14181 12686 17749 5240 4275 18703 2829 21458 11995 27 6317 6663 3477 20590 18670 12205 1922 1521 18479 4240 5506 14846 9668 1819 15465 7954 1125 14978 3424 13192 3274 19994 7494 16101 688 18197 10909 5257 2574 2176 7085 14102 12437 4134 14438 6766 16623 8109 20378 5911 9673 20506 14083 8070 21801 1652 8672 7756 7481 7060 20390 12392 6139 12425 18395 21260 4666 11813 1111 6787 16532 14428 10343 9582 7274 5127 18498 5572 16217 7607 5991 16706 8025 12162 3629 4664 45 15756 1999 10724 19659 21050 13801 15682 16038 21302 12432 15264 21038 18531 4930 7105 21267 2150 3328 1911 12762 20401 2775 4683 12322 9361 18788 302 3319 9240 15343 19708 2550 20057 10524 14208 17297 1653 9938 13035 916 13012 20234 9435 6128 4699 19344 18857 3495 5543 19213 14216 21536 12983 12454 7939 16520 15172 1894 16574 18421 566 20033 4459 17941 11646 7703 7443 3279 20142 7476 330 2079 18939 8818 14768 21920 16295 2349 21874 15847 4097 6852 563 180 8677 13483 10566 20840 19985 6970 21877 9608 14483 13207 14388 3688 18040 9605 9003 17223 11008 2716 13327 7265 8230 4631 6089 20419 21352 5476 698 414 729 14493 10559 4205 8898 11682 7938 20054 13728 13602 6535 6261 13699 9167 7968 14668 1047 2442 18217 17629 18781 17734 7845 9110 9330 17235 8171 14085 14822 16577 4707 11177 19026 12660 15073 17815 16354 16890 15517 2535 10150 16714 5448 17277 5221 1461 15961 329 13108 19588 3740 9852 8469 18093 9557 17407 12651 19607 4501 6775 19829 10569 21662 4432 16065 640 9601 5253 17661 421 20875 602 3694 5819 15816 19526 14166 21157 3210 16380 4039 6147 6750 14983 6190 8460 3163 8793 618 12456 15481 7149 10490 18636 10412 12570 10489 8470 9734 2492 7729 11777 15427 8662 20031 14494 14861 13142 509 17923 14679 19761 4705 8834 4 7260 578 4444 21210 2208 1517 6029 16 17972 15914 8778 15120 20684 4636 5090 3700 12308 794 9178 14082 15491 15149 18678 4089 8768 16924 7405 20794 6182 5312 21567 20963 8998 8841 9715 17987 9415 8431 12587 3558 7529 19315 9456 1198 13966 8876 13383 13365 16192 34 14988 17337 6363 20783 15606 14850 3545 20748 10774 19878 18901 12620 20632 18324 15963 20037 16099 8041 16199 13817 14140 18413 12845 13098 14714 14501 3420 15828 9795 10855 12836 6068 20686 2642 21951 3963 7315 17538 8029 16526 7209 8526 21239 16121 5005 9393 5021 14898 3076 4243 13610 10069 6878 8716 11309 12313 21993 14549 8427 13230 10795 11052 9321 6300 17865 19115 14174 2653 3551 5879 12102 2327 21846 2545 1327 5065 9610 9659 9669 16913 10817 15551 12042 9297 5595 6631 2368 7924 5829 17881 9018 21288 11661 13975 17481 5153 15087 18483 2265 8191 7385 17117 9467 16768 3498 16949 18914 2890 2615 9189 5226 7542 1710 38 2436 11500 1466 331 4753 1513 12337 16810 8943 16452 13247 2621 4763 2138 801 12451 21085 2599 4449 20013 21976 17045 7471 20457 18328 18863 20006 18314 5416 9412 20452 1224 6568 17730 8730 10940 7827 17967 8884 3725 4231 324 6277 13171 10399 9106 20424 7686 1236 21938 12231 5940 21556 11971 14095 18273 20306 9769 16587 6581 2940 8227 1253 6807 9893 7675 21015 10237 19990 1629 19365 7257 19480 1005 18810 2593 4823 12371 5656 20607 21121 17456 7015 13353 15623 8300 20098 16927 14036 15256 20492 6746 20442 19663 7674 3447 2605 3300 9093 19779 4774 11548 2399 6324 4643 10478 15545 12811 3008 8449 1219 4201 3329 948 1391 14274 1632 18587 8794 1038 9009 14027 5024 11355 3982 4803 976 11770 17024 6270 782 11071 21159 7951 16460 8798 6690 20587 19107 13170 15235 21881 15020 21289 952 3272 17774 14971 10202 84 4429 12638 3964 9138 13827 16519 1024 10119 3689 12131 16739 2780 4468 17584 13879 4302 4295 5753 21800 12953 12861 6808 7115 10339 19065 6859 9575 16693 2280 10390 9897 6052 17515 18979 18174 3708 9360 1778 17165 10873 17469 5599 8823 298 4337 10758 21970 17631 13861 7691 18603 1388 11802 2805 14200 11709 11352 18764 15086 10694 1120 2450 15489 15301 10221 7076 15858 12722 15977 14290 6113 3266 14948 14728 6252 4358 17047 15360 3671 8004 5170 7519 5353 1076 14993 12751 18067 14729 8537 4553 5904 17726 21823 6480 21192 12997 11871 6093 15496 11686 1335 18569 21607 20093 3317 14487 18269 12841 5113 5277 17768 10297 18184 16992 21488 9209 8325 6922 4286 19445 5201 2378 887 14086 18799 11150 10216 11424 1178 20484 4438 20012 15524 12348 14175 21602 10027 3794 8066 85 21875 10979 11261 8329 15633 14873 21687 20638 5822 11868 2913 17883 1374 2990 13079 13561 18459 6735 2511 13813 1112 5392 19498 21723 20825 21137 20443 7746 4484 11780 10930 6405 278 20120 16457 2316 12925 10796 999 4974 15621 11217 21673 9354 6048 10209 2911 17095 7820 1777 14757 17555 11980 14110 12464 1587 169 16750 6708 21637 15930 7527 17867 15998 18481 18406 16119 6956 8276 1584 20304 12644 16919 249 13536 2579 18706 21304 6439 21265 12332 20449 10606 9103 18708 11934 4080 6481 21369 6294 2714 18814 4071 11040 6066 10508 10058 11621 13132 4658 18293 4914 10452 13446 8721 8249 3522 10021 3523 10281 21886 10245 5046 17880 2950 9335 6101 9861 3702 17858 20199 4731 1886 3442 6238 3062 13591 2 5231 9288 1299 13882 3368 2086 17033 17269 21018 21590 9098 6353 20440 7700 14244 7543 9454 16411 14654 713 7964 20729 3336 2148 7784 10544 10031 14726 2729 15432 9799 7803 16501 8802 17489 19631 20322 191 150 16888 6350 3632 12780 11256 7057 13048 12021 3185 8009 19280 14970 21717 8982 18982 8692 3286 15636 2543 9998 9995 3677 4529 18142 7457 7263 8429 15034 11031 15923 6453 8323 14492 18486 11026 17218 20548 12649 21994 8160 15237 9888 4236 20546 21008 1932 14302 4964 13964 8565 21274 7982 6882 11444 19545 10063 19165 9099 1607 1831 8254 8789 7139 2315 10693 11300 17240 5061 7193 21923 17578 20935 5934 7844 10621 3501 6421 11732 17004 8808 15344 17199 3116 16320 6602 9301 11913 20695 2234 18992 4947 12777 5275 11188 15753 18943 9773 13717 1760 9625 14139 14280 21125 15112 15747 1543 4255 2170 5131 18815 12004 16665 7699 21325 7401 12078 1714 9159 18743 10218 10577 21558 6382 1558 11364 11552 10310 18594 2927 13425 17783 9722 5074 10098 11902 4393 7515 21012 17832 13926 12352 18616 7377 17496 14432 17805 16348 17640 5651 9916 20024 1215 12121 5644 21980 8046 6542 10082 14790 3804 13166 11629 17677 21331 21167 17995 2421 1423 13471 19741 12452 8500 3152 8981 2468 18458 4441 6678 6896 13304 12139 11795 19139 8177 8015 15206 11204 1603 15370 18789 380 19127 15810 16039 17706 12690 20018 14609 3013 3164 21318 2212 14706 5388 16692 19850 4696 11781 10410 10349 8209 14123 12188 8801 4052 15161 7432 9642 15050 18998 18667 19242 8163 12790 10742 14907 18186 14642 8278 8127 20201 964 2235 9985 9960 593 20668 7326 11145 13955 15057 9127 1048 21573 9128 12540 914 12945 14571 1873 5851 5071 12244 7358 5665 9480 5969 6086 8280 18024 18920 16299 16472 14010 7205 3150 2010 7073 2752 17581 15374 7070 2439 20270 20260 9464 1952 20310 8072 11625 3968 2999 16252 19431 16610 10654 1286 5057 646 12304 21462 12286 6037 15183 21126 3355 9706 17904 1884 21481 9351 466 6347 21345 892 6188 3478 7919 6623 18960 3607 3276 5256 1709 15267 7036 3747 21471 456 19922 840 16454 5583 5967 15706 4687 2408 5430 19370 10679 5190 11650 11037 10695 8344 3524 1399 11769 2352 19553 17099 18352 17296 1828 11400 15462 7786 19782 19513 21217 16182 21022 12926 11836 5068 19007 21958 16562 12445 5951 561 1810 3660 7817 21079 12808 21744 16894 12981 7181 6936 7033 17016 4483 11479 13198 21447 1034 10598 15463 15293 9823 14759 10755 1317 17504 9193 1453 2289 19621 5049 18735 17457 770 15575 16269 9880 1220 19014 1769 4646 4336 18679 21421 15522 2656 21134 19897 4973 19501 18922 1208 12967 20040 4691 1378 4055 13941 2438 14554 3566 12871 19398 393 13429 3807 9591 17614 19523 8217 12264 18931 2686 6748 20441 17613 14772 21755 7767 8690 20899 18989 5357 19787 20630 14788 10395 14655 12930 13842 7316 7121 18476 21394 5008 8099 15318 12673 15060 10691 2755 14666 1359 21189 10453 9550 14269 19098 1658 9558 344 17168 18415 9634 19745 11307 12401 5117 15587 11778 6800 3548 20480 8376 8720 11761 463 16363 9635 6044 16109 16611 11263 16850 9674 12510 7996 14848 18911 8340 1693 15532 20906 16735 8685 18875 6258 10135 304 18250 14294 8642 10769 16905 2632 5031 11727 20514 21724 3093 17395 15563 11627 14165 6661 10864 18632 8180 10274 10700 21813 5426 12761 6309 19460 4996 7166 19223 10748 2009 14599 15543 12576 13043 5404 1420 18664 195 10473 21279 11801 16781 11106 9078 8147 19563 9198 17378 7585 4277 7657 14227 1211 5102 9967 20698 8035 2732 15709 7372 1379 809 631 9207 15645 11336 19048 5691 16998 737 3198 14308 20974 20687 1247 6181 16120 4896 5387 21752 11720 12581 4603 17141 3221 16187 17539 17108 10699 1998 11929 7917 10536 21868 14380 8474 5703 2817 3579 13453 18380 5200 13607 7420 13922 7226 9533 20739 2455 6965 13023 21570 12941 6417 19646 4328 5412 4670 17905 18274 3987 14343 4338 17593 10990 17112 9511 4149 16425 19468 20558 5784 4422 9370 18749 21135 19766 289 10684 19208 18359 10357 19493 21720 8717 3614 17695 1610 20491 13232 13469 21378 4526 9782 13985 8122 8984 11569 412 19633 13217 6367 13737 16190 17090 2977 7896 16809 17252 4851 16084 874 11755 11592 3952 5210 15950 7790 5638 13943 7192 230 12853 8366 16211 11533 14435 7285 400 6416 15409 11371 15018 16079 18414 7155 6541 12550 1226 18648 14745 14203 21071 10386 6937 3184 6923 8199 1154 5363 20395 14192 11588 4374 21506 7812 12544 6830 5123 10861 13082 17482 20754 9594 10799 2590 617 803 15716 12370 21997 14451 21461 13106 490 16210 9640 19209 13243 6747 1702 15697 21000 19737 17397 5300 14839 6697 21065 17516 21947 9572 1351 9258 20594 7711 4271 18450 9232 6476 2586 15289 7959 10389 10420 17681 9712 10591 13447 2571 11891 15026 16393 6330 246 3223 5351 3051 3753 14481 20461 10178 21184 9925 20324 4017 21651 13766 8262 11812 11105 10206 2576 2334 9427 13260 3112 16483 13615 10597 1592 15292 18117 16637 12913 681 10084 19228 4490 5017 2312 6079 15061 6255 9028 19720 897 15737 18793 16388 11357 14697 14844 9224 5151 15762 20715 4029 2218 14345 12278 8622 2973 14167 2530 7719 4319 5876 6584 5920 18241 9576 14382 17668 4270 13560 17505 5716 11598 8194 1269 6306 16367 2077 20167 1476 1515 7388 7198 11943 20218 17194 20472 17398 1554 12485 20856 6713 1674 17162 18513 20075 5195 14092 1124 13756 16763 11716 9220 1791 8144 16652 15458 11185 17334 4522 17733 19092 12315 11581 16167 5759 4460 19820 21155 6695 14775 4545 7375 5838 2075 11574 15069 3696 16089 7918 8547 20968 5849 5785 5739 11366 2426 14939 16662 5835 10860 15893 5813 20122 17800 20413 7871 6892 1577 13133 15124 15662 16537 2879 15502 14741 16974 21632 20757 14882 878 8809 8001 15420 4469 7874 16394 11455 2033 4404 4259 10824 16154 1910 14423 2482 15107 374 12197 18122 10506 11654 10168 4600 12758 5914 3225 14856 595 4675 10352 20870 19304 8435 16977 10365 13481 15077 6458 4461 5396 7795 16825 16639 14231 20046 11594 15384 18121 6983 7186 8560 17678 8196 12990 5520 8138 18986 18942 2257 253 6183 2431 10922 10258 10229 6357 5830 7206 20734 2794 9182 8352 3004 12237 19796 1679 19573 1816 8653 9004 12791 12285 7920 6433 4398 10164 277 7926 10946 13579 12958 1015 4967 8827 11425 13244 16026 3502 18883 8852 3130 6301 19590 17081 826 13748 375 14823 2449 10601 20241 18025 255 796 16558 5455 51 12804 8977 15419 11572 21971 17937 20083 18304 3005 5878 9176 19061 13739 950 11740 10562 1951 19285 806 5030 19038 1165 16264 8978 8425 6241 2518 1716 11322 16593 11800 15579 18539 19307 19824 17427 4268 14615 12759 1316 12005 3830 16489 6045 9968 5679 20242 3886 4965 2987 20718 11434 19867 11167 18541 1785 5518 659 30 15564 15305 7159 1546 9917 3667 10730 2846 9280 13316 17525 20291 19511 20209 13657 1356 5486 12064 1025 5588 3913 2175 19323 786 12702 3715 8849 7654 16254 18868 6207 4880 11097 1560 18228 12135 10996 6615 1583 1611 9760 6531 1958 5522 115I0814 08:07:36.503315 28599 custom_data_layer.cpp:541] output 0 data size: 4,3,540,960
I0814 08:07:36.503353 28599 custom_data_layer.cpp:541] output 1 data size: 4,3,540,960
I0814 08:07:36.503374 28599 custom_data_layer.cpp:541] output 2 data size: 4,1,540,960
I0814 08:07:36.529762 28599 net.cpp:144] Setting up CustomData1
I0814 08:07:36.529814 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.529819 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.529822 28599 net.cpp:151] Top shape: 4 1 540 960 (2073600)
I0814 08:07:36.529824 28599 net.cpp:159] Memory required for data: 58060800
I0814 08:07:36.529831 28599 layer_factory.hpp:77] Creating layer blob0_CustomData1_0_split
I0814 08:07:36.529847 28599 net.cpp:91] Creating Layer blob0_CustomData1_0_split
I0814 08:07:36.529852 28599 net.cpp:432] blob0_CustomData1_0_split <- blob0
I0814 08:07:36.529861 28599 net.cpp:404] blob0_CustomData1_0_split -> blob0_CustomData1_0_split_0
I0814 08:07:36.529871 28599 net.cpp:404] blob0_CustomData1_0_split -> blob0_CustomData1_0_split_1
I0814 08:07:36.529932 28599 net.cpp:144] Setting up blob0_CustomData1_0_split
I0814 08:07:36.529937 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.529940 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.529942 28599 net.cpp:159] Memory required for data: 107827200
I0814 08:07:36.529945 28599 layer_factory.hpp:77] Creating layer blob1_CustomData1_1_split
I0814 08:07:36.529950 28599 net.cpp:91] Creating Layer blob1_CustomData1_1_split
I0814 08:07:36.529953 28599 net.cpp:432] blob1_CustomData1_1_split <- blob1
I0814 08:07:36.529956 28599 net.cpp:404] blob1_CustomData1_1_split -> blob1_CustomData1_1_split_0
I0814 08:07:36.529961 28599 net.cpp:404] blob1_CustomData1_1_split -> blob1_CustomData1_1_split_1
I0814 08:07:36.529983 28599 net.cpp:144] Setting up blob1_CustomData1_1_split
I0814 08:07:36.529986 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.529989 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.529991 28599 net.cpp:159] Memory required for data: 157593600
I0814 08:07:36.529994 28599 layer_factory.hpp:77] Creating layer blob2_CustomData1_2_split
I0814 08:07:36.529997 28599 net.cpp:91] Creating Layer blob2_CustomData1_2_split
I0814 08:07:36.530000 28599 net.cpp:432] blob2_CustomData1_2_split <- blob2
I0814 08:07:36.530004 28599 net.cpp:404] blob2_CustomData1_2_split -> blob2_CustomData1_2_split_0
I0814 08:07:36.530007 28599 net.cpp:404] blob2_CustomData1_2_split -> blob2_CustomData1_2_split_1
I0814 08:07:36.530030 28599 net.cpp:144] Setting up blob2_CustomData1_2_split
I0814 08:07:36.530051 28599 net.cpp:151] Top shape: 4 1 540 960 (2073600)
I0814 08:07:36.530055 28599 net.cpp:151] Top shape: 4 1 540 960 (2073600)
I0814 08:07:36.530056 28599 net.cpp:159] Memory required for data: 174182400
I0814 08:07:36.530059 28599 layer_factory.hpp:77] Creating layer Eltwise1
I0814 08:07:36.530069 28599 net.cpp:91] Creating Layer Eltwise1
I0814 08:07:36.530072 28599 net.cpp:432] Eltwise1 <- blob0_CustomData1_0_split_0
I0814 08:07:36.530077 28599 net.cpp:404] Eltwise1 -> blob3
I0814 08:07:36.530099 28599 net.cpp:144] Setting up Eltwise1
I0814 08:07:36.530103 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.530105 28599 net.cpp:159] Memory required for data: 199065600
I0814 08:07:36.530107 28599 layer_factory.hpp:77] Creating layer blob3_Eltwise1_0_split
I0814 08:07:36.530112 28599 net.cpp:91] Creating Layer blob3_Eltwise1_0_split
I0814 08:07:36.530113 28599 net.cpp:432] blob3_Eltwise1_0_split <- blob3
I0814 08:07:36.530118 28599 net.cpp:404] blob3_Eltwise1_0_split -> blob3_Eltwise1_0_split_0
I0814 08:07:36.530122 28599 net.cpp:404] blob3_Eltwise1_0_split -> blob3_Eltwise1_0_split_1
I0814 08:07:36.530141 28599 net.cpp:144] Setting up blob3_Eltwise1_0_split
I0814 08:07:36.530144 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.530148 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.530149 28599 net.cpp:159] Memory required for data: 248832000
I0814 08:07:36.530153 28599 layer_factory.hpp:77] Creating layer Eltwise2
I0814 08:07:36.530158 28599 net.cpp:91] Creating Layer Eltwise2
I0814 08:07:36.530160 28599 net.cpp:432] Eltwise2 <- blob1_CustomData1_1_split_0
I0814 08:07:36.530164 28599 net.cpp:404] Eltwise2 -> blob4
I0814 08:07:36.530175 28599 net.cpp:144] Setting up Eltwise2
I0814 08:07:36.530179 28599 net.cpp:151] Top shape: 4 3 540 960 (6220800)
I0814 08:07:36.530181 28599 net.cpp:159] Memory required for data: 273715200
I0814 08:07:36.530184 28599 layer_factory.hpp:77] Creating layer img0s_aug
I0814 08:07:36.530199 28599 net.cpp:91] Creating Layer img0s_aug
I0814 08:07:36.530201 28599 net.cpp:432] img0s_aug <- blob3_Eltwise1_0_split_0
I0814 08:07:36.530207 28599 net.cpp:404] img0s_aug -> img0_aug
I0814 08:07:36.530217 28599 net.cpp:404] img0s_aug -> blob6
W0814 08:07:36.530225 28599 data_augmentation_layer.cpp:40] DataAugmentationLayer only runs Reshape on setup
I0814 08:07:36.530232 28599 data_augmentation_layer.cpp:45] Recompute mean
W0814 08:07:36.530349 28599 data_augmentation_layer.cpp:77] Reshape of Augmentation layer should only be called once? Check this
I0814 08:07:36.530387 28599 data_augmentation_layer.cpp:123] Emitting 42 augmentation params
I0814 08:07:36.530977 28599 net.cpp:144] Setting up img0s_aug
I0814 08:07:36.530988 28599 net.cpp:151] Top shape: 4 3 384 768 (3538944)
I0814 08:07:36.530992 28599 net.cpp:151] Top shape: 4 42 1 1 (168)
I0814 08:07:36.530994 28599 net.cpp:159] Memory required for data: 287871648
I0814 08:07:36.531006 28599 layer_factory.hpp:77] Creating layer img0_aug_img0s_aug_0_split
I0814 08:07:36.531013 28599 net.cpp:91] Creating Layer img0_aug_img0s_aug_0_split
I0814 08:07:36.531016 28599 net.cpp:432] img0_aug_img0s_aug_0_split <- img0_aug
I0814 08:07:36.531020 28599 net.cpp:404] img0_aug_img0s_aug_0_split -> img0_aug_img0s_aug_0_split_0
I0814 08:07:36.531026 28599 net.cpp:404] img0_aug_img0s_aug_0_split -> img0_aug_img0s_aug_0_split_1
I0814 08:07:36.531049 28599 net.cpp:144] Setting up img0_aug_img0s_aug_0_split
I0814 08:07:36.531052 28599 net.cpp:151] Top shape: 4 3 384 768 (3538944)
I0814 08:07:36.531056 28599 net.cpp:151] Top shape: 4 3 384 768 (3538944)
I0814 08:07:36.531059 28599 net.cpp:159] Memory required for data: 316183200
I0814 08:07:36.531061 28599 layer_factory.hpp:77] Creating layer blob6_img0s_aug_1_split
I0814 08:07:36.531065 28599 net.cpp:91] Creating Layer blob6_img0s_aug_1_split
I0814 08:07:36.531067 28599 net.cpp:432] blob6_img0s_aug_1_split <- blob6
I0814 08:07:36.531072 28599 net.cpp:404] blob6_img0s_aug_1_split -> blob6_img0s_aug_1_split_0
I0814 08:07:36.531076 28599 net.cpp:404] blob6_img0s_aug_1_split -> blob6_img0s_aug_1_split_1
I0814 08:07:36.531105 28599 net.cpp:144] Setting up blob6_img0s_aug_1_split
I0814 08:07:36.531117 28599 net.cpp:151] Top shape: 4 42 1 1 (168)
I0814 08:07:36.531121 28599 net.cpp:151] Top shape: 4 42 1 1 (168)
I0814 08:07:36.531122 28599 net.cpp:159] Memory required for data: 316184544
I0814 08:07:36.531126 28599 layer_factory.hpp:77] Creating layer aug_params1
I0814 08:07:36.531136 28599 net.cpp:91] Creating Layer aug_params1
I0814 08:07:36.531138 28599 net.cpp:432] aug_params1 <- blob6_img0s_aug_1_split_0
I0814 08:07:36.531141 28599 net.cpp:432] aug_params1 <- blob3_Eltwise1_0_split_1
I0814 08:07:36.531144 28599 net.cpp:432] aug_params1 <- img0_aug_img0s_aug_0_split_0
I0814 08:07:36.531153 28599 net.cpp:404] aug_params1 -> blob7
W0814 08:07:36.531160 28599 generate_augmentation_parameters_layer.cpp:36] GenerateAugmentationParametersLayer only runs Reshape only on setup
I0814 08:07:36.531167 28599 generate_augmentation_parameters_layer.cpp:68] mode: add
I0814 08:07:36.531179 28599 net.cpp:144] Setting up aug_params1
I0814 08:07:36.531183 28599 net.cpp:151] Top shape: 4 42 1 1 (168)
I0814 08:07:36.531185 28599 net.cpp:159] Memory required for data: 316185216
I0814 08:07:36.531188 28599 layer_factory.hpp:77] Creating layer blob7_aug_params1_0_split
I0814 08:07:36.531191 28599 net.cpp:91] Creating Layer blob7_aug_params1_0_split
I0814 08:07:36.531194 28599 net.cpp:432] blob7_aug_params1_0_split <- blob7
I0814 08:07:36.531198 28599 net.cpp:404] blob7_aug_params1_0_split -> blob7_aug_params1_0_split_0
I0814 08:07:36.531203 28599 net.cpp:404] blob7_aug_params1_0_split -> blob7_aug_params1_0_split_1
I0814 08:07:36.531222 28599 net.cpp:144] Setting up blob7_aug_params1_0_split
I0814 08:07:36.531226 28599 net.cpp:151] Top shape: 4 42 1 1 (168)
I0814 08:07:36.531229 28599 net.cpp:151] Top shape: 4 42 1 1 (168)
I0814 08:07:36.531231 28599 net.cpp:159] Memory required for data: 316186560
I0814 08:07:36.531234 28599 layer_factory.hpp:77] Creating layer img1s_aug
I0814 08:07:36.531239 28599 net.cpp:91] Creating Layer img1s_aug
I0814 08:07:36.531241 28599 net.cpp:432] img1s_aug <- blob4
I0814 08:07:36.531244 28599 net.cpp:432] img1s_aug <- blob7_aug_params1_0_split_0
I0814 08:07:36.531249 28599 net.cpp:404] img1s_aug -> img1_aug
W0814 08:07:36.531253 28599 data_augmentation_layer.cpp:40] DataAugmentationLayer only runs Reshape on setup
I0814 08:07:36.531257 28599 data_augmentation_layer.cpp:45] Recompute mean
W0814 08:07:36.531298 28599 data_augmentation_layer.cpp:77] Reshape of Augmentation layer should only be called once? Check this
I0814 08:07:36.531316 28599 data_augmentation_layer.cpp:116] Receiving 42 augmentation params
I0814 08:07:36.531805 28599 net.cpp:144] Setting up img1s_aug
I0814 08:07:36.531812 28599 net.cpp:151] Top shape: 4 3 384 768 (3538944)
I0814 08:07:36.531816 28599 net.cpp:159] Memory required for data: 330342336
I0814 08:07:36.531822 28599 layer_factory.hpp:77] Creating layer DummyData1
I0814 08:07:36.531831 28599 net.cpp:91] Creating Layer DummyData1
I0814 08:07:36.531836 28599 net.cpp:404] DummyData1 -> blob9
I0814 08:07:36.535568 28599 net.cpp:144] Setting up DummyData1
I0814 08:07:36.535599 28599 net.cpp:151] Top shape: 4 1 540 960 (2073600)
I0814 08:07:36.535603 28599 net.cpp:159] Memory required for data: 338636736
I0814 08:07:36.535607 28599 layer_factory.hpp:77] Creating layer blob9_DummyData1_0_split
I0814 08:07:36.535615 28599 net.cpp:91] Creating Layer blob9_DummyData1_0_split
I0814 08:07:36.535619 28599 net.cpp:432] blob9_DummyData1_0_split <- blob9
I0814 08:07:36.535625 28599 net.cpp:404] blob9_DummyData1_0_split -> blob9_DummyData1_0_split_0
I0814 08:07:36.535632 28599 net.cpp:404] blob9_DummyData1_0_split -> blob9_DummyData1_0_split_1
I0814 08:07:36.535686 28599 net.cpp:144] Setting up blob9_DummyData1_0_split
I0814 08:07:36.535689 28599 net.cpp:151] Top shape: 4 1 540 960 (2073600)
I0814 08:07:36.535692 28599 net.cpp:151] Top shape: 4 1 540 960 (2073600)
I0814 08:07:36.535694 28599 net.cpp:159] Memory required for data: 355225536
I0814 08:07:36.535713 28599 layer_factory.hpp:77] Creating layer Concat1
I0814 08:07:36.535724 28599 net.cpp:91] Creating Layer Concat1
I0814 08:07:36.535727 28599 net.cpp:432] Concat1 <- blob2_CustomData1_2_split_0
I0814 08:07:36.535732 28599 net.cpp:432] Concat1 <- blob9_DummyData1_0_split_0
I0814 08:07:36.535737 28599 net.cpp:404] Concat1 -> blob10
I0814 08:07:36.535753 28599 net.cpp:144] Setting up Concat1
I0814 08:07:36.535756 28599 net.cpp:151] Top shape: 4 2 540 960 (4147200)
I0814 08:07:36.535759 28599 net.cpp:159] Memory required for data: 371814336
I0814 08:07:36.535761 28599 layer_factory.hpp:77] Creating layer FlowAugmentation1
I0814 08:07:36.535773 28599 net.cpp:91] Creating Layer FlowAugmentation1
I0814 08:07:36.535775 28599 net.cpp:432] FlowAugmentation1 <- blob10
I0814 08:07:36.535778 28599 net.cpp:432] FlowAugmentation1 <- blob6_img0s_aug_1_split_1
I0814 08:07:36.535781 28599 net.cpp:432] FlowAugmentation1 <- blob7_aug_params1_0_split_1
I0814 08:07:36.535786 28599 net.cpp:404] FlowAugmentation1 -> blob11
W0814 08:07:36.535791 28599 flow_augmentation_layer.cpp:36] FlowAugmentationLayer only runs Reshape only on setup
I0814 08:07:36.535825 28599 net.cpp:144] Setting up FlowAugmentation1
I0814 08:07:36.535830 28599 net.cpp:151] Top shape: 4 2 384 768 (2359296)
I0814 08:07:36.535833 28599 net.cpp:159] Memory required for data: 381251520
I0814 08:07:36.535835 28599 layer_factory.hpp:77] Creating layer Slice1
I0814 08:07:36.535845 28599 net.cpp:91] Creating Layer Slice1
I0814 08:07:36.535846 28599 net.cpp:432] Slice1 <- blob11
I0814 08:07:36.535851 28599 net.cpp:404] Slice1 -> disp_gt_aug
I0814 08:07:36.535857 28599 net.cpp:404] Slice1 -> blob13
I0814 08:07:36.535879 28599 net.cpp:144] Setting up Slice1
I0814 08:07:36.535883 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.535886 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.535888 28599 net.cpp:159] Memory required for data: 390688704
I0814 08:07:36.535890 28599 layer_factory.hpp:77] Creating layer ref_disp_scale
I0814 08:07:36.535897 28599 net.cpp:91] Creating Layer ref_disp_scale
I0814 08:07:36.535900 28599 net.cpp:404] ref_disp_scale -> ref_disp_scale
I0814 08:07:36.536366 28599 net.cpp:144] Setting up ref_disp_scale
I0814 08:07:36.536375 28599 net.cpp:151] Top shape: 1 4 96 192 (73728)
I0814 08:07:36.536377 28599 net.cpp:159] Memory required for data: 390983616
I0814 08:07:36.536381 28599 layer_factory.hpp:77] Creating layer ref_disp_scale_half
I0814 08:07:36.536386 28599 net.cpp:91] Creating Layer ref_disp_scale_half
I0814 08:07:36.536391 28599 net.cpp:404] ref_disp_scale_half -> ref_disp_scale_half
I0814 08:07:36.536851 28599 net.cpp:144] Setting up ref_disp_scale_half
I0814 08:07:36.536859 28599 net.cpp:151] Top shape: 1 4 192 384 (294912)
I0814 08:07:36.536861 28599 net.cpp:159] Memory required for data: 392163264
I0814 08:07:36.536864 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm
I0814 08:07:36.536870 28599 net.cpp:91] Creating Layer disp_gt_aug_norm
I0814 08:07:36.536873 28599 net.cpp:432] disp_gt_aug_norm <- disp_gt_aug
I0814 08:07:36.536878 28599 net.cpp:404] disp_gt_aug_norm -> disp_gt_aug_norm
I0814 08:07:36.536900 28599 net.cpp:144] Setting up disp_gt_aug_norm
I0814 08:07:36.536905 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.536906 28599 net.cpp:159] Memory required for data: 396881856
I0814 08:07:36.536909 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm_disp_gt_aug_norm_0_split
I0814 08:07:36.536913 28599 net.cpp:91] Creating Layer disp_gt_aug_norm_disp_gt_aug_norm_0_split
I0814 08:07:36.536916 28599 net.cpp:432] disp_gt_aug_norm_disp_gt_aug_norm_0_split <- disp_gt_aug_norm
I0814 08:07:36.536921 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_0
I0814 08:07:36.536926 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_1
I0814 08:07:36.536931 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_2
I0814 08:07:36.536942 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_3
I0814 08:07:36.536947 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_4
I0814 08:07:36.536988 28599 net.cpp:144] Setting up disp_gt_aug_norm_disp_gt_aug_norm_0_split
I0814 08:07:36.536991 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.536994 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.536998 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.537000 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.537003 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.537005 28599 net.cpp:159] Memory required for data: 420474816
I0814 08:07:36.537008 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm_down
I0814 08:07:36.537011 28599 net.cpp:91] Creating Layer disp_gt_aug_norm_down
I0814 08:07:36.537014 28599 net.cpp:432] disp_gt_aug_norm_down <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_0
I0814 08:07:36.537019 28599 net.cpp:432] disp_gt_aug_norm_down <- ref_disp_scale
I0814 08:07:36.537022 28599 net.cpp:404] disp_gt_aug_norm_down -> disp_gt_aug_norm_down
W0814 08:07:36.537027 28599 downsample_layer.cpp:25] DownsampleLayer only runs Reshape on setup
I0814 08:07:36.537041 28599 net.cpp:144] Setting up disp_gt_aug_norm_down
I0814 08:07:36.537045 28599 net.cpp:151] Top shape: 4 1 96 192 (73728)
I0814 08:07:36.537048 28599 net.cpp:159] Memory required for data: 420769728
I0814 08:07:36.537050 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split
I0814 08:07:36.537055 28599 net.cpp:91] Creating Layer disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split
I0814 08:07:36.537056 28599 net.cpp:432] disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split <- disp_gt_aug_norm_down
I0814 08:07:36.537062 28599 net.cpp:404] disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split -> disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split_0
I0814 08:07:36.537066 28599 net.cpp:404] disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split -> disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split_1
I0814 08:07:36.537086 28599 net.cpp:144] Setting up disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split
I0814 08:07:36.537088 28599 net.cpp:151] Top shape: 4 1 96 192 (73728)
I0814 08:07:36.537091 28599 net.cpp:151] Top shape: 4 1 96 192 (73728)
I0814 08:07:36.537093 28599 net.cpp:159] Memory required for data: 421359552
I0814 08:07:36.537096 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm_down_half
I0814 08:07:36.537101 28599 net.cpp:91] Creating Layer disp_gt_aug_norm_down_half
I0814 08:07:36.537102 28599 net.cpp:432] disp_gt_aug_norm_down_half <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_1
I0814 08:07:36.537106 28599 net.cpp:432] disp_gt_aug_norm_down_half <- ref_disp_scale_half
I0814 08:07:36.537109 28599 net.cpp:404] disp_gt_aug_norm_down_half -> disp_gt_aug_norm_down_half
W0814 08:07:36.537113 28599 downsample_layer.cpp:25] DownsampleLayer only runs Reshape on setup
I0814 08:07:36.537127 28599 net.cpp:144] Setting up disp_gt_aug_norm_down_half
I0814 08:07:36.537129 28599 net.cpp:151] Top shape: 4 1 192 384 (294912)
I0814 08:07:36.537132 28599 net.cpp:159] Memory required for data: 422539200
I0814 08:07:36.537134 28599 layer_factory.hpp:77] Creating layer upsample_disparity_itr1_half
I0814 08:07:36.537138 28599 net.cpp:91] Creating Layer upsample_disparity_itr1_half
I0814 08:07:36.537142 28599 net.cpp:432] upsample_disparity_itr1_half <- disp_gt_aug_norm_down_half
I0814 08:07:36.537147 28599 net.cpp:404] upsample_disparity_itr1_half -> upsampled_disparity_itr1_half
W0814 08:07:36.537153 28599 resample_layer.cpp:28] ResampleLayer only runs Reshape on setup
I0814 08:07:36.537165 28599 net.cpp:144] Setting up upsample_disparity_itr1_half
I0814 08:07:36.537168 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.537170 28599 net.cpp:159] Memory required for data: 427257792
I0814 08:07:36.537173 28599 layer_factory.hpp:77] Creating layer upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split
I0814 08:07:36.537181 28599 net.cpp:91] Creating Layer upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split
I0814 08:07:36.537184 28599 net.cpp:432] upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split <- upsampled_disparity_itr1_half
I0814 08:07:36.537189 28599 net.cpp:404] upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split -> upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split_0
I0814 08:07:36.537192 28599 net.cpp:404] upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split -> upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split_1
I0814 08:07:36.537211 28599 net.cpp:144] Setting up upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split
I0814 08:07:36.537215 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.537219 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.537220 28599 net.cpp:159] Memory required for data: 436694976
I0814 08:07:36.537222 28599 layer_factory.hpp:77] Creating layer Silence1
I0814 08:07:36.537230 28599 net.cpp:91] Creating Layer Silence1
I0814 08:07:36.537233 28599 net.cpp:432] Silence1 <- blob0_CustomData1_0_split_1
I0814 08:07:36.537236 28599 net.cpp:144] Setting up Silence1
I0814 08:07:36.537238 28599 net.cpp:159] Memory required for data: 436694976
I0814 08:07:36.541038 28599 layer_factory.hpp:77] Creating layer Silence2
I0814 08:07:36.541054 28599 net.cpp:91] Creating Layer Silence2
I0814 08:07:36.541057 28599 net.cpp:432] Silence2 <- blob1_CustomData1_1_split_1
I0814 08:07:36.541061 28599 net.cpp:144] Setting up Silence2
I0814 08:07:36.541064 28599 net.cpp:159] Memory required for data: 436694976
I0814 08:07:36.541067 28599 layer_factory.hpp:77] Creating layer Silence3
I0814 08:07:36.541070 28599 net.cpp:91] Creating Layer Silence3
I0814 08:07:36.541074 28599 net.cpp:432] Silence3 <- blob2_CustomData1_2_split_1
I0814 08:07:36.541076 28599 net.cpp:144] Setting up Silence3
I0814 08:07:36.541079 28599 net.cpp:159] Memory required for data: 436694976
I0814 08:07:36.541081 28599 layer_factory.hpp:77] Creating layer Silence4
I0814 08:07:36.541087 28599 net.cpp:91] Creating Layer Silence4
I0814 08:07:36.541090 28599 net.cpp:432] Silence4 <- blob9_DummyData1_0_split_1
I0814 08:07:36.541095 28599 net.cpp:144] Setting up Silence4
I0814 08:07:36.541096 28599 net.cpp:159] Memory required for data: 436694976
I0814 08:07:36.541098 28599 layer_factory.hpp:77] Creating layer Silence5
I0814 08:07:36.541102 28599 net.cpp:91] Creating Layer Silence5
I0814 08:07:36.541105 28599 net.cpp:432] Silence5 <- blob13
I0814 08:07:36.541107 28599 net.cpp:144] Setting up Silence5
I0814 08:07:36.541110 28599 net.cpp:159] Memory required for data: 436694976
I0814 08:07:36.541112 28599 layer_factory.hpp:77] Creating layer Silence6
I0814 08:07:36.541118 28599 net.cpp:91] Creating Layer Silence6
I0814 08:07:36.541121 28599 net.cpp:432] Silence6 <- img1_aug
I0814 08:07:36.541124 28599 net.cpp:144] Setting up Silence6
I0814 08:07:36.541127 28599 net.cpp:159] Memory required for data: 436694976
I0814 08:07:36.541129 28599 layer_factory.hpp:77] Creating layer upsample_disparity_itr1
I0814 08:07:36.541134 28599 net.cpp:91] Creating Layer upsample_disparity_itr1
I0814 08:07:36.541137 28599 net.cpp:432] upsample_disparity_itr1 <- disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split_0
I0814 08:07:36.541142 28599 net.cpp:404] upsample_disparity_itr1 -> upsampled_disparity_itr1
W0814 08:07:36.541148 28599 resample_layer.cpp:28] ResampleLayer only runs Reshape on setup
I0814 08:07:36.541177 28599 net.cpp:144] Setting up upsample_disparity_itr1
I0814 08:07:36.541182 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.541184 28599 net.cpp:159] Memory required for data: 441413568
I0814 08:07:36.541188 28599 layer_factory.hpp:77] Creating layer upsampled_disparity_itr1_upsample_disparity_itr1_0_split
I0814 08:07:36.541193 28599 net.cpp:91] Creating Layer upsampled_disparity_itr1_upsample_disparity_itr1_0_split
I0814 08:07:36.541195 28599 net.cpp:432] upsampled_disparity_itr1_upsample_disparity_itr1_0_split <- upsampled_disparity_itr1
I0814 08:07:36.541206 28599 net.cpp:404] upsampled_disparity_itr1_upsample_disparity_itr1_0_split -> upsampled_disparity_itr1_upsample_disparity_itr1_0_split_0
I0814 08:07:36.541213 28599 net.cpp:404] upsampled_disparity_itr1_upsample_disparity_itr1_0_split -> upsampled_disparity_itr1_upsample_disparity_itr1_0_split_1
I0814 08:07:36.541240 28599 net.cpp:144] Setting up upsampled_disparity_itr1_upsample_disparity_itr1_0_split
I0814 08:07:36.541245 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.541249 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:36.541251 28599 net.cpp:159] Memory required for data: 450850752
I0814 08:07:36.541254 28599 layer_factory.hpp:77] Creating layer upsample_disparity2x_itr1
I0814 08:07:36.541259 28599 net.cpp:91] Creating Layer upsample_disparity2x_itr1
I0814 08:07:36.541261 28599 net.cpp:432] upsample_disparity2x_itr1 <- disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split_1
I0814 08:07:36.541265 28599 net.cpp:404] upsample_disparity2x_itr1 -> upsampled_disparity2x_itr1
W0814 08:07:36.541270 28599 resample_layer.cpp:28] ResampleLayer only runs Reshape on setup
I0814 08:07:36.541287 28599 net.cpp:144] Setting up upsample_disparity2x_itr1
I0814 08:07:36.541292 28599 net.cpp:151] Top shape: 4 1 192 384 (294912)
I0814 08:07:36.541294 28599 net.cpp:159] Memory required for data: 452030400
I0814 08:07:36.541297 28599 layer_factory.hpp:77] Creating layer concat_input_itr1
I0814 08:07:36.541301 28599 net.cpp:91] Creating Layer concat_input_itr1
I0814 08:07:36.541304 28599 net.cpp:432] concat_input_itr1 <- img0_aug_img0s_aug_0_split_1
I0814 08:07:36.541308 28599 net.cpp:432] concat_input_itr1 <- upsampled_disparity_itr1_upsample_disparity_itr1_0_split_0
I0814 08:07:36.541312 28599 net.cpp:404] concat_input_itr1 -> zoom_input_itr1
I0814 08:07:36.541327 28599 net.cpp:144] Setting up concat_input_itr1
I0814 08:07:36.541332 28599 net.cpp:151] Top shape: 4 4 384 768 (4718592)
I0814 08:07:36.541334 28599 net.cpp:159] Memory required for data: 470904768
I0814 08:07:36.541337 28599 layer_factory.hpp:77] Creating layer zoom_conv0_itr1
I0814 08:07:36.541352 28599 net.cpp:91] Creating Layer zoom_conv0_itr1
I0814 08:07:36.541355 28599 net.cpp:432] zoom_conv0_itr1 <- zoom_input_itr1
I0814 08:07:36.541362 28599 net.cpp:404] zoom_conv0_itr1 -> zoom_conv0_itr1
I0814 08:07:37.348233 28599 net.cpp:144] Setting up zoom_conv0_itr1
I0814 08:07:37.348260 28599 net.cpp:151] Top shape: 4 32 384 768 (37748736)
I0814 08:07:37.348263 28599 net.cpp:159] Memory required for data: 621899712
I0814 08:07:37.348273 28599 layer_factory.hpp:77] Creating layer zoom_ReLU0_itr1
I0814 08:07:37.348292 28599 net.cpp:91] Creating Layer zoom_ReLU0_itr1
I0814 08:07:37.348320 28599 net.cpp:432] zoom_ReLU0_itr1 <- zoom_conv0_itr1
I0814 08:07:37.348330 28599 net.cpp:391] zoom_ReLU0_itr1 -> zoom_conv0_itr1 (in-place)
I0814 08:07:37.348819 28599 net.cpp:144] Setting up zoom_ReLU0_itr1
I0814 08:07:37.348829 28599 net.cpp:151] Top shape: 4 32 384 768 (37748736)
I0814 08:07:37.348842 28599 net.cpp:159] Memory required for data: 772894656
I0814 08:07:37.348845 28599 layer_factory.hpp:77] Creating layer zoom_conv0_itr1_zoom_ReLU0_itr1_0_split
I0814 08:07:37.348851 28599 net.cpp:91] Creating Layer zoom_conv0_itr1_zoom_ReLU0_itr1_0_split
I0814 08:07:37.348879 28599 net.cpp:432] zoom_conv0_itr1_zoom_ReLU0_itr1_0_split <- zoom_conv0_itr1
I0814 08:07:37.348887 28599 net.cpp:404] zoom_conv0_itr1_zoom_ReLU0_itr1_0_split -> zoom_conv0_itr1_zoom_ReLU0_itr1_0_split_0
I0814 08:07:37.348894 28599 net.cpp:404] zoom_conv0_itr1_zoom_ReLU0_itr1_0_split -> zoom_conv0_itr1_zoom_ReLU0_itr1_0_split_1
I0814 08:07:37.348938 28599 net.cpp:144] Setting up zoom_conv0_itr1_zoom_ReLU0_itr1_0_split
I0814 08:07:37.348944 28599 net.cpp:151] Top shape: 4 32 384 768 (37748736)
I0814 08:07:37.348986 28599 net.cpp:151] Top shape: 4 32 384 768 (37748736)
I0814 08:07:37.349002 28599 net.cpp:159] Memory required for data: 1074884544
I0814 08:07:37.349007 28599 layer_factory.hpp:77] Creating layer zoom_conv1_itr1
I0814 08:07:37.349027 28599 net.cpp:91] Creating Layer zoom_conv1_itr1
I0814 08:07:37.349032 28599 net.cpp:432] zoom_conv1_itr1 <- zoom_conv0_itr1_zoom_ReLU0_itr1_0_split_0
I0814 08:07:37.349037 28599 net.cpp:404] zoom_conv1_itr1 -> zoom_conv1_itr1
I0814 08:07:37.351229 28599 net.cpp:144] Setting up zoom_conv1_itr1
I0814 08:07:37.351243 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.351245 28599 net.cpp:159] Memory required for data: 1150382016
I0814 08:07:37.351265 28599 layer_factory.hpp:77] Creating layer zoom_ReLU1_itr1
I0814 08:07:37.351271 28599 net.cpp:91] Creating Layer zoom_ReLU1_itr1
I0814 08:07:37.351275 28599 net.cpp:432] zoom_ReLU1_itr1 <- zoom_conv1_itr1
I0814 08:07:37.351279 28599 net.cpp:391] zoom_ReLU1_itr1 -> zoom_conv1_itr1 (in-place)
I0814 08:07:37.351431 28599 net.cpp:144] Setting up zoom_ReLU1_itr1
I0814 08:07:37.351439 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.351451 28599 net.cpp:159] Memory required for data: 1225879488
I0814 08:07:37.351454 28599 layer_factory.hpp:77] Creating layer zoom_conv1b_itr1
I0814 08:07:37.351462 28599 net.cpp:91] Creating Layer zoom_conv1b_itr1
I0814 08:07:37.351465 28599 net.cpp:432] zoom_conv1b_itr1 <- zoom_conv1_itr1
I0814 08:07:37.351470 28599 net.cpp:404] zoom_conv1b_itr1 -> zoom_conv1b_itr1
I0814 08:07:37.353539 28599 net.cpp:144] Setting up zoom_conv1b_itr1
I0814 08:07:37.353560 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.353564 28599 net.cpp:159] Memory required for data: 1301376960
I0814 08:07:37.353569 28599 layer_factory.hpp:77] Creating layer zoom_ReLU1b_itr1
I0814 08:07:37.353574 28599 net.cpp:91] Creating Layer zoom_ReLU1b_itr1
I0814 08:07:37.353586 28599 net.cpp:432] zoom_ReLU1b_itr1 <- zoom_conv1b_itr1
I0814 08:07:37.353590 28599 net.cpp:391] zoom_ReLU1b_itr1 -> zoom_conv1b_itr1 (in-place)
I0814 08:07:37.353724 28599 net.cpp:144] Setting up zoom_ReLU1b_itr1
I0814 08:07:37.353730 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.353744 28599 net.cpp:159] Memory required for data: 1376874432
I0814 08:07:37.353746 28599 layer_factory.hpp:77] Creating layer zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split
I0814 08:07:37.353751 28599 net.cpp:91] Creating Layer zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split
I0814 08:07:37.353754 28599 net.cpp:432] zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split <- zoom_conv1b_itr1
I0814 08:07:37.353759 28599 net.cpp:404] zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split -> zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split_0
I0814 08:07:37.353765 28599 net.cpp:404] zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split -> zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split_1
I0814 08:07:37.353794 28599 net.cpp:144] Setting up zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split
I0814 08:07:37.353808 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.353812 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.353823 28599 net.cpp:159] Memory required for data: 1527869376
I0814 08:07:37.353826 28599 layer_factory.hpp:77] Creating layer zoom_conv2_itr1
I0814 08:07:37.353842 28599 net.cpp:91] Creating Layer zoom_conv2_itr1
I0814 08:07:37.353845 28599 net.cpp:432] zoom_conv2_itr1 <- zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split_0
I0814 08:07:37.353850 28599 net.cpp:404] zoom_conv2_itr1 -> zoom_conv2_itr1
I0814 08:07:37.356015 28599 net.cpp:144] Setting up zoom_conv2_itr1
I0814 08:07:37.356029 28599 net.cpp:151] Top shape: 4 64 96 192 (4718592)
I0814 08:07:37.356031 28599 net.cpp:159] Memory required for data: 1546743744
I0814 08:07:37.356037 28599 layer_factory.hpp:77] Creating layer zoom_ReLU2_itr1
I0814 08:07:37.356043 28599 net.cpp:91] Creating Layer zoom_ReLU2_itr1
I0814 08:07:37.356046 28599 net.cpp:432] zoom_ReLU2_itr1 <- zoom_conv2_itr1
I0814 08:07:37.356050 28599 net.cpp:391] zoom_ReLU2_itr1 -> zoom_conv2_itr1 (in-place)
I0814 08:07:37.356600 28599 net.cpp:144] Setting up zoom_ReLU2_itr1
I0814 08:07:37.356609 28599 net.cpp:151] Top shape: 4 64 96 192 (4718592)
I0814 08:07:37.356612 28599 net.cpp:159] Memory required for data: 1565618112
I0814 08:07:37.356627 28599 layer_factory.hpp:77] Creating layer zoom_conv2b_itr1
I0814 08:07:37.356637 28599 net.cpp:91] Creating Layer zoom_conv2b_itr1
I0814 08:07:37.356640 28599 net.cpp:432] zoom_conv2b_itr1 <- zoom_conv2_itr1
I0814 08:07:37.356645 28599 net.cpp:404] zoom_conv2b_itr1 -> zoom_conv2b_itr1
I0814 08:07:37.358645 28599 net.cpp:144] Setting up zoom_conv2b_itr1
I0814 08:07:37.358656 28599 net.cpp:151] Top shape: 4 64 96 192 (4718592)
I0814 08:07:37.358669 28599 net.cpp:159] Memory required for data: 1584492480
I0814 08:07:37.358675 28599 layer_factory.hpp:77] Creating layer zoom_ReLU2b_itr1
I0814 08:07:37.358680 28599 net.cpp:91] Creating Layer zoom_ReLU2b_itr1
I0814 08:07:37.358682 28599 net.cpp:432] zoom_ReLU2b_itr1 <- zoom_conv2b_itr1
I0814 08:07:37.358687 28599 net.cpp:391] zoom_ReLU2b_itr1 -> zoom_conv2b_itr1 (in-place)
I0814 08:07:37.358820 28599 net.cpp:144] Setting up zoom_ReLU2b_itr1
I0814 08:07:37.358827 28599 net.cpp:151] Top shape: 4 64 96 192 (4718592)
I0814 08:07:37.358839 28599 net.cpp:159] Memory required for data: 1603366848
I0814 08:07:37.358842 28599 layer_factory.hpp:77] Creating layer zoom_deconv2_itr1
I0814 08:07:37.358849 28599 net.cpp:91] Creating Layer zoom_deconv2_itr1
I0814 08:07:37.358852 28599 net.cpp:432] zoom_deconv2_itr1 <- zoom_conv2b_itr1
I0814 08:07:37.358856 28599 net.cpp:404] zoom_deconv2_itr1 -> zoom_deconv2_itr1
I0814 08:07:37.360308 28599 net.cpp:144] Setting up zoom_deconv2_itr1
I0814 08:07:37.360333 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.360347 28599 net.cpp:159] Memory required for data: 1678864320
I0814 08:07:37.360358 28599 layer_factory.hpp:77] Creating layer zoom_ReLU0p_itr1
I0814 08:07:37.360366 28599 net.cpp:91] Creating Layer zoom_ReLU0p_itr1
I0814 08:07:37.360370 28599 net.cpp:432] zoom_ReLU0p_itr1 <- zoom_deconv2_itr1
I0814 08:07:37.360375 28599 net.cpp:391] zoom_ReLU0p_itr1 -> zoom_deconv2_itr1 (in-place)
I0814 08:07:37.360934 28599 net.cpp:144] Setting up zoom_ReLU0p_itr1
I0814 08:07:37.360944 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.360947 28599 net.cpp:159] Memory required for data: 1754361792
I0814 08:07:37.360950 28599 layer_factory.hpp:77] Creating layer zoom_Concat2_itr1
I0814 08:07:37.360965 28599 net.cpp:91] Creating Layer zoom_Concat2_itr1
I0814 08:07:37.360968 28599 net.cpp:432] zoom_Concat2_itr1 <- zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split_1
I0814 08:07:37.360972 28599 net.cpp:432] zoom_Concat2_itr1 <- zoom_deconv2_itr1
I0814 08:07:37.360976 28599 net.cpp:432] zoom_Concat2_itr1 <- upsampled_disparity2x_itr1
I0814 08:07:37.360981 28599 net.cpp:404] zoom_Concat2_itr1 -> zoom_concat2_itr1
I0814 08:07:37.361021 28599 net.cpp:144] Setting up zoom_Concat2_itr1
I0814 08:07:37.361035 28599 net.cpp:151] Top shape: 4 129 192 384 (38043648)
I0814 08:07:37.361038 28599 net.cpp:159] Memory required for data: 1906536384
I0814 08:07:37.361040 28599 layer_factory.hpp:77] Creating layer zoom_fused1_itr1
I0814 08:07:37.361057 28599 net.cpp:91] Creating Layer zoom_fused1_itr1
I0814 08:07:37.361069 28599 net.cpp:432] zoom_fused1_itr1 <- zoom_concat2_itr1
I0814 08:07:37.361074 28599 net.cpp:404] zoom_fused1_itr1 -> zoom_fused1_itr1
I0814 08:07:37.363404 28599 net.cpp:144] Setting up zoom_fused1_itr1
I0814 08:07:37.363421 28599 net.cpp:151] Top shape: 4 64 192 384 (18874368)
I0814 08:07:37.363425 28599 net.cpp:159] Memory required for data: 1982033856
I0814 08:07:37.363432 28599 layer_factory.hpp:77] Creating layer zoom_deconv1_itr1
I0814 08:07:37.363441 28599 net.cpp:91] Creating Layer zoom_deconv1_itr1
I0814 08:07:37.363446 28599 net.cpp:432] zoom_deconv1_itr1 <- zoom_fused1_itr1
I0814 08:07:37.363451 28599 net.cpp:404] zoom_deconv1_itr1 -> zoom_deconv1_itr1
I0814 08:07:37.364879 28599 net.cpp:144] Setting up zoom_deconv1_itr1
I0814 08:07:37.364897 28599 net.cpp:151] Top shape: 4 32 384 768 (37748736)
I0814 08:07:37.364900 28599 net.cpp:159] Memory required for data: 2133028800
I0814 08:07:37.364915 28599 layer_factory.hpp:77] Creating layer zoom_ReLU1p_itr1
I0814 08:07:37.364923 28599 net.cpp:91] Creating Layer zoom_ReLU1p_itr1
I0814 08:07:37.364940 28599 net.cpp:432] zoom_ReLU1p_itr1 <- zoom_deconv1_itr1
I0814 08:07:37.364946 28599 net.cpp:391] zoom_ReLU1p_itr1 -> zoom_deconv1_itr1 (in-place)
I0814 08:07:37.365469 28599 net.cpp:144] Setting up zoom_ReLU1p_itr1
I0814 08:07:37.365479 28599 net.cpp:151] Top shape: 4 32 384 768 (37748736)
I0814 08:07:37.365483 28599 net.cpp:159] Memory required for data: 2284023744
I0814 08:07:37.365485 28599 layer_factory.hpp:77] Creating layer zoom_Concat1_itr1
I0814 08:07:37.365491 28599 net.cpp:91] Creating Layer zoom_Concat1_itr1
I0814 08:07:37.365504 28599 net.cpp:432] zoom_Concat1_itr1 <- zoom_conv0_itr1_zoom_ReLU0_itr1_0_split_1
I0814 08:07:37.365509 28599 net.cpp:432] zoom_Concat1_itr1 <- zoom_deconv1_itr1
I0814 08:07:37.365512 28599 net.cpp:432] zoom_Concat1_itr1 <- upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split_0
I0814 08:07:37.365516 28599 net.cpp:404] zoom_Concat1_itr1 -> zoom_concat1_itr1
I0814 08:07:37.365550 28599 net.cpp:144] Setting up zoom_Concat1_itr1
I0814 08:07:37.365556 28599 net.cpp:151] Top shape: 4 65 384 768 (76677120)
I0814 08:07:37.365566 28599 net.cpp:159] Memory required for data: 2590732224
I0814 08:07:37.365569 28599 layer_factory.hpp:77] Creating layer zoom_fused0_itr1
I0814 08:07:37.365586 28599 net.cpp:91] Creating Layer zoom_fused0_itr1
I0814 08:07:37.365589 28599 net.cpp:432] zoom_fused0_itr1 <- zoom_concat1_itr1
I0814 08:07:37.365603 28599 net.cpp:404] zoom_fused0_itr1 -> zoom_fused0_itr1
I0814 08:07:37.367597 28599 net.cpp:144] Setting up zoom_fused0_itr1
I0814 08:07:37.367612 28599 net.cpp:151] Top shape: 4 32 384 768 (37748736)
I0814 08:07:37.367615 28599 net.cpp:159] Memory required for data: 2741727168
I0814 08:07:37.367621 28599 layer_factory.hpp:77] Creating layer zoom_Convolution0_itr1
I0814 08:07:37.367631 28599 net.cpp:91] Creating Layer zoom_Convolution0_itr1
I0814 08:07:37.367635 28599 net.cpp:432] zoom_Convolution0_itr1 <- zoom_fused0_itr1
I0814 08:07:37.367640 28599 net.cpp:404] zoom_Convolution0_itr1 -> zoom_predict0_itr1
I0814 08:07:37.370065 28599 net.cpp:144] Setting up zoom_Convolution0_itr1
I0814 08:07:37.370088 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:37.370091 28599 net.cpp:159] Memory required for data: 2746445760
I0814 08:07:37.370097 28599 layer_factory.hpp:77] Creating layer zoom_ReLU_predict_itr1
I0814 08:07:37.370103 28599 net.cpp:91] Creating Layer zoom_ReLU_predict_itr1
I0814 08:07:37.370116 28599 net.cpp:432] zoom_ReLU_predict_itr1 <- zoom_predict0_itr1
I0814 08:07:37.370121 28599 net.cpp:404] zoom_ReLU_predict_itr1 -> zoom_predict0n_itr1
I0814 08:07:37.370630 28599 net.cpp:144] Setting up zoom_ReLU_predict_itr1
I0814 08:07:37.370640 28599 net.cpp:151] Top shape: 4 1 384 768 (1179648)
I0814 08:07:37.370652 28599 net.cpp:159] Memory required for data: 2751164352
I0814 08:07:37.370656 28599 layer_factory.hpp:77] Creating layer zoom_disp_loss0
I0814 08:07:37.370667 28599 net.cpp:91] Creating Layer zoom_disp_loss0
I0814 08:07:37.370671 28599 net.cpp:432] zoom_disp_loss0 <- zoom_predict0n_itr1
I0814 08:07:37.370676 28599 net.cpp:432] zoom_disp_loss0 <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_2
I0814 08:07:37.370681 28599 net.cpp:404] zoom_disp_loss0 -> zoom_disp_loss0
I0814 08:07:37.370748 28599 net.cpp:144] Setting up zoom_disp_loss0
I0814 08:07:37.370755 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.370757 28599 net.cpp:154]     with loss weight 1
I0814 08:07:37.370772 28599 net.cpp:159] Memory required for data: 2751164356
I0814 08:07:37.370775 28599 layer_factory.hpp:77] Creating layer zoom_disp_loss0_zoom_disp_loss0_0_split
I0814 08:07:37.370780 28599 net.cpp:91] Creating Layer zoom_disp_loss0_zoom_disp_loss0_0_split
I0814 08:07:37.370784 28599 net.cpp:432] zoom_disp_loss0_zoom_disp_loss0_0_split <- zoom_disp_loss0
I0814 08:07:37.370787 28599 net.cpp:404] zoom_disp_loss0_zoom_disp_loss0_0_split -> zoom_disp_loss0_zoom_disp_loss0_0_split_0
I0814 08:07:37.370792 28599 net.cpp:404] zoom_disp_loss0_zoom_disp_loss0_0_split -> zoom_disp_loss0_zoom_disp_loss0_0_split_1
I0814 08:07:37.370826 28599 net.cpp:144] Setting up zoom_disp_loss0_zoom_disp_loss0_0_split
I0814 08:07:37.370831 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.370833 28599 net.cpp:154]     with loss weight 1
I0814 08:07:37.370838 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.370841 28599 net.cpp:159] Memory required for data: 2751164364
I0814 08:07:37.370843 28599 layer_factory.hpp:77] Creating layer down_up_loss
I0814 08:07:37.370848 28599 net.cpp:91] Creating Layer down_up_loss
I0814 08:07:37.370851 28599 net.cpp:432] down_up_loss <- upsampled_disparity_itr1_upsample_disparity_itr1_0_split_1
I0814 08:07:37.370856 28599 net.cpp:432] down_up_loss <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_3
I0814 08:07:37.370859 28599 net.cpp:404] down_up_loss -> down_up_loss
I0814 08:07:37.370908 28599 net.cpp:144] Setting up down_up_loss
I0814 08:07:37.370913 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.370915 28599 net.cpp:159] Memory required for data: 2751164368
I0814 08:07:37.370918 28599 layer_factory.hpp:77] Creating layer down_up_half_loss
I0814 08:07:37.370923 28599 net.cpp:91] Creating Layer down_up_half_loss
I0814 08:07:37.370926 28599 net.cpp:432] down_up_half_loss <- upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split_1
I0814 08:07:37.370930 28599 net.cpp:432] down_up_half_loss <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_4
I0814 08:07:37.370934 28599 net.cpp:404] down_up_half_loss -> down_up_half_loss
I0814 08:07:37.370978 28599 net.cpp:144] Setting up down_up_half_loss
I0814 08:07:37.370985 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.370986 28599 net.cpp:159] Memory required for data: 2751164372
I0814 08:07:37.370990 28599 layer_factory.hpp:77] Creating layer to_real_disp_loss
I0814 08:07:37.370995 28599 net.cpp:91] Creating Layer to_real_disp_loss
I0814 08:07:37.370997 28599 net.cpp:432] to_real_disp_loss <- zoom_disp_loss0_zoom_disp_loss0_0_split_1
I0814 08:07:37.371001 28599 net.cpp:404] to_real_disp_loss -> zoom_disp_loss0_real
I0814 08:07:37.371006 28599 net.cpp:144] Setting up to_real_disp_loss
I0814 08:07:37.371011 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.371013 28599 net.cpp:159] Memory required for data: 2751164376
I0814 08:07:37.371016 28599 layer_factory.hpp:77] Creating layer to_real_downup_loss
I0814 08:07:37.371024 28599 net.cpp:91] Creating Layer to_real_downup_loss
I0814 08:07:37.371028 28599 net.cpp:432] to_real_downup_loss <- down_up_loss
I0814 08:07:37.371032 28599 net.cpp:404] to_real_downup_loss -> down_up_loss_real
I0814 08:07:37.371037 28599 net.cpp:144] Setting up to_real_downup_loss
I0814 08:07:37.371042 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.371044 28599 net.cpp:159] Memory required for data: 2751164380
I0814 08:07:37.371047 28599 layer_factory.hpp:77] Creating layer to_real_downup_loss
I0814 08:07:37.371052 28599 net.cpp:91] Creating Layer to_real_downup_loss
I0814 08:07:37.371053 28599 net.cpp:432] to_real_downup_loss <- down_up_half_loss
I0814 08:07:37.371057 28599 net.cpp:404] to_real_downup_loss -> down_up_half_loss_real
I0814 08:07:37.371062 28599 net.cpp:144] Setting up to_real_downup_loss
I0814 08:07:37.371067 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.371068 28599 net.cpp:159] Memory required for data: 2751164384
I0814 08:07:37.371071 28599 net.cpp:224] to_real_downup_loss does not need backward computation.
I0814 08:07:37.371074 28599 net.cpp:224] to_real_downup_loss does not need backward computation.
I0814 08:07:37.371076 28599 net.cpp:224] to_real_disp_loss does not need backward computation.
I0814 08:07:37.371079 28599 net.cpp:224] down_up_half_loss does not need backward computation.
I0814 08:07:37.371083 28599 net.cpp:224] down_up_loss does not need backward computation.
I0814 08:07:37.371085 28599 net.cpp:222] zoom_disp_loss0_zoom_disp_loss0_0_split needs backward computation.
I0814 08:07:37.371088 28599 net.cpp:222] zoom_disp_loss0 needs backward computation.
I0814 08:07:37.371091 28599 net.cpp:222] zoom_ReLU_predict_itr1 needs backward computation.
I0814 08:07:37.371094 28599 net.cpp:222] zoom_Convolution0_itr1 needs backward computation.
I0814 08:07:37.371104 28599 net.cpp:222] zoom_fused0_itr1 needs backward computation.
I0814 08:07:37.371107 28599 net.cpp:222] zoom_Concat1_itr1 needs backward computation.
I0814 08:07:37.371111 28599 net.cpp:222] zoom_ReLU1p_itr1 needs backward computation.
I0814 08:07:37.371114 28599 net.cpp:222] zoom_deconv1_itr1 needs backward computation.
I0814 08:07:37.371116 28599 net.cpp:222] zoom_fused1_itr1 needs backward computation.
I0814 08:07:37.371119 28599 net.cpp:222] zoom_Concat2_itr1 needs backward computation.
I0814 08:07:37.371124 28599 net.cpp:222] zoom_ReLU0p_itr1 needs backward computation.
I0814 08:07:37.371125 28599 net.cpp:222] zoom_deconv2_itr1 needs backward computation.
I0814 08:07:37.371129 28599 net.cpp:222] zoom_ReLU2b_itr1 needs backward computation.
I0814 08:07:37.371131 28599 net.cpp:222] zoom_conv2b_itr1 needs backward computation.
I0814 08:07:37.371134 28599 net.cpp:222] zoom_ReLU2_itr1 needs backward computation.
I0814 08:07:37.371137 28599 net.cpp:222] zoom_conv2_itr1 needs backward computation.
I0814 08:07:37.371140 28599 net.cpp:222] zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split needs backward computation.
I0814 08:07:37.371143 28599 net.cpp:222] zoom_ReLU1b_itr1 needs backward computation.
I0814 08:07:37.371146 28599 net.cpp:222] zoom_conv1b_itr1 needs backward computation.
I0814 08:07:37.371150 28599 net.cpp:222] zoom_ReLU1_itr1 needs backward computation.
I0814 08:07:37.371151 28599 net.cpp:222] zoom_conv1_itr1 needs backward computation.
I0814 08:07:37.371155 28599 net.cpp:222] zoom_conv0_itr1_zoom_ReLU0_itr1_0_split needs backward computation.
I0814 08:07:37.371157 28599 net.cpp:222] zoom_ReLU0_itr1 needs backward computation.
I0814 08:07:37.371160 28599 net.cpp:222] zoom_conv0_itr1 needs backward computation.
I0814 08:07:37.371163 28599 net.cpp:222] concat_input_itr1 needs backward computation.
I0814 08:07:37.371167 28599 net.cpp:222] upsample_disparity2x_itr1 needs backward computation.
I0814 08:07:37.371170 28599 net.cpp:222] upsampled_disparity_itr1_upsample_disparity_itr1_0_split needs backward computation.
I0814 08:07:37.371173 28599 net.cpp:222] upsample_disparity_itr1 needs backward computation.
I0814 08:07:37.371177 28599 net.cpp:224] Silence6 does not need backward computation.
I0814 08:07:37.371179 28599 net.cpp:224] Silence5 does not need backward computation.
I0814 08:07:37.371182 28599 net.cpp:224] Silence4 does not need backward computation.
I0814 08:07:37.371186 28599 net.cpp:224] Silence3 does not need backward computation.
I0814 08:07:37.371187 28599 net.cpp:224] Silence2 does not need backward computation.
I0814 08:07:37.371191 28599 net.cpp:224] Silence1 does not need backward computation.
I0814 08:07:37.371193 28599 net.cpp:222] upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split needs backward computation.
I0814 08:07:37.371196 28599 net.cpp:222] upsample_disparity_itr1_half needs backward computation.
I0814 08:07:37.371201 28599 net.cpp:224] disp_gt_aug_norm_down_half does not need backward computation.
I0814 08:07:37.371204 28599 net.cpp:224] disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split does not need backward computation.
I0814 08:07:37.371207 28599 net.cpp:224] disp_gt_aug_norm_down does not need backward computation.
I0814 08:07:37.371212 28599 net.cpp:222] disp_gt_aug_norm_disp_gt_aug_norm_0_split needs backward computation.
I0814 08:07:37.371215 28599 net.cpp:222] disp_gt_aug_norm needs backward computation.
I0814 08:07:37.371218 28599 net.cpp:224] ref_disp_scale_half does not need backward computation.
I0814 08:07:37.371222 28599 net.cpp:224] ref_disp_scale does not need backward computation.
I0814 08:07:37.371224 28599 net.cpp:222] Slice1 needs backward computation.
I0814 08:07:37.371227 28599 net.cpp:224] FlowAugmentation1 does not need backward computation.
I0814 08:07:37.371232 28599 net.cpp:224] Concat1 does not need backward computation.
I0814 08:07:37.371235 28599 net.cpp:224] blob9_DummyData1_0_split does not need backward computation.
I0814 08:07:37.371238 28599 net.cpp:224] DummyData1 does not need backward computation.
I0814 08:07:37.371245 28599 net.cpp:224] img1s_aug does not need backward computation.
I0814 08:07:37.371249 28599 net.cpp:224] blob7_aug_params1_0_split does not need backward computation.
I0814 08:07:37.371253 28599 net.cpp:224] aug_params1 does not need backward computation.
I0814 08:07:37.371258 28599 net.cpp:224] blob6_img0s_aug_1_split does not need backward computation.
I0814 08:07:37.371261 28599 net.cpp:222] img0_aug_img0s_aug_0_split needs backward computation.
I0814 08:07:37.371264 28599 net.cpp:222] img0s_aug needs backward computation.
I0814 08:07:37.371268 28599 net.cpp:224] Eltwise2 does not need backward computation.
I0814 08:07:37.371271 28599 net.cpp:224] blob3_Eltwise1_0_split does not need backward computation.
I0814 08:07:37.371274 28599 net.cpp:224] Eltwise1 does not need backward computation.
I0814 08:07:37.371279 28599 net.cpp:224] blob2_CustomData1_2_split does not need backward computation.
I0814 08:07:37.371281 28599 net.cpp:224] blob1_CustomData1_1_split does not need backward computation.
I0814 08:07:37.371284 28599 net.cpp:224] blob0_CustomData1_0_split does not need backward computation.
I0814 08:07:37.371289 28599 net.cpp:224] CustomData1 does not need backward computation.
I0814 08:07:37.371290 28599 net.cpp:266] This network produces output down_up_half_loss_real
I0814 08:07:37.371294 28599 net.cpp:266] This network produces output down_up_loss_real
I0814 08:07:37.371296 28599 net.cpp:266] This network produces output zoom_disp_loss0_real
I0814 08:07:37.371300 28599 net.cpp:266] This network produces output zoom_disp_loss0_zoom_disp_loss0_0_split_0
I0814 08:07:37.371338 28599 net.cpp:279] Network initialization done.
I0814 08:07:37.372475 28599 solver.cpp:181] Creating test net (#0) specified by net file: train_zoom_itr1.prototxt
I0814 08:07:37.372539 28599 net.cpp:318] The NetState phase (1) differed from the phase (0) specified by a rule in layer CustomData1
I0814 08:07:37.372552 28599 net.cpp:318] The NetState phase (1) differed from the phase (0) specified by a rule in layer DummyData1
I0814 08:07:37.372560 28599 net.cpp:318] The NetState phase (1) differed from the phase (0) specified by a rule in layer ref_disp_scale
I0814 08:07:37.372565 28599 net.cpp:318] The NetState phase (1) differed from the phase (0) specified by a rule in layer ref_disp_scale_half
I0814 08:07:37.372894 28599 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "CustomData2"
  type: "CustomData"
  top: "blob0"
  top: "blob1"
  top: "blob2"
  include {
    phase: TEST
  }
  data_param {
    source: "/media/leo/game/dispflownet-release/data/FlyingThings3D_release_TEST_clean_lmdb"
    batch_size: 1
    backend: LMDB
    rand_permute: true
    rand_permute_seed: 77
    slice_point: 3
    slice_point: 6
    encoding: UINT8
    encoding: UINT8
    encoding: UINT16FLOW
    verbose: true
  }
}
layer {
  name: "Eltwise1"
  type: "Eltwise"
  bottom: "blob0"
  top: "blob3"
  eltwise_param {
    operation: SUM
    coeff: 0.0039215689
  }
}
layer {
  name: "Eltwise2"
  type: "Eltwise"
  bottom: "blob1"
  top: "blob4"
  eltwise_param {
    operation: SUM
    coeff: 0.0039215689
  }
}
layer {
  name: "img0s_aug"
  type: "DataAugmentation"
  bottom: "blob3"
  top: "img0_aug"
  top: "blob6"
  propagate_down: false
  augmentation_param {
    max_multiplier: 1
    augment_during_test: false
    recompute_mean: 1000
    mean_per_pixel: false
    translate {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0
      spread: 0.4
      prob: 1
    }
    zoom {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: 0.2
      spread: 0.4
      prob: 1
    }
    squeeze {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: 0
      spread: 0.3
      prob: 1
    }
    lmult_pow {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: -0.2
      spread: 0.4
      prob: 1
    }
    lmult_mult {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    lmult_add {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0
      spread: 0.03
      prob: 1
    }
    sat_pow {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    sat_mult {
      rand_type: "uniform_bernoulli"
      exp: true
      mean: -0.3
      spread: 0.5
      prob: 1
    }
    sat_add {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0
      spread: 0.03
      prob: 1
    }
    col_pow {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    col_mult {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.2
      prob: 1
    }
    col_add {
      rand_type: "gaussian_bernoulli"
      exp: false
      mean: 0
      spread: 0.02
      prob: 1
    }
    ladd_pow {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    ladd_mult {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.4
      prob: 1
    }
    ladd_add {
      rand_type: "gaussian_bernoulli"
      exp: false
      mean: 0
      spread: 0.04
      prob: 1
    }
    col_rotate {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0
      spread: 1
      prob: 1
    }
    crop_width: 768
    crop_height: 384
    chromatic_eigvec: 0.51
    chromatic_eigvec: 0.56
    chromatic_eigvec: 0.65
    chromatic_eigvec: 0.79
    chromatic_eigvec: 0.01
    chromatic_eigvec: -0.62
    chromatic_eigvec: 0.35
    chromatic_eigvec: -0.83
    chromatic_eigvec: 0.44
    noise {
      rand_type: "uniform_bernoulli"
      exp: false
      mean: 0.03
      spread: 0.03
      prob: 1
    }
  }
}
layer {
  name: "aug_params1"
  type: "GenerateAugmentationParameters"
  bottom: "blob6"
  bottom: "blob3"
  bottom: "img0_aug"
  top: "blob7"
  coeff_schedule_param {
    half_life: 50000
    initial_coeff: 0.5
    final_coeff: 1
  }
  augmentation_param {
    augment_during_test: false
    gamma {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.01
      prob: 1
    }
    brightness {
      rand_type: "gaussian_bernoulli"
      exp: false
      mean: 0
      spread: 0.01
      prob: 1
    }
    contrast {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.01
      prob: 1
    }
    color {
      rand_type: "gaussian_bernoulli"
      exp: true
      mean: 0
      spread: 0.01
      prob: 1
    }
  }
}
layer {
  name: "img1s_aug"
  type: "DataAugmentation"
  bottom: "blob4"
  bottom: "blob7"
  top: "img1_aug"
  propagate_down: false
  propagate_down: false
  augmentation_param {
    max_multiplier: 1
    augment_during_test: false
    recompute_mean: 1000
    mean_per_pixel: false
    crop_width: 768
    crop_height: 384
    chromatic_eigvec: 0.51
    chromatic_eigvec: 0.56
    chromatic_eigvec: 0.65
    chromatic_eigvec: 0.79
    chromatic_eigvec: 0.01
    chromatic_eigvec: -0.62
    chromatic_eigvec: 0.35
    chromatic_eigvec: -0.83
    chromatic_eigvec: 0.44
  }
}
layer {
  name: "DummyData2"
  type: "DummyData"
  top: "blob9"
  include {
    phase: TEST
  }
  dummy_data_param {
    data_filler {
      type: "constant"
      value: 0
    }
    num: 1
    channels: 1
    height: 540
    width: 960
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "blob2"
  bottom: "blob9"
  top: "blob10"
  propagate_down: false
  propagate_down: false
  concat_param {
    concat_dim: 1
  }
}
layer {
  name: "FlowAugmentation1"
  type: "FlowAugmentation"
  bottom: "blob10"
  bottom: "blob6"
  bottom: "blob7"
  top: "blob11"
  propagate_down: false
  propagate_down: false
  propagate_down: false
  augmentation_param {
    crop_width: 768
    crop_height: 384
  }
}
layer {
  name: "Slice1"
  type: "Slice"
  bottom: "blob11"
  top: "disp_gt_aug"
  top: "blob13"
  propagate_down: false
  slice_param {
    slice_point: 1
    axis: 1
  }
}
layer {
  name: "ref_disp_scale"
  type: "DummyData"
  top: "ref_disp_scale"
  include {
    phase: TEST
  }
  dummy_data_param {
    data_filler {
      type: "constant"
      value: 0
    }
    num: 1
    channels: 1
    height: 96
    width: 192
  }
}
layer {
  name: "ref_disp_scale_half"
  type: "DummyData"
  top: "ref_disp_scale_half"
  include {
    phase: TEST
  }
  dummy_data_param {
    data_filler {
      type: "constant"
      value: 0
    }
    num: 1
    channels: 4
    height: 192
    width: 384
  }
}
layer {
  name: "disp_gt_aug_norm"
  type: "Eltwise"
  bottom: "disp_gt_aug"
  top: "disp_gt_aug_norm"
  eltwise_param {
    operation: SUM
    coeff: -0.1
  }
}
layer {
  name: "disp_gt_aug_norm_down"
  type: "Downsample"
  bottom: "disp_gt_aug_norm"
  bottom: "ref_disp_scale"
  top: "disp_gt_aug_norm_down"
  propagate_down: false
  propagate_down: false
}
layer {
  name: "disp_gt_aug_norm_down_half"
  type: "Downsample"
  bottom: "disp_gt_aug_norm"
  bottom: "ref_disp_scale_half"
  top: "disp_gt_aug_norm_down_half"
  propagate_down: false
  propagate_down: false
}
layer {
  name: "upsample_disparity_itr1_half"
  type: "Resample"
  bottom: "disp_gt_aug_norm_down_half"
  top: "upsampled_disparity_itr1_half"
  propagate_down: false
  resample_param {
    width: 768
    height: 384
    type: LINEAR
    antialias: true
  }
}
layer {
  name: "Silence1"
  type: "Silence"
  bottom: "blob0"
}
layer {
  name: "Silence2"
  type: "Silence"
  bottom: "blob1"
}
layer {
  name: "Silence3"
  type: "Silence"
  bottom: "blob2"
}
layer {
  name: "Silence4"
  type: "Silence"
  bottom: "blob9"
}
layer {
  name: "Silence5"
  type: "Silence"
  bottom: "blob13"
}
layer {
  name: "Silence6"
  type: "Silence"
  bottom: "img1_aug"
}
layer {
  name: "upsample_disparity_itr1"
  type: "Resample"
  bottom: "disp_gt_aug_norm_down"
  top: "upsampled_disparity_itr1"
  propagate_down: false
  resample_param {
    width: 768
    height: 384
    type: LINEAR
    antialias: true
  }
}
layer {
  name: "upsample_disparity2x_itr1"
  type: "Resample"
  bottom: "disp_gt_aug_norm_down"
  top: "upsampled_disparity2x_itr1"
  propagate_down: false
  resample_param {
    width: 384
    height: 192
    type: LINEAR
    antialias: true
  }
}
layer {
  name: "concat_input_itr1"
  type: "Concat"
  bottom: "img0_aug"
  bottom: "upsampled_disparity_itr1"
  top: "zoom_input_itr1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "zoom_conv0_itr1"
  type: "Convolution"
  bottom: "zoom_input_itr1"
  top: "zoom_conv0_itr1"
  param {
    name: "zoom_conv0_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv0_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU0_itr1"
  type: "ReLU"
  bottom: "zoom_conv0_itr1"
  top: "zoom_conv0_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_conv1_itr1"
  type: "Convolution"
  bottom: "zoom_conv0_itr1"
  top: "zoom_conv1_itr1"
  param {
    name: "zoom_conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv1_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU1_itr1"
  type: "ReLU"
  bottom: "zoom_conv1_itr1"
  top: "zoom_conv1_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_conv1b_itr1"
  type: "Convolution"
  bottom: "zoom_conv1_itr1"
  top: "zoom_conv1b_itr1"
  param {
    name: "zoom_conv1b_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv1b_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU1b_itr1"
  type: "ReLU"
  bottom: "zoom_conv1b_itr1"
  top: "zoom_conv1b_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_conv2_itr1"
  type: "Convolution"
  bottom: "zoom_conv1b_itr1"
  top: "zoom_conv2_itr1"
  param {
    name: "zoom_conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv2_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU2_itr1"
  type: "ReLU"
  bottom: "zoom_conv2_itr1"
  top: "zoom_conv2_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_conv2b_itr1"
  type: "Convolution"
  bottom: "zoom_conv2_itr1"
  top: "zoom_conv2b_itr1"
  param {
    name: "zoom_conv2b_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "zoom_conv2b_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU2b_itr1"
  type: "ReLU"
  bottom: "zoom_conv2b_itr1"
  top: "zoom_conv2b_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_deconv2_itr1"
  type: "Deconvolution"
  bottom: "zoom_conv2b_itr1"
  top: "zoom_deconv2_itr1"
  param {
    name: "zoom_deconv2_w"
    lr_mult: 1
    decay_mult: 0
  }
  param {
    name: "zoom_deconv2_b"
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 4
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU0p_itr1"
  type: "ReLU"
  bottom: "zoom_deconv2_itr1"
  top: "zoom_deconv2_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_Concat2_itr1"
  type: "Concat"
  bottom: "zoom_conv1b_itr1"
  bottom: "zoom_deconv2_itr1"
  bottom: "upsampled_disparity2x_itr1"
  top: "zoom_concat2_itr1"
}
layer {
  name: "zoom_fused1_itr1"
  type: "Convolution"
  bottom: "zoom_concat2_itr1"
  top: "zoom_fused1_itr1"
  param {
    name: "zoom_fused1_w"
    lr_mult: 1
    decay_mult: 0
  }
  param {
    name: "zoom_fused1_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 64
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_deconv1_itr1"
  type: "Deconvolution"
  bottom: "zoom_fused1_itr1"
  top: "zoom_deconv1_itr1"
  param {
    lr_mult: 1
    decay_mult: 0
  }
  param {
    lr_mult: 0
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 4
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU1p_itr1"
  type: "ReLU"
  bottom: "zoom_deconv1_itr1"
  top: "zoom_deconv1_itr1"
  relu_param {
    negative_slope: 0.1
  }
}
layer {
  name: "zoom_Concat1_itr1"
  type: "Concat"
  bottom: "zoom_conv0_itr1"
  bottom: "zoom_deconv1_itr1"
  bottom: "upsampled_disparity_itr1_half"
  top: "zoom_concat1_itr1"
}
layer {
  name: "zoom_fused0_itr1"
  type: "Convolution"
  bottom: "zoom_concat1_itr1"
  top: "zoom_fused0_itr1"
  param {
    name: "zoom_fused0_w"
    lr_mult: 1
    decay_mult: 0
  }
  param {
    name: "zoom_fused0_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 32
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_Convolution0_itr1"
  type: "Convolution"
  bottom: "zoom_fused0_itr1"
  top: "zoom_predict0_itr1"
  param {
    name: "zoom_predict0_w"
    lr_mult: 1
    decay_mult: 0
  }
  param {
    name: "zoom_predict0_b"
    lr_mult: 1
    decay_mult: 0
  }
  convolution_param {
    num_output: 1
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
    engine: CUDNN
  }
}
layer {
  name: "zoom_ReLU_predict_itr1"
  type: "ReLU"
  bottom: "zoom_predict0_itr1"
  top: "zoom_predict0n_itr1"
}
layer {
  name: "zoom_disp_loss0"
  type: "L1Loss"
  bottom: "zoom_predict0n_itr1"
  bottom: "disp_gt_aug_norm"
  top: "zoom_disp_loss0"
  loss_weight: 1
  l1_loss_param {
    l2_per_location: false
    normalize_by_num_entries: true
  }
}
layer {
  name: "down_up_loss"
  type: "L1Loss"
  bottom: "upsampled_disparity_itr1"
  bottom: "disp_gt_aug_norm"
  top: "down_up_loss"
  loss_weight: 0
  propagate_down: false
  propagate_down: false
  l1_loss_param {
    l2_per_location: false
    normalize_by_num_entries: true
  }
}
layer {
  name: "down_up_half_loss"
  type: "L1Loss"
  bottom: "upsampled_disparity_itr1_half"
  bottom: "disp_gt_aug_norm"
  top: "down_up_half_loss"
  loss_weight: 0
  propagate_down: false
  propagate_down: false
  l1_loss_param {
    l2_per_location: false
    normalize_by_num_entries: true
  }
}
layer {
  name: "to_real_disp_loss"
  type: "Eltwise"
  bottom: "zoom_disp_loss0"
  top: "zoom_disp_loss0_real"
  propagate_down: false
  eltwise_param {
    operation: SUM
    coeff: 1
  }
}
layer {
  name: "to_real_downup_loss"
  type: "Eltwise"
  bottom: "down_up_loss"
  top: "down_up_loss_real"
  propagate_down: false
  eltwise_param {
    operation: SUM
    coeff: 1
  }
}
layer {
  name: "to_real_downup_loss"
  type: "Eltwise"
  bottom: "down_up_half_loss"
  top: "down_up_half_loss_real"
  propagate_down: false
  eltwise_param {
    operation: SUM
    coeff: 1
  }
}
I0814 08:07:37.373128 28599 layer_factory.hpp:77] Creating layer CustomData2
I0814 08:07:37.373137 28599 net.cpp:91] Creating Layer CustomData2
I0814 08:07:37.373142 28599 net.cpp:404] CustomData2 -> blob0
I0814 08:07:37.373148 28599 net.cpp:404] CustomData2 -> blob1
I0814 08:07:37.373153 28599 net.cpp:404] CustomData2 -> blob2
I0814 08:07:37.387233 28599 custom_data_layer.cpp:364] LMDB: Cleaned 0 stale readers.
I0814 08:07:37.387256 28599 custom_data_layer.cpp:373] Opening lmdb /media/leo/game/dispflownet-release/data/FlyingThings3D_release_TEST_clean_lmdb
I0814 08:07:37.387260 28599 custom_data_layer.cpp:379] DB entries: 4334
I0814 08:07:37.387285 28599 custom_data_layer.cpp:437] Data range: 0 to 4333 = 4334 entries.
23 15654 15880 3992 15515 21741 1403 13398 11304 17993 3906 13874 5553 4655 21924 21758 14171 18424 17219 8297 2895 20978 18577 21275 9041 20392 8243 5211 7566 21635 4653 7564 9490 4112 15165 8561 13361 20599 5807 20642 2878 315 10467 2146 11109 12963 8745 15312 10793 16406 13680 12312 8091 20806 15429 4963 12143 18009 8178 16802 548 20244 17179 12975 7577 16228 10262 20641 4322 6968 18392 13531 1806 1443 7153 6761 12569 2696 21163 4693 14507 7857 8902 20368 10045 13862 21131 14214 13308 12061 8796 15195 7368 18957 12723 18630 18319 3554 6908 18823 11711 16680 1000 2320 19184 18329 14774 18725 7040 17459 6305 18296 17991 9714 13301 11882 18858 18132 1426 915 10488 146 1897 2094 4283 18350 13352 5593 7912 15022 6570 14633 8578 15999 21795 7063 15215 19808 17326 14570 19927 5744 12503 20635 17554 17418 986 1823 21664 9380 8412 17927 21490 348 10294 6436 12973 3544 435 1349 10494 4918 8867 3262 13703 5341 10624 7017 7025 8933 5677 13664 6133 13804 11512 11055 17994 5512 13590 5524 1474 19253 12789 11827 20437 3960 3 13757 17293 19866 20850 8668 1242 13625 17611 15694 2253 3407 9930 3161 171 7038 15850 4280 6673 3057 13419 20689 6235 18300 2978 11083 16029 11796 16783 454 19689 3500 5067 15076 13687 1352 17460 416 8530 10086 6832 19557 13745 6653 7321 7923 16828 1968 18071 3898 5086 16507 12080 17759 14297 8951 11018 43 10585 9641 13859 11841 8413 1455 5097 3693 12215 9990 19030 7294 1942 16330 81 10657 19847 11126 5580 10256 19489 3213 20380 518 5534 11525 11030 8360 8397 129 12921 9154 19731 1183 10915 12594 10005 6566 17574 21043 14291 11883 9876 16884 11390 6102 3427 21069 8000 21695 17139 17612 20313 4626 7973 12911 4333 12019 6372 13466 10255 4662 651 19916 19688 4238 20347 606 16019 3927 5697 8564 5804 6599 6204 4419 18439 9785 14592 13151 12863 15973 17147 11019 15134 20001 10143 5854 5247 11587 2546 10200 7237 7235 20403 516 4184 13287 7967 4745 9236 13821 8349 21269 6979 41 4186 14710 5642 19579 15932 2617 120 19278 18926 16094 3090 1303 14764 20382 1052 10449 17281 1393 955 4612 7161 16570 5033 1504 17052 19645 14600 8472 16898 12519 114 1464 341 17836 17960 19665 7371 14891 
Permutation:
1519 173 3765 1121 2041 4333 1653 1213 1731 1649 3977 192 4092 1236 2118 1617 1802 1671 1753 3070 3191 3166 615 2318 2665 527 3169 2579 2559 4190 451 1960 166 2600 239 126 104 2839 1109 2081 3339 286 412 3400 3871 2366 710 2706 2883 1781 4066 4262 2603 3897 3562 3066 1065 2573 2218 1249 1497 2596 1376 4148 63 1135 1201 2349 3160 3326 549 1824 2884 820 1857 2269 2071 2854 2828 3174 3879 4008 754 1337 646 1629 936 1053 783 2435 79 1927 217 3369 3587 2589 2571 3330 2096 1021 3592 1610 3734 1045 2292 1722 1522 2617 598 1929 1601 727 3493 2937 2837 4103 4105 1312 1147 922 594 116 899 896 539 2538 216 3401 760 3833 3272 950 2229 3092 91 3288 1218 1392 3538 621 2623 1953 1144 3754 3949 3129 4057 3286 2784 1801 1341 2135 3608 1891 3438 2849 3341 3607 2161 4251 1436 2424 49 2787 1970 2575 4063 1562 1064 3262 1210 1772 4037 3907 2178 1150 2261 735 3588 3226 706 1768 1920 961 1237 1914 4015 169 591 3547 1375 1094 1754 1186 2967 726 1585 3920 3042 3689 190 1410 4151 3189 1884 2788 1192 1142 2718 3450 573 755 2625 1363 2099 2477 60 2421 337 1268 73 509 3926 1667 1985 4019 3175 2537 4281 285 3473 3202 2970 2976 917 3363 1037 3993 291 4097 205 3212 1193 3324 3073 3366 819 296 36 2392 2950 3131 1217 3279 3057 3414 440 773 3784 1613 734 3887 1496 1933 2756 1315 2952 3571 625 2878 3859 2529 607 1417 2779 3908 587 3086 2228 3549 4225 2064 4027 4317 4189 537 1957 2593 2470 2638 3761 940 426 3260 1471 2439 187 3332 1158 3067 1545 3141 3361 2495 1747 2188 3036 3282 374 1303 1106 3774 2154 3027 2560 4155 2165 2425 2207 1124 4212 600 167 3664 1697 1537 2321 316 3110 2395 2181 367 2499 1977 224 3206 390 942 563 1608 3068 2820 3630 991 4233 1941 3500 742 2964 678 1339 1398 1048 1400 1095 1614 3313 3329 449 2436 1558 2004 2089 3874 2290 1195 1742 2337 4177 2199 2949 3678 3995 1796 1423 713 2217 95 3130 1878 1016 3979 2400 2811 1990 3721 2168 2679 4173 3558 1968 2872 2227 438 4093 502 1062 1592 1328 356 1611 3434 1346 2520 3581 1783 3916 4215 1704 663 2058 1479 840 220 1415 2522 1892 1445 690 4086 2236 4118 1637 6 3443 2944 1017 1335 2211 1332 3244 3511 2526 2616 3462 1057 2245 3367 342 4146 1483 1198 2844 2362 2053 1721 3251 3354 1561 1854 1138 1823 3791 2434 465 2013 1973 1378 354 3527 2748 766 511 1822 924 326 708 2566 1706 2798 3946 385 2158 4130 682 80 1750 1813 4213 3362 2930 4123 4071 1180 1531 2709 44 2534 3923 248 2489 2251 1690 811 2918 2694 3427 3457 3145 1565 3357 3149 2413 3381 3247 1448 3126 2544 3 1389 254 85 1488 1864 2714 2146 134 3008 1856 2286 1275 868 366 1355 2415 3723 1468 1809 2661 1446 1883 2191 4070 1230 4328 2102 2739 130 3590 2456 2848 3395 995 2001 1203 1805 4114 3449 4292 1530 529 3663 1770 3872 4200 4115 2257 1715 2810 4332 829 1516 1128 325 858 3444 287 2078 7 2253 2634 3115 2197 103 2110 609 4142 2237 157 3217 421 1191 3704 2656 2346 642 2765 1719 2464 1661 666 1680 3242 864 2309 1126 2767 2681 264 972 292 2442 604 249 1580 2271 3018 47 1683 1512 3323 2809 648 1402 1347 1434 2715 1477 2007 1917 3828 1091 470 2881 4049 1631 3480 13 2791 1792 2288 595 1876 4316 898 1212 673 1313 3184 4265 1385 2528 3469 2707 3075 1641 2587 1182 1907 2838 771 4293 652 3231 1861 4264 592 1307 101 3290 941 1858 3105 1678 3153 2958 4117 877 3309 4276 2076 2170 2678 1425 1294 1405 2819 2430 2540 3807 3509 3506 1486 119 1730 1931 2786 3364 3914 1552 2862 1759 780 4150 324 3948 3616 2513 3672 3264 2503 1270 3360 1113 238 2429 3489 910 2354 3541 1461 778 740 3986 321 1187 1287 1504 1728 1618 4268 2792 1709 2411 669 3952 320 712 369 447 590 420 885 2103 4135 2708 3960 1261 2388 532 1677 2015 1593 143 488 946 1932 2368 1354 628 2509 1408 3216 2604 211 3338 4102 1118 4193 4170 1450 1131 1428 3466 1650 1633 987 352 3113 2766 2855 1969 2830 2785 1308 198 413 756 1767 3825 1110 4238 4257 2476 2982 4143 1691 955 4139 871 1271 2471 817 2121 2502 3416 3744 1583 1626 256 1382 3947 845 351 2642 2631 1262 408 2242 617 3670 484 3698 1074 1070 3545 2259 2144 2835 1447 2597 2816 3795 3460 1226 818 4284 1491 2813 3311 2057 1243 3019 2778 733 2517 524 1157 2705 3204 105 813 2921 3476 874 2808 2417 78 2831 363 1480 1433 944 4291 4091 593 139 2010 4084 814 1190 2493 2412 2214 2326 907 3428 3041 2056 2651 3532 2381 1327 3168 1821 4035 1744 2523 3417 1342 1459 1204 2127 1554 40 3913 890 208 3702 976 2462 2558 1362 915 405 2157 2494 1273 2585 3978 2149 2151 51 1296 1194 4024 4204 3049 1441 3011 2260 4208 2591 2386 1111 4055 2094 4329 188 634 2313 3724 3835 1587 4054 1258 2814 3767 23 2122 3971 3283 3794 3786 2781 1330 1412 582 434 4301 3760 251 2942 3886 2407 2226 2834 3255 266 1304 3015 937 1831 1834 3000 1797 3788 3482 1648 469 2672 3347 567 1051 429 4162 3422 627 43 209 3725 1844 1280 452 2997 3613 2107 54 3451 3374 3975 1621 911 191 2306 2725 236 2414 1749 495 3781 3351 1961 1776 3598 2697 990 1406 1222 1946 3127 2091 1411 2673 3850 302 2983 992 2480 3537 3111 2014 2030 3072 2590 146 4323 901 536 1364 3840 3140 2311 802 4192 1371 809 212 399 1229 503 3718 3053 1018 3846 2235 2695 2195 1256 2116 2171 2941 3077 782 3306 3988 1432 1039 275 1041 1915 225 3452 859 2630 904 453 1300 3349 300 15 1835 1899 748 252 3621 3252 2033 3248 639 312 2082 3726 1184 1159 3406 3327 1702 2547 3425 151 2457 4001 3555 705 759 3157 2068 3176 3156 3384 117 2622 1651 2167 3966 4109 491 772 854 3854 1553 1711 1538 1290 3666 3747 2036 298 2213 2281 2911 3312 2404 3832 2553 1357 154 2969 3240 1815 303 1923 2244 1930 1529 3573 3961 4000 3233 2092 2416 1279 913 3386 1524 3353 4287 1672 2141 400 560 3142 4152 3853 1503 3284 4168 2051 345 2975 2473 1645 3484 3576 3139 2093 1700 2953 2312 498 4230 4154 2564 963 2059 1575 597 395 2348 2256 1584 3646 998 3520 4045 1458 664 394 3601 2900 2212 4065 1508 2649 4331 199 3135 243 810 4314 262 213 2451 30 4240 2981 3756 1467 383 3334 3984 2543 3873 2974 1978 328 1061 1829 3164 3319 831 3785 3159 3526 2319 4249 2189 4167 4199 240 2545 1853 1901 2448 3579 3868 384 3715 334 1202 14 32 3225 3631 2996 1209 3955 512 443 168 1297 1403 700 1419 3031 2475 3133 3039 3732 3964 4207 2266 1247 1465 1133 761 1188 994 3839 1498 1947 1619 2061 2896 1902 2977 3685 1248 338 3483 3342 12 1228 1154 475 855 4050 1295 1534 4243 949 2583 3298 3660 2535 2182 2032 38 1975 882 2539 2231 3647 1439 3097 135 2610 945 1225 3090 1343 2647 1349 1616 3305 3230 3900 821 3600 1281 3446 1833 997 339 2202 3996 4120 4034 2561 1595 2192 3278 3301 1024 2355 3229 435 2519 2088 3695 2902 124 3905 2267 3442 1321 98 3793 1836 2762 996 1019 665 3193 3078 1942 294 2793 4104 1738 1762 880 1784 2002 1657 1358 3001 1108 1055 4083 1707 4072 2063 1163 289 827 1951 3463 3054 2185 1684 1252 3909 873 3686 1149 2657 3448 2780 201 2304 3709 221 1485 398 1995 1717 1453 3383 3932 636 427 3959 2920 3787 3938 1517 866 1989 3809 1874 3296 1938 3644 0 1693 3316 3910 2889 3803 3148 1060 3125 3308 1104 2859 3796 4234 3024 1391 3479 3167 2689 1005 265 1981 3605 3911 2454 1324 1579 4040 889 3939 977 2840 2390 1042 1179 1948 3745 1622 389 552 2163 1081 1493 4277 424 1115 1752 2852 3677 1168 3270 2877 2006 3088 2682 739 1543 203 1473 2377 1274 307 2549 2275 568 1040 2334 3408 732 4272 340 3407 2240 1276 2685 1646 3834 2615 1514 3269 2879 1333 2510 1998 1898 3536 1866 4099 4295 2378 322 947 158 738 692 2711 841 1662 3771 175 3030 3250 4023 2500 849 3564 3355 2085 3847 226 3836 2893 3675 3638 2198 1404 3471 3056 3287 88 4017 1390 3582 3004 569 768 2753 422 2928 2683 659 1031 3826 4187 1632 1090 3705 2115 242 522 1320 3020 1535 1129 2845 4181 1965 2888 2608 377 2026 905 2555 231 3609 2263 4094 4222 3365 432 1022 4004 803 1569 319 969 725 629 2769 343 3542 155 1316 1219 605 237 2399 3081 3842 1652 2934 792 2459 1185 3380 4179 393 3534 2155 932 1542 3232 3214 3303 2527 2037 132 2343 2418 985 4227 2295 2621 3719 3172 959 2254 4254 31 4014 2098 2324 2948 3814 2637 1935 3998 3388 500 2248 1235 1093 3317 4030 2978 3094 613 1570 282 2083 670 912 3477 2376 2984 1521 3348 638 207 2874 3691 17 1773 3820 110 1825 1663 3178 2333 2369 9 2581 875 939 2164 2356 4285 1082 691 1101 956 186 1267 918 3904 4229 3716 3980 2994 2858 4202 3143 3681 409 1766 2653 414 736 2184 1943 3158 1311 2655 2890 4067 3165 402 2865 1388 960 644 3580 2302 2824 2670 1963 1875 1877 3328 1365 603 1987 2332 815 2734 2126 623 228 1054 355 3146 1029 2233 620 2265 2173 2973 2444 3841 223 2801 4269 268 3848 3544 1130 4047 2915 3925 1566 1828 382 3415 2498 1178 2351 929 1949 657 3116 3736 1205 1455 3852 4296 767 2325 867 2710 4205 926 1712 3559 3987 631 2659 3799 1380 2652 417 660 1387 4267 1674 865 1409 3257 2598 2420 3917 2774 3038 3662 1905 1964 2140 2556 4279 202 1253 3758 4319 487 2249 3268 4129 3315 1642 3934 3743 1034 3453 283 3958 3209 3350 4131 3336 2287 3737 3624 2359 2738 3394 2870 570 1301 4064 33 2490 1869 3379 3985 193 4260 3397 3447 1189 546 2084 626 2917 2886 4253 3655 4172 3640 2577 1394 1696 84 4081 785 2599 2606 2258 196 630 50 2818 3046 4116 3409 1049 279 3385 3370 872 848 3467 1176 3741 1338 4286 599 513 1800 3963 3523 3851 523 71 4307 142 707 3775 3465 1906 4138 145 2955 2891 3731 789 3426 1560 313 3518 392 2241 534 763 4113 3717 3922 3052 3120 1748 1574 3213 3629 2931 3412 2402 790 1215 2533 1511 4263 1096 579 311 2861 1418 2823 97 507 2479 906 2620 1699 862 2035 3047 1780 1692 694 2987 230 1344 4315 2077 1046 2806 353 515 3876 3121 1832 3161 2340 1658 654 1687 1755 457 2578 580 454 2027 315 2771 2847 1540 2995 57 93 387 4044 1689 541 1983 1791 3441 3079 1567 1909 4310 1999 3919 3935 4007 3994 1143 3393 433 1011 1873 1589 1886 2531 1725 3749 3657 1890 614 1266 3843 1278 4046 1778 439 3345 3151 65 3222 3433 2134 2179 2487 4327 4297 2142 2612 561 577 953 683 25 1786 293 1077 3811 2925 698 3918 3100 2040 1462 927 3817 2506 3957 1317 261 3997 1841 3789 1660 348 1087 1259 4036 3398 958 962 505 1340 2419 952 2307 165 2940 161 1120 612 1984 2124 2654 4147 3236 1183 3490 892 4289 327 1777 2049 2992 332 1664 1967 461 2361 3735 121 136 274 501 1896 1851 1125 3514 571 1775 1895 3228 3769 3059 4056 3331 371 2445 376 1181 2749 2514 2303 1012 2449 1794 1745 1310 1714 578 2717 3507 2147 2629 1426 1936 3606 335 2693 1494 179 3641 2626 3194 2139 2663 542 3276 301 2671 381 4003 1733 519 1956 4300 3010 714 3634 3103 2832 127 1002 3271 3314 1782 1470 3486 111 3898 3759 2880 2021 3711 3294 1359 2565 2328 2956 1302 3945 4061 1944 1528 1197 674 828 2028 1708 764 1723 2692 1659 4247 1424 3403 661 229 2914 3693 177 3614 4325 1298 3550 2018 3813 3981 3273 3162 4088 4244 2732 2365 2827 1123 125 34 3083 410 842 3525 3533 2316 2299 3289 988 2491 2524 1607 1847 2857 1798 1170 2373 1020 3468 3382 1870 114 3604 3007 861 1958 3389 2372 3502 3456 3122 3830 1200 2919 3878 1827 3645 2712 824 415 4246 3205 4175 45 35 745 2262 1586 806 3902 2385 250 1032 460 3200 3025 441 832 914 1551 3207 1790 2272 329 1598 1227 462 3281 90 3392 793 195 2 1414 3187 2342 2829 2350 883 2238 2204 4236 64 1166 4242 1849 791 4266 3501 2438 635 200 416 1336 2067 3940 514 3494 1329 574 1484 1988 3275 1430 3563 2515 150 2205 2658 141 909 3969 1174 3529 3937 2364 1654 1260 3089 3838 18 56 2936 4214 3823 123 2794 10 3831 757 2408 3065 2586 2252 645 3972 876 2143 1500 2278 77 2200 1563 4195 1765 2353 4022 2079 2619 2174 2508 3968 4089 4312 2405 2993 1952 3703 3585 3028 2669 3488 3967 2072 113 4052 109 3093 2358 1216 3293 1739 359 4060 1102 4098 1134 720 4283 970 3458 272 816 401 3234 468 2481 822 4 1429 836 1047 2841 4178 3710 1838 1175 3819 3261 2842 1916 3589 2112 1962 4095 1971 1550 138 3578 2728 3210 2511 697 1035 688 448 2726 984 2339 1013 1068 1588 517 2474 2703 480 4078 1571 1549 723 581 4087 1594 1151 472 3307 1811 1366 650 42 857 276 695 347 52 1435 2100 3885 3373 3485 1145 72 3912 1602 2341 731 3190 1050 3051 1103 2961 2773 2790 2675 878 218 4069 3596 526 3376 3454 1263 3132 968 2016 396 3933 3855 4145 1889 2777 1996 3346 67 1 1720 378 1245 3762 2022 1615 1568 2344 3635 185 1460 2985 1729 2939 3003 1146 4184 3487 3792 3999 596 2821 2391 3299 2301 4211 2148 2817 3497 2568 1407 181 2897 902 3002 2132 1283 1979 4304 1894 4278 762 1257 1992 2447 1420 1466 4100 2747 3866 3387 2860 3637 2452 1536 3974 4132 3223 3768 3642 796 2446 2131 2410 3941 1277 471 2763 687 3418 2308 2031 4252 1839 3076 2614 3337 716 2210 3943 1515 1056 215 3665 3371 3291 2450 2922 686 76 270 3295 1954 533 903 2776 2268 1427 2713 3865 550 3924 1489 2803 2628 2193 4182 178 2336 2648 1513 1331 1596 1826 2397 170 1440 2636 1116 2038 3080 3459 3258 1003 1059 2882 2988 3658 1001 3138 1421 1997 3729 2497 3991 967 566 3790 3023 3091 4306 2990 2315 1401 794 4169 833 5 4122 1014 1381 777 4290 2323 3553 3824 2300 2466 3437 4075 863 719 3593 1527 4223 2012 572 835 2338 75 608 1482 2968 3254 473 3037 278 2764 586 4270 147 3669 450 3763 981 3181 3751 476 350 2650 3560 2745 1986 81 1638 4259 983 3849 2736 1665 602 3063 2640 2276 730 4197 2483 4221 4016 330 2293 3936 1323 2426 4111 3182 1763 1807 1284 1679 1073 1086 807 3173 149 2203 2453 2080 2133 2177 1928 1846 3566 2187 693 1239 446 4206 3499 4203 2871 411 3727 3706 3671 4020 3714 1475 1872 2427 3892 3956 4058 1505 1597 160 2234 746 1688 2554 3440 3368 489 1472 1925 2850 3584 2743 89 1888 2137 2065 3040 107 1863 826 318 2746 3277 701 2512 1208 1581 3044 3170 2757 2310 372 1643 2954 299 1169 1993 1444 214 37 2352 288 317 131 3404 4005 3237 4288 1903 1945 797 2219 787 86 3628 753 1071 999 1286 2796 3815 3930 2998 1288 3782 458 425 943 4183 1994 3856 4127 1282 2159 3927 585 2437 4082 1452 1564 933 689 3420 281 206 1393 850 3639 3515 1026 2929 2455 1758 456 2645 3611 4149 2701 3780 2166 3568 4163 4298 3114 2289 1044 925 1599 2960 4112 3901 649 3539 2702 3530 3128 1437 3192 3586 2909 4038 1695 4026 3679 2180 851 2875 474 2054 3152 3101 106 2772 2908 709 3869 2742 4313 853 4157 3322 562 1900 481 3498 2050 8 2959 2250 3155 3894 1885 1694 3410 965 1525 1289 1231 1161 3320 2152 1808 3435 1627 1352 19 1868 2609 2109 2770 2635 3636 1080 894 3649 1396 2371 3583 4311 3227 430 3808 1015 1360 1177 3186 1463 1845 2986 4051 2641 679 4160 1457 3962 3006 1843 1675 1469 3310 4013 886 3548 341 702 1478 2046 4039 4198 4273 2525 1167 1926 464 4330 2825 1122 3325 1224 2876 1573 310 1628 82 3543 1207 2225 258 3436 3390 4166 1374 1156 3557 3134 993 1819 3827 1771 2691 492 2255 704 1774 1241 62 3099 1751 358 1368 2905 333 4140 2965 1139 3423 1100 2846 2467 2567 971 4110 2735 2907 1098 2375 4261 3297 4220 129 4133 3058 1910 800 244 253 1132 3171 538 2812 3713 2536 3890 2114 423 3597 2662 884 508 1160 344 2387 4043 1058 3753 2775 1818 3522 1555 1416 564 931 48 156 3861 3561 4041 4320 148 182 2518 68 696 1582 3517 485 3595 1840 4318 163 3195 2507 2563 3554 4128 2145 2624 2029 2802 11 3648 1264 2393 1072 184 3033 1367 3280 2111 2885 1605 1148 973 3742 2979 3219 3599 3504 373 923 3746 1939 3118 1701 2962 2327 2469 1509 3154 3845 2570 1578 2605 1666 3570 1727 684 540 1532 2927 1233 543 4194 3399 3603 4106 3863 2162 1640 3577 3764 2894 1859 805 444 3239 3074 2282 1506 2856 2789 3503 2611 2169 1533 1449 1165 164 632 1373 4073 1682 583 916 1232 3694 2194 375 685 2601 2160 2097 3150 364 1793 3372 2643 555 1735 4239 4159 2101 2664 2396 3942 1518 774 2562 1476 2946 3124 2592 506 92 852 1557 2660 989 2557 1137 521 1991 3421 1795 1919 2034 520 3712 3730 1520 4121 938 3062 2668 497 2760 189 2331 2335 2201 259 2468 2086 1746 3106 21 3864 729 3069 1703 4275 4076 1196 948 2627 1164 747 3180 1136 2867 3029 368 2868 4218 1669 406 616 624 1067 801 1724 3707 2991 255 1246 2892 675 1431 3026 2285 837 3510 1686 2686 601 3123 2569 1559 1636 1413 4012 3352 718 3810 830 2901 1698 834 3777 1912 3673 2360 1495 2432 3667 3659 27 2423 232 2232 2153 2887 2805 3249 1318 3188 3857 3235 1572 1881 2296 2916 3651 3728 1982 3048 3739 856 436 2108 3844 2441 781 750 22 1934 1269 3740 4185 4010 3822 1539 2720 2298 1309 2639 1351 3513 219 3818 3778 3179 3009 715 3755 548 1314 559 2667 4134 3612 1244 2297 1830 3757 3652 1397 847 3992 2945 717 295 3071 2380 1955 1604 4090 2684 3877 1817 172 3034 2129 2588 2799 3944 3858 3921 2070 2125 545 2595 838 3690 2999 1372 2389 1112 3990 24 1630 3445 3014 3478 284 3575 3591 2815 1804 2783 4011 3954 1084 2899 1117 908 455 788 2317 2923 3084 743 3674 1974 3432 2768 2150 1127 2440 655 2723 895 3429 3107 1272 3565 4062 482 365 3680 4164 2504 128 2963 2926 1075 3211 204 4059 2584 2530 3021 4258 2463 3656 3633 4141 4144 346 4282 2516 4209 4029 4186 677 2382 1779 2060 1913 3461 2521 269 869 1043 846 260 46 1119 934 3253 349 2186 3108 2119 1760 3496 4077 1710 3797 3881 2120 87 2912 2087 2384 20 1379 1732 227 897 3481 2374 919 565 2322 391 2208 2433 2370 744 676 2727 2230 951 844 3304 535 3654 1897 305 2106 4101 2320 2277 1386 4042 3552 2674 2548 2807 3915 2379 2898 900 1788 2284 2633 2216 584 1027 4107 1173 776 478 2329 4053 2729 3256 120 3610 2003 2017 3109 637 2270 2676 297 3243 171 2314 2104 2957 2123 3619 4248 271 1756 3196 235 651 1254 3906 893 1972 2797 3661 361 2532 576 1620 2239 658 3798 2113 1097 3472 2136 799 3431 2980 3177 3224 2550 407 647 2247 96 3343 770 798 3265 3333 2443 28 222 3929 2731 2039 1474 1291 3738 3650 1924 3668 556 1454 241 3087 1848 2505 957 2680 3005 1141 633 4079 1078 1716 3246 4153 2719 804 611 1009 2913 1852 3524 4025 2873 1171 3643 3870 3802 1079 1673 336 234 69 2073 3973 2294 397 3772 2175 1871 3318 245 2025 4096 1879 1008 728 2552 2833 3632 2000 4228 2933 3899 2486 3285 3752 3982 112 3095 3748 751 2223 2932 1023 2291 2485 2075 2488 4171 2305 641 1590 1523 2138 1499 4308 3829 2602 544 4217 1681 314 4174 144 2947 2924 1238 418 2761 4018 610 1980 1803 2069 3720 1092 1880 1223 2730 1502 3816 3546 1882 3492 640 3682 1438 2383 622 357 3556 1741 3862 1761 3117 323 2800 3770 653 1153 1155 2190 721 3976 888 1442 4224 183 437 2582 784 1544 115 1305 3215 3773 662 4226 3951 2864 1214 812 618 1306 3050 1356 2458 4196 3396 1162 2465 4108 3241 3896 4124 2220 2741 2618 1501 1820 930 3112 1319 2347 2699 3783 510 1370 1464 445 1443 3701 891 3060 4210 737 1705 2704 3821 3521 2401 1655 277 2280 3430 879 3800 3356 39 2062 3391 1976 1199 1787 2422 380 3411 404 1918 2224 1609 466 176 1850 1799 2273 2752 2398 2666 986 1656 1036 795 2172 2677 486 379 516 1353 656 881 1634 1526 180 2492 1812 431 3082 3263 3470 3375 3653 3508 4031 1492 531 1487 4080 2754 1172 722 3495 672 839 3259 3359 2851 3875 1547 975 606 4232 1685 2055 823 1624 4119 668 3266 3475 1814 978 2853 1676 3221 1299 4136 1806 1507 1837 1326 2943 3455 699 1940 1234 2904 3274 4309 3358 1350 2357 304 3696 4032 3012 3567 887 2750 1893 1265 775 2938 1577 530 4074 1007 3699 1937 2367 3883 2759 3867 194 83 290 3569 94 1006 808 3733 1004 3208I0814 08:07:37.389566 28599 custom_data_layer.cpp:541] output 0 data size: 1,3,540,960
I0814 08:07:37.389607 28599 custom_data_layer.cpp:541] output 1 data size: 1,3,540,960
I0814 08:07:37.389628 28599 custom_data_layer.cpp:541] output 2 data size: 1,1,540,960
I0814 08:07:37.399215 28599 net.cpp:144] Setting up CustomData2
I0814 08:07:37.399263 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399269 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399273 28599 net.cpp:151] Top shape: 1 1 540 960 (518400)
I0814 08:07:37.399277 28599 net.cpp:159] Memory required for data: 14515200
I0814 08:07:37.399286 28599 layer_factory.hpp:77] Creating layer blob0_CustomData2_0_split
I0814 08:07:37.399304 28599 net.cpp:91] Creating Layer blob0_CustomData2_0_split
I0814 08:07:37.399310 28599 net.cpp:432] blob0_CustomData2_0_split <- blob0
I0814 08:07:37.399319 28599 net.cpp:404] blob0_CustomData2_0_split -> blob0_CustomData2_0_split_0
I0814 08:07:37.399333 28599 net.cpp:404] blob0_CustomData2_0_split -> blob0_CustomData2_0_split_1
I0814 08:07:37.399487 28599 net.cpp:144] Setting up blob0_CustomData2_0_split
I0814 08:07:37.399492 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399495 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399498 28599 net.cpp:159] Memory required for data: 26956800
I0814 08:07:37.399502 28599 layer_factory.hpp:77] Creating layer blob1_CustomData2_1_split
I0814 08:07:37.399507 28599 net.cpp:91] Creating Layer blob1_CustomData2_1_split
I0814 08:07:37.399511 28599 net.cpp:432] blob1_CustomData2_1_split <- blob1
I0814 08:07:37.399516 28599 net.cpp:404] blob1_CustomData2_1_split -> blob1_CustomData2_1_split_0
I0814 08:07:37.399521 28599 net.cpp:404] blob1_CustomData2_1_split -> blob1_CustomData2_1_split_1
I0814 08:07:37.399551 28599 net.cpp:144] Setting up blob1_CustomData2_1_split
I0814 08:07:37.399588 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399592 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399595 28599 net.cpp:159] Memory required for data: 39398400
I0814 08:07:37.399598 28599 layer_factory.hpp:77] Creating layer blob2_CustomData2_2_split
I0814 08:07:37.399605 28599 net.cpp:91] Creating Layer blob2_CustomData2_2_split
I0814 08:07:37.399608 28599 net.cpp:432] blob2_CustomData2_2_split <- blob2
I0814 08:07:37.399612 28599 net.cpp:404] blob2_CustomData2_2_split -> blob2_CustomData2_2_split_0
I0814 08:07:37.399617 28599 net.cpp:404] blob2_CustomData2_2_split -> blob2_CustomData2_2_split_1
I0814 08:07:37.399651 28599 net.cpp:144] Setting up blob2_CustomData2_2_split
I0814 08:07:37.399654 28599 net.cpp:151] Top shape: 1 1 540 960 (518400)
I0814 08:07:37.399658 28599 net.cpp:151] Top shape: 1 1 540 960 (518400)
I0814 08:07:37.399662 28599 net.cpp:159] Memory required for data: 43545600
I0814 08:07:37.399664 28599 layer_factory.hpp:77] Creating layer Eltwise1
I0814 08:07:37.399674 28599 net.cpp:91] Creating Layer Eltwise1
I0814 08:07:37.399677 28599 net.cpp:432] Eltwise1 <- blob0_CustomData2_0_split_0
I0814 08:07:37.399682 28599 net.cpp:404] Eltwise1 -> blob3
I0814 08:07:37.399703 28599 net.cpp:144] Setting up Eltwise1
I0814 08:07:37.399708 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399710 28599 net.cpp:159] Memory required for data: 49766400
I0814 08:07:37.399713 28599 layer_factory.hpp:77] Creating layer blob3_Eltwise1_0_split
I0814 08:07:37.399719 28599 net.cpp:91] Creating Layer blob3_Eltwise1_0_split
I0814 08:07:37.399721 28599 net.cpp:432] blob3_Eltwise1_0_split <- blob3
I0814 08:07:37.399725 28599 net.cpp:404] blob3_Eltwise1_0_split -> blob3_Eltwise1_0_split_0
I0814 08:07:37.399730 28599 net.cpp:404] blob3_Eltwise1_0_split -> blob3_Eltwise1_0_split_1
I0814 08:07:37.399757 28599 net.cpp:144] Setting up blob3_Eltwise1_0_split
I0814 08:07:37.399761 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399765 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399766 28599 net.cpp:159] Memory required for data: 62208000
I0814 08:07:37.399770 28599 layer_factory.hpp:77] Creating layer Eltwise2
I0814 08:07:37.399776 28599 net.cpp:91] Creating Layer Eltwise2
I0814 08:07:37.399780 28599 net.cpp:432] Eltwise2 <- blob1_CustomData2_1_split_0
I0814 08:07:37.399783 28599 net.cpp:404] Eltwise2 -> blob4
I0814 08:07:37.399801 28599 net.cpp:144] Setting up Eltwise2
I0814 08:07:37.399806 28599 net.cpp:151] Top shape: 1 3 540 960 (1555200)
I0814 08:07:37.399807 28599 net.cpp:159] Memory required for data: 68428800
I0814 08:07:37.399811 28599 layer_factory.hpp:77] Creating layer img0s_aug
I0814 08:07:37.399826 28599 net.cpp:91] Creating Layer img0s_aug
I0814 08:07:37.399828 28599 net.cpp:432] img0s_aug <- blob3_Eltwise1_0_split_0
I0814 08:07:37.399834 28599 net.cpp:404] img0s_aug -> img0_aug
I0814 08:07:37.399844 28599 net.cpp:404] img0s_aug -> blob6
W0814 08:07:37.399852 28599 data_augmentation_layer.cpp:40] DataAugmentationLayer only runs Reshape on setup
I0814 08:07:37.399862 28599 data_augmentation_layer.cpp:45] Recompute mean
W0814 08:07:37.400001 28599 data_augmentation_layer.cpp:77] Reshape of Augmentation layer should only be called once? Check this
I0814 08:07:37.400053 28599 data_augmentation_layer.cpp:123] Emitting 42 augmentation params
I0814 08:07:37.401242 28599 net.cpp:144] Setting up img0s_aug
I0814 08:07:37.401286 28599 net.cpp:151] Top shape: 1 3 384 768 (884736)
I0814 08:07:37.401290 28599 net.cpp:151] Top shape: 1 42 1 1 (42)
I0814 08:07:37.401293 28599 net.cpp:159] Memory required for data: 71967912
I0814 08:07:37.401314 28599 layer_factory.hpp:77] Creating layer img0_aug_img0s_aug_0_split
I0814 08:07:37.401332 28599 net.cpp:91] Creating Layer img0_aug_img0s_aug_0_split
I0814 08:07:37.401337 28599 net.cpp:432] img0_aug_img0s_aug_0_split <- img0_aug
I0814 08:07:37.401345 28599 net.cpp:404] img0_aug_img0s_aug_0_split -> img0_aug_img0s_aug_0_split_0
I0814 08:07:37.401383 28599 net.cpp:404] img0_aug_img0s_aug_0_split -> img0_aug_img0s_aug_0_split_1
I0814 08:07:37.401433 28599 net.cpp:144] Setting up img0_aug_img0s_aug_0_split
I0814 08:07:37.401438 28599 net.cpp:151] Top shape: 1 3 384 768 (884736)
I0814 08:07:37.401442 28599 net.cpp:151] Top shape: 1 3 384 768 (884736)
I0814 08:07:37.401444 28599 net.cpp:159] Memory required for data: 79045800
I0814 08:07:37.401448 28599 layer_factory.hpp:77] Creating layer blob6_img0s_aug_1_split
I0814 08:07:37.401453 28599 net.cpp:91] Creating Layer blob6_img0s_aug_1_split
I0814 08:07:37.401455 28599 net.cpp:432] blob6_img0s_aug_1_split <- blob6
I0814 08:07:37.401459 28599 net.cpp:404] blob6_img0s_aug_1_split -> blob6_img0s_aug_1_split_0
I0814 08:07:37.401464 28599 net.cpp:404] blob6_img0s_aug_1_split -> blob6_img0s_aug_1_split_1
I0814 08:07:37.401492 28599 net.cpp:144] Setting up blob6_img0s_aug_1_split
I0814 08:07:37.401496 28599 net.cpp:151] Top shape: 1 42 1 1 (42)
I0814 08:07:37.401500 28599 net.cpp:151] Top shape: 1 42 1 1 (42)
I0814 08:07:37.401502 28599 net.cpp:159] Memory required for data: 79046136
I0814 08:07:37.401505 28599 layer_factory.hpp:77] Creating layer aug_params1
I0814 08:07:37.401520 28599 net.cpp:91] Creating Layer aug_params1
I0814 08:07:37.401522 28599 net.cpp:432] aug_params1 <- blob6_img0s_aug_1_split_0
I0814 08:07:37.401526 28599 net.cpp:432] aug_params1 <- blob3_Eltwise1_0_split_1
I0814 08:07:37.401530 28599 net.cpp:432] aug_params1 <- img0_aug_img0s_aug_0_split_0
I0814 08:07:37.401535 28599 net.cpp:404] aug_params1 -> blob7
W0814 08:07:37.401542 28599 generate_augmentation_parameters_layer.cpp:36] GenerateAugmentationParametersLayer only runs Reshape only on setup
I0814 08:07:37.401556 28599 generate_augmentation_parameters_layer.cpp:68] mode: add
I0814 08:07:37.401576 28599 net.cpp:144] Setting up aug_params1
I0814 08:07:37.401579 28599 net.cpp:151] Top shape: 1 42 1 1 (42)
I0814 08:07:37.401582 28599 net.cpp:159] Memory required for data: 79046304
I0814 08:07:37.401585 28599 layer_factory.hpp:77] Creating layer blob7_aug_params1_0_split
I0814 08:07:37.401589 28599 net.cpp:91] Creating Layer blob7_aug_params1_0_split
I0814 08:07:37.401592 28599 net.cpp:432] blob7_aug_params1_0_split <- blob7
I0814 08:07:37.401597 28599 net.cpp:404] blob7_aug_params1_0_split -> blob7_aug_params1_0_split_0
I0814 08:07:37.401602 28599 net.cpp:404] blob7_aug_params1_0_split -> blob7_aug_params1_0_split_1
I0814 08:07:37.401628 28599 net.cpp:144] Setting up blob7_aug_params1_0_split
I0814 08:07:37.401633 28599 net.cpp:151] Top shape: 1 42 1 1 (42)
I0814 08:07:37.401634 28599 net.cpp:151] Top shape: 1 42 1 1 (42)
I0814 08:07:37.401638 28599 net.cpp:159] Memory required for data: 79046640
I0814 08:07:37.401640 28599 layer_factory.hpp:77] Creating layer img1s_aug
I0814 08:07:37.401648 28599 net.cpp:91] Creating Layer img1s_aug
I0814 08:07:37.401650 28599 net.cpp:432] img1s_aug <- blob4
I0814 08:07:37.401654 28599 net.cpp:432] img1s_aug <- blob7_aug_params1_0_split_0
I0814 08:07:37.401659 28599 net.cpp:404] img1s_aug -> img1_aug
W0814 08:07:37.401664 28599 data_augmentation_layer.cpp:40] DataAugmentationLayer only runs Reshape on setup
I0814 08:07:37.401669 28599 data_augmentation_layer.cpp:45] Recompute mean
W0814 08:07:37.401764 28599 data_augmentation_layer.cpp:77] Reshape of Augmentation layer should only be called once? Check this
I0814 08:07:37.401792 28599 data_augmentation_layer.cpp:116] Receiving 42 augmentation params
I0814 08:07:37.402593 28599 net.cpp:144] Setting up img1s_aug
I0814 08:07:37.402632 28599 net.cpp:151] Top shape: 1 3 384 768 (884736)
I0814 08:07:37.402637 28599 net.cpp:159] Memory required for data: 82585584
I0814 08:07:37.402653 28599 layer_factory.hpp:77] Creating layer DummyData2
I0814 08:07:37.402673 28599 net.cpp:91] Creating Layer DummyData2
I0814 08:07:37.402678 28599 net.cpp:404] DummyData2 -> blob9
I0814 08:07:37.403555 28599 net.cpp:144] Setting up DummyData2
I0814 08:07:37.403569 28599 net.cpp:151] Top shape: 1 1 540 960 (518400)
I0814 08:07:37.403573 28599 net.cpp:159] Memory required for data: 84659184
I0814 08:07:37.403591 28599 layer_factory.hpp:77] Creating layer blob9_DummyData2_0_split
I0814 08:07:37.403597 28599 net.cpp:91] Creating Layer blob9_DummyData2_0_split
I0814 08:07:37.403601 28599 net.cpp:432] blob9_DummyData2_0_split <- blob9
I0814 08:07:37.403606 28599 net.cpp:404] blob9_DummyData2_0_split -> blob9_DummyData2_0_split_0
I0814 08:07:37.403612 28599 net.cpp:404] blob9_DummyData2_0_split -> blob9_DummyData2_0_split_1
I0814 08:07:37.403656 28599 net.cpp:144] Setting up blob9_DummyData2_0_split
I0814 08:07:37.403659 28599 net.cpp:151] Top shape: 1 1 540 960 (518400)
I0814 08:07:37.403663 28599 net.cpp:151] Top shape: 1 1 540 960 (518400)
I0814 08:07:37.403666 28599 net.cpp:159] Memory required for data: 88806384
I0814 08:07:37.403667 28599 layer_factory.hpp:77] Creating layer Concat1
I0814 08:07:37.403673 28599 net.cpp:91] Creating Layer Concat1
I0814 08:07:37.403676 28599 net.cpp:432] Concat1 <- blob2_CustomData2_2_split_0
I0814 08:07:37.403681 28599 net.cpp:432] Concat1 <- blob9_DummyData2_0_split_0
I0814 08:07:37.403684 28599 net.cpp:404] Concat1 -> blob10
I0814 08:07:37.403702 28599 net.cpp:144] Setting up Concat1
I0814 08:07:37.403705 28599 net.cpp:151] Top shape: 1 2 540 960 (1036800)
I0814 08:07:37.403707 28599 net.cpp:159] Memory required for data: 92953584
I0814 08:07:37.403710 28599 layer_factory.hpp:77] Creating layer FlowAugmentation1
I0814 08:07:37.403718 28599 net.cpp:91] Creating Layer FlowAugmentation1
I0814 08:07:37.403722 28599 net.cpp:432] FlowAugmentation1 <- blob10
I0814 08:07:37.403724 28599 net.cpp:432] FlowAugmentation1 <- blob6_img0s_aug_1_split_1
I0814 08:07:37.403728 28599 net.cpp:432] FlowAugmentation1 <- blob7_aug_params1_0_split_1
I0814 08:07:37.403733 28599 net.cpp:404] FlowAugmentation1 -> blob11
W0814 08:07:37.403738 28599 flow_augmentation_layer.cpp:36] FlowAugmentationLayer only runs Reshape only on setup
I0814 08:07:37.403775 28599 net.cpp:144] Setting up FlowAugmentation1
I0814 08:07:37.403779 28599 net.cpp:151] Top shape: 1 2 384 768 (589824)
I0814 08:07:37.403781 28599 net.cpp:159] Memory required for data: 95312880
I0814 08:07:37.403784 28599 layer_factory.hpp:77] Creating layer Slice1
I0814 08:07:37.403790 28599 net.cpp:91] Creating Layer Slice1
I0814 08:07:37.403794 28599 net.cpp:432] Slice1 <- blob11
I0814 08:07:37.403797 28599 net.cpp:404] Slice1 -> disp_gt_aug
I0814 08:07:37.403802 28599 net.cpp:404] Slice1 -> blob13
I0814 08:07:37.403836 28599 net.cpp:144] Setting up Slice1
I0814 08:07:37.403839 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.403843 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.403846 28599 net.cpp:159] Memory required for data: 97672176
I0814 08:07:37.403848 28599 layer_factory.hpp:77] Creating layer ref_disp_scale
I0814 08:07:37.403854 28599 net.cpp:91] Creating Layer ref_disp_scale
I0814 08:07:37.403857 28599 net.cpp:404] ref_disp_scale -> ref_disp_scale
I0814 08:07:37.403905 28599 net.cpp:144] Setting up ref_disp_scale
I0814 08:07:37.403909 28599 net.cpp:151] Top shape: 1 1 96 192 (18432)
I0814 08:07:37.403913 28599 net.cpp:159] Memory required for data: 97745904
I0814 08:07:37.403914 28599 layer_factory.hpp:77] Creating layer ref_disp_scale_half
I0814 08:07:37.403919 28599 net.cpp:91] Creating Layer ref_disp_scale_half
I0814 08:07:37.403923 28599 net.cpp:404] ref_disp_scale_half -> ref_disp_scale_half
I0814 08:07:37.404434 28599 net.cpp:144] Setting up ref_disp_scale_half
I0814 08:07:37.404444 28599 net.cpp:151] Top shape: 1 4 192 384 (294912)
I0814 08:07:37.404448 28599 net.cpp:159] Memory required for data: 98925552
I0814 08:07:37.404450 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm
I0814 08:07:37.404456 28599 net.cpp:91] Creating Layer disp_gt_aug_norm
I0814 08:07:37.404459 28599 net.cpp:432] disp_gt_aug_norm <- disp_gt_aug
I0814 08:07:37.404464 28599 net.cpp:404] disp_gt_aug_norm -> disp_gt_aug_norm
I0814 08:07:37.404491 28599 net.cpp:144] Setting up disp_gt_aug_norm
I0814 08:07:37.404496 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404500 28599 net.cpp:159] Memory required for data: 100105200
I0814 08:07:37.404510 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm_disp_gt_aug_norm_0_split
I0814 08:07:37.404515 28599 net.cpp:91] Creating Layer disp_gt_aug_norm_disp_gt_aug_norm_0_split
I0814 08:07:37.404517 28599 net.cpp:432] disp_gt_aug_norm_disp_gt_aug_norm_0_split <- disp_gt_aug_norm
I0814 08:07:37.404521 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_0
I0814 08:07:37.404526 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_1
I0814 08:07:37.404531 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_2
I0814 08:07:37.404536 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_3
I0814 08:07:37.404539 28599 net.cpp:404] disp_gt_aug_norm_disp_gt_aug_norm_0_split -> disp_gt_aug_norm_disp_gt_aug_norm_0_split_4
I0814 08:07:37.404593 28599 net.cpp:144] Setting up disp_gt_aug_norm_disp_gt_aug_norm_0_split
I0814 08:07:37.404597 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404600 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404603 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404606 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404608 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404611 28599 net.cpp:159] Memory required for data: 106003440
I0814 08:07:37.404613 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm_down
I0814 08:07:37.404618 28599 net.cpp:91] Creating Layer disp_gt_aug_norm_down
I0814 08:07:37.404621 28599 net.cpp:432] disp_gt_aug_norm_down <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_0
I0814 08:07:37.404624 28599 net.cpp:432] disp_gt_aug_norm_down <- ref_disp_scale
I0814 08:07:37.404628 28599 net.cpp:404] disp_gt_aug_norm_down -> disp_gt_aug_norm_down
W0814 08:07:37.404633 28599 downsample_layer.cpp:25] DownsampleLayer only runs Reshape on setup
I0814 08:07:37.404651 28599 net.cpp:144] Setting up disp_gt_aug_norm_down
I0814 08:07:37.404655 28599 net.cpp:151] Top shape: 1 1 96 192 (18432)
I0814 08:07:37.404659 28599 net.cpp:159] Memory required for data: 106077168
I0814 08:07:37.404660 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split
I0814 08:07:37.404664 28599 net.cpp:91] Creating Layer disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split
I0814 08:07:37.404667 28599 net.cpp:432] disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split <- disp_gt_aug_norm_down
I0814 08:07:37.404671 28599 net.cpp:404] disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split -> disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split_0
I0814 08:07:37.404676 28599 net.cpp:404] disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split -> disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split_1
I0814 08:07:37.404701 28599 net.cpp:144] Setting up disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split
I0814 08:07:37.404705 28599 net.cpp:151] Top shape: 1 1 96 192 (18432)
I0814 08:07:37.404707 28599 net.cpp:151] Top shape: 1 1 96 192 (18432)
I0814 08:07:37.404711 28599 net.cpp:159] Memory required for data: 106224624
I0814 08:07:37.404712 28599 layer_factory.hpp:77] Creating layer disp_gt_aug_norm_down_half
I0814 08:07:37.404716 28599 net.cpp:91] Creating Layer disp_gt_aug_norm_down_half
I0814 08:07:37.404719 28599 net.cpp:432] disp_gt_aug_norm_down_half <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_1
I0814 08:07:37.404722 28599 net.cpp:432] disp_gt_aug_norm_down_half <- ref_disp_scale_half
I0814 08:07:37.404726 28599 net.cpp:404] disp_gt_aug_norm_down_half -> disp_gt_aug_norm_down_half
W0814 08:07:37.404731 28599 downsample_layer.cpp:25] DownsampleLayer only runs Reshape on setup
I0814 08:07:37.404747 28599 net.cpp:144] Setting up disp_gt_aug_norm_down_half
I0814 08:07:37.404750 28599 net.cpp:151] Top shape: 1 1 192 384 (73728)
I0814 08:07:37.404752 28599 net.cpp:159] Memory required for data: 106519536
I0814 08:07:37.404755 28599 layer_factory.hpp:77] Creating layer upsample_disparity_itr1_half
I0814 08:07:37.404764 28599 net.cpp:91] Creating Layer upsample_disparity_itr1_half
I0814 08:07:37.404768 28599 net.cpp:432] upsample_disparity_itr1_half <- disp_gt_aug_norm_down_half
I0814 08:07:37.404772 28599 net.cpp:404] upsample_disparity_itr1_half -> upsampled_disparity_itr1_half
W0814 08:07:37.404778 28599 resample_layer.cpp:28] ResampleLayer only runs Reshape on setup
I0814 08:07:37.404794 28599 net.cpp:144] Setting up upsample_disparity_itr1_half
I0814 08:07:37.404798 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404800 28599 net.cpp:159] Memory required for data: 107699184
I0814 08:07:37.404803 28599 layer_factory.hpp:77] Creating layer upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split
I0814 08:07:37.404806 28599 net.cpp:91] Creating Layer upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split
I0814 08:07:37.404809 28599 net.cpp:432] upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split <- upsampled_disparity_itr1_half
I0814 08:07:37.404814 28599 net.cpp:404] upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split -> upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split_0
I0814 08:07:37.404817 28599 net.cpp:404] upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split -> upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split_1
I0814 08:07:37.404842 28599 net.cpp:144] Setting up upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split
I0814 08:07:37.404846 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404850 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404851 28599 net.cpp:159] Memory required for data: 110058480
I0814 08:07:37.404855 28599 layer_factory.hpp:77] Creating layer Silence1
I0814 08:07:37.404857 28599 net.cpp:91] Creating Layer Silence1
I0814 08:07:37.404860 28599 net.cpp:432] Silence1 <- blob0_CustomData2_0_split_1
I0814 08:07:37.404865 28599 net.cpp:144] Setting up Silence1
I0814 08:07:37.404866 28599 net.cpp:159] Memory required for data: 110058480
I0814 08:07:37.404870 28599 layer_factory.hpp:77] Creating layer Silence2
I0814 08:07:37.404872 28599 net.cpp:91] Creating Layer Silence2
I0814 08:07:37.404875 28599 net.cpp:432] Silence2 <- blob1_CustomData2_1_split_1
I0814 08:07:37.404878 28599 net.cpp:144] Setting up Silence2
I0814 08:07:37.404881 28599 net.cpp:159] Memory required for data: 110058480
I0814 08:07:37.404882 28599 layer_factory.hpp:77] Creating layer Silence3
I0814 08:07:37.404886 28599 net.cpp:91] Creating Layer Silence3
I0814 08:07:37.404888 28599 net.cpp:432] Silence3 <- blob2_CustomData2_2_split_1
I0814 08:07:37.404891 28599 net.cpp:144] Setting up Silence3
I0814 08:07:37.404893 28599 net.cpp:159] Memory required for data: 110058480
I0814 08:07:37.404896 28599 layer_factory.hpp:77] Creating layer Silence4
I0814 08:07:37.404898 28599 net.cpp:91] Creating Layer Silence4
I0814 08:07:37.404901 28599 net.cpp:432] Silence4 <- blob9_DummyData2_0_split_1
I0814 08:07:37.404904 28599 net.cpp:144] Setting up Silence4
I0814 08:07:37.404906 28599 net.cpp:159] Memory required for data: 110058480
I0814 08:07:37.404908 28599 layer_factory.hpp:77] Creating layer Silence5
I0814 08:07:37.404912 28599 net.cpp:91] Creating Layer Silence5
I0814 08:07:37.404914 28599 net.cpp:432] Silence5 <- blob13
I0814 08:07:37.404917 28599 net.cpp:144] Setting up Silence5
I0814 08:07:37.404919 28599 net.cpp:159] Memory required for data: 110058480
I0814 08:07:37.404922 28599 layer_factory.hpp:77] Creating layer Silence6
I0814 08:07:37.404927 28599 net.cpp:91] Creating Layer Silence6
I0814 08:07:37.404929 28599 net.cpp:432] Silence6 <- img1_aug
I0814 08:07:37.404932 28599 net.cpp:144] Setting up Silence6
I0814 08:07:37.404934 28599 net.cpp:159] Memory required for data: 110058480
I0814 08:07:37.404937 28599 layer_factory.hpp:77] Creating layer upsample_disparity_itr1
I0814 08:07:37.404940 28599 net.cpp:91] Creating Layer upsample_disparity_itr1
I0814 08:07:37.404943 28599 net.cpp:432] upsample_disparity_itr1 <- disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split_0
I0814 08:07:37.404952 28599 net.cpp:404] upsample_disparity_itr1 -> upsampled_disparity_itr1
W0814 08:07:37.404956 28599 resample_layer.cpp:28] ResampleLayer only runs Reshape on setup
I0814 08:07:37.404974 28599 net.cpp:144] Setting up upsample_disparity_itr1
I0814 08:07:37.404978 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.404980 28599 net.cpp:159] Memory required for data: 111238128
I0814 08:07:37.404983 28599 layer_factory.hpp:77] Creating layer upsampled_disparity_itr1_upsample_disparity_itr1_0_split
I0814 08:07:37.404986 28599 net.cpp:91] Creating Layer upsampled_disparity_itr1_upsample_disparity_itr1_0_split
I0814 08:07:37.404989 28599 net.cpp:432] upsampled_disparity_itr1_upsample_disparity_itr1_0_split <- upsampled_disparity_itr1
I0814 08:07:37.404994 28599 net.cpp:404] upsampled_disparity_itr1_upsample_disparity_itr1_0_split -> upsampled_disparity_itr1_upsample_disparity_itr1_0_split_0
I0814 08:07:37.404999 28599 net.cpp:404] upsampled_disparity_itr1_upsample_disparity_itr1_0_split -> upsampled_disparity_itr1_upsample_disparity_itr1_0_split_1
I0814 08:07:37.405022 28599 net.cpp:144] Setting up upsampled_disparity_itr1_upsample_disparity_itr1_0_split
I0814 08:07:37.405025 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.405028 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.405031 28599 net.cpp:159] Memory required for data: 113597424
I0814 08:07:37.405033 28599 layer_factory.hpp:77] Creating layer upsample_disparity2x_itr1
I0814 08:07:37.405037 28599 net.cpp:91] Creating Layer upsample_disparity2x_itr1
I0814 08:07:37.405040 28599 net.cpp:432] upsample_disparity2x_itr1 <- disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split_1
I0814 08:07:37.405045 28599 net.cpp:404] upsample_disparity2x_itr1 -> upsampled_disparity2x_itr1
W0814 08:07:37.405048 28599 resample_layer.cpp:28] ResampleLayer only runs Reshape on setup
I0814 08:07:37.405064 28599 net.cpp:144] Setting up upsample_disparity2x_itr1
I0814 08:07:37.405068 28599 net.cpp:151] Top shape: 1 1 192 384 (73728)
I0814 08:07:37.405071 28599 net.cpp:159] Memory required for data: 113892336
I0814 08:07:37.405073 28599 layer_factory.hpp:77] Creating layer concat_input_itr1
I0814 08:07:37.405077 28599 net.cpp:91] Creating Layer concat_input_itr1
I0814 08:07:37.405081 28599 net.cpp:432] concat_input_itr1 <- img0_aug_img0s_aug_0_split_1
I0814 08:07:37.405084 28599 net.cpp:432] concat_input_itr1 <- upsampled_disparity_itr1_upsample_disparity_itr1_0_split_0
I0814 08:07:37.405087 28599 net.cpp:404] concat_input_itr1 -> zoom_input_itr1
I0814 08:07:37.405103 28599 net.cpp:144] Setting up concat_input_itr1
I0814 08:07:37.405107 28599 net.cpp:151] Top shape: 1 4 384 768 (1179648)
I0814 08:07:37.405109 28599 net.cpp:159] Memory required for data: 118610928
I0814 08:07:37.405112 28599 layer_factory.hpp:77] Creating layer zoom_conv0_itr1
I0814 08:07:37.405120 28599 net.cpp:91] Creating Layer zoom_conv0_itr1
I0814 08:07:37.405122 28599 net.cpp:432] zoom_conv0_itr1 <- zoom_input_itr1
I0814 08:07:37.405128 28599 net.cpp:404] zoom_conv0_itr1 -> zoom_conv0_itr1
I0814 08:07:37.407521 28599 net.cpp:144] Setting up zoom_conv0_itr1
I0814 08:07:37.407537 28599 net.cpp:151] Top shape: 1 32 384 768 (9437184)
I0814 08:07:37.407541 28599 net.cpp:159] Memory required for data: 156359664
I0814 08:07:37.407547 28599 layer_factory.hpp:77] Creating layer zoom_ReLU0_itr1
I0814 08:07:37.407554 28599 net.cpp:91] Creating Layer zoom_ReLU0_itr1
I0814 08:07:37.407557 28599 net.cpp:432] zoom_ReLU0_itr1 <- zoom_conv0_itr1
I0814 08:07:37.407562 28599 net.cpp:391] zoom_ReLU0_itr1 -> zoom_conv0_itr1 (in-place)
I0814 08:07:37.407707 28599 net.cpp:144] Setting up zoom_ReLU0_itr1
I0814 08:07:37.407714 28599 net.cpp:151] Top shape: 1 32 384 768 (9437184)
I0814 08:07:37.407716 28599 net.cpp:159] Memory required for data: 194108400
I0814 08:07:37.407719 28599 layer_factory.hpp:77] Creating layer zoom_conv0_itr1_zoom_ReLU0_itr1_0_split
I0814 08:07:37.407724 28599 net.cpp:91] Creating Layer zoom_conv0_itr1_zoom_ReLU0_itr1_0_split
I0814 08:07:37.407737 28599 net.cpp:432] zoom_conv0_itr1_zoom_ReLU0_itr1_0_split <- zoom_conv0_itr1
I0814 08:07:37.407742 28599 net.cpp:404] zoom_conv0_itr1_zoom_ReLU0_itr1_0_split -> zoom_conv0_itr1_zoom_ReLU0_itr1_0_split_0
I0814 08:07:37.407748 28599 net.cpp:404] zoom_conv0_itr1_zoom_ReLU0_itr1_0_split -> zoom_conv0_itr1_zoom_ReLU0_itr1_0_split_1
I0814 08:07:37.407781 28599 net.cpp:144] Setting up zoom_conv0_itr1_zoom_ReLU0_itr1_0_split
I0814 08:07:37.407786 28599 net.cpp:151] Top shape: 1 32 384 768 (9437184)
I0814 08:07:37.407789 28599 net.cpp:151] Top shape: 1 32 384 768 (9437184)
I0814 08:07:37.407791 28599 net.cpp:159] Memory required for data: 269605872
I0814 08:07:37.407793 28599 layer_factory.hpp:77] Creating layer zoom_conv1_itr1
I0814 08:07:37.407801 28599 net.cpp:91] Creating Layer zoom_conv1_itr1
I0814 08:07:37.407804 28599 net.cpp:432] zoom_conv1_itr1 <- zoom_conv0_itr1_zoom_ReLU0_itr1_0_split_0
I0814 08:07:37.407809 28599 net.cpp:404] zoom_conv1_itr1 -> zoom_conv1_itr1
I0814 08:07:37.410030 28599 net.cpp:144] Setting up zoom_conv1_itr1
I0814 08:07:37.410042 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.410044 28599 net.cpp:159] Memory required for data: 288480240
I0814 08:07:37.410053 28599 layer_factory.hpp:77] Creating layer zoom_ReLU1_itr1
I0814 08:07:37.410058 28599 net.cpp:91] Creating Layer zoom_ReLU1_itr1
I0814 08:07:37.410061 28599 net.cpp:432] zoom_ReLU1_itr1 <- zoom_conv1_itr1
I0814 08:07:37.410065 28599 net.cpp:391] zoom_ReLU1_itr1 -> zoom_conv1_itr1 (in-place)
I0814 08:07:37.410606 28599 net.cpp:144] Setting up zoom_ReLU1_itr1
I0814 08:07:37.410625 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.410629 28599 net.cpp:159] Memory required for data: 307354608
I0814 08:07:37.410631 28599 layer_factory.hpp:77] Creating layer zoom_conv1b_itr1
I0814 08:07:37.410640 28599 net.cpp:91] Creating Layer zoom_conv1b_itr1
I0814 08:07:37.410643 28599 net.cpp:432] zoom_conv1b_itr1 <- zoom_conv1_itr1
I0814 08:07:37.410650 28599 net.cpp:404] zoom_conv1b_itr1 -> zoom_conv1b_itr1
I0814 08:07:37.413209 28599 net.cpp:144] Setting up zoom_conv1b_itr1
I0814 08:07:37.413223 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.413225 28599 net.cpp:159] Memory required for data: 326228976
I0814 08:07:37.413231 28599 layer_factory.hpp:77] Creating layer zoom_ReLU1b_itr1
I0814 08:07:37.413238 28599 net.cpp:91] Creating Layer zoom_ReLU1b_itr1
I0814 08:07:37.413241 28599 net.cpp:432] zoom_ReLU1b_itr1 <- zoom_conv1b_itr1
I0814 08:07:37.413245 28599 net.cpp:391] zoom_ReLU1b_itr1 -> zoom_conv1b_itr1 (in-place)
I0814 08:07:37.413384 28599 net.cpp:144] Setting up zoom_ReLU1b_itr1
I0814 08:07:37.413390 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.413393 28599 net.cpp:159] Memory required for data: 345103344
I0814 08:07:37.413396 28599 layer_factory.hpp:77] Creating layer zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split
I0814 08:07:37.413401 28599 net.cpp:91] Creating Layer zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split
I0814 08:07:37.413404 28599 net.cpp:432] zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split <- zoom_conv1b_itr1
I0814 08:07:37.413408 28599 net.cpp:404] zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split -> zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split_0
I0814 08:07:37.413414 28599 net.cpp:404] zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split -> zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split_1
I0814 08:07:37.413450 28599 net.cpp:144] Setting up zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split
I0814 08:07:37.413455 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.413457 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.413460 28599 net.cpp:159] Memory required for data: 382852080
I0814 08:07:37.413462 28599 layer_factory.hpp:77] Creating layer zoom_conv2_itr1
I0814 08:07:37.413470 28599 net.cpp:91] Creating Layer zoom_conv2_itr1
I0814 08:07:37.413473 28599 net.cpp:432] zoom_conv2_itr1 <- zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split_0
I0814 08:07:37.413478 28599 net.cpp:404] zoom_conv2_itr1 -> zoom_conv2_itr1
I0814 08:07:37.415771 28599 net.cpp:144] Setting up zoom_conv2_itr1
I0814 08:07:37.415782 28599 net.cpp:151] Top shape: 1 64 96 192 (1179648)
I0814 08:07:37.415786 28599 net.cpp:159] Memory required for data: 387570672
I0814 08:07:37.415792 28599 layer_factory.hpp:77] Creating layer zoom_ReLU2_itr1
I0814 08:07:37.415797 28599 net.cpp:91] Creating Layer zoom_ReLU2_itr1
I0814 08:07:37.415802 28599 net.cpp:432] zoom_ReLU2_itr1 <- zoom_conv2_itr1
I0814 08:07:37.415805 28599 net.cpp:391] zoom_ReLU2_itr1 -> zoom_conv2_itr1 (in-place)
I0814 08:07:37.415940 28599 net.cpp:144] Setting up zoom_ReLU2_itr1
I0814 08:07:37.415946 28599 net.cpp:151] Top shape: 1 64 96 192 (1179648)
I0814 08:07:37.415949 28599 net.cpp:159] Memory required for data: 392289264
I0814 08:07:37.415952 28599 layer_factory.hpp:77] Creating layer zoom_conv2b_itr1
I0814 08:07:37.415961 28599 net.cpp:91] Creating Layer zoom_conv2b_itr1
I0814 08:07:37.415963 28599 net.cpp:432] zoom_conv2b_itr1 <- zoom_conv2_itr1
I0814 08:07:37.415968 28599 net.cpp:404] zoom_conv2b_itr1 -> zoom_conv2b_itr1
I0814 08:07:37.418089 28599 net.cpp:144] Setting up zoom_conv2b_itr1
I0814 08:07:37.418099 28599 net.cpp:151] Top shape: 1 64 96 192 (1179648)
I0814 08:07:37.418102 28599 net.cpp:159] Memory required for data: 397007856
I0814 08:07:37.418107 28599 layer_factory.hpp:77] Creating layer zoom_ReLU2b_itr1
I0814 08:07:37.418112 28599 net.cpp:91] Creating Layer zoom_ReLU2b_itr1
I0814 08:07:37.418115 28599 net.cpp:432] zoom_ReLU2b_itr1 <- zoom_conv2b_itr1
I0814 08:07:37.418119 28599 net.cpp:391] zoom_ReLU2b_itr1 -> zoom_conv2b_itr1 (in-place)
I0814 08:07:37.418602 28599 net.cpp:144] Setting up zoom_ReLU2b_itr1
I0814 08:07:37.418611 28599 net.cpp:151] Top shape: 1 64 96 192 (1179648)
I0814 08:07:37.418613 28599 net.cpp:159] Memory required for data: 401726448
I0814 08:07:37.418617 28599 layer_factory.hpp:77] Creating layer zoom_deconv2_itr1
I0814 08:07:37.418622 28599 net.cpp:91] Creating Layer zoom_deconv2_itr1
I0814 08:07:37.418625 28599 net.cpp:432] zoom_deconv2_itr1 <- zoom_conv2b_itr1
I0814 08:07:37.418630 28599 net.cpp:404] zoom_deconv2_itr1 -> zoom_deconv2_itr1
I0814 08:07:37.420037 28599 net.cpp:144] Setting up zoom_deconv2_itr1
I0814 08:07:37.420049 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.420053 28599 net.cpp:159] Memory required for data: 420600816
I0814 08:07:37.420063 28599 layer_factory.hpp:77] Creating layer zoom_ReLU0p_itr1
I0814 08:07:37.420069 28599 net.cpp:91] Creating Layer zoom_ReLU0p_itr1
I0814 08:07:37.420073 28599 net.cpp:432] zoom_ReLU0p_itr1 <- zoom_deconv2_itr1
I0814 08:07:37.420078 28599 net.cpp:391] zoom_ReLU0p_itr1 -> zoom_deconv2_itr1 (in-place)
I0814 08:07:37.420248 28599 net.cpp:144] Setting up zoom_ReLU0p_itr1
I0814 08:07:37.420264 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.420267 28599 net.cpp:159] Memory required for data: 439475184
I0814 08:07:37.420270 28599 layer_factory.hpp:77] Creating layer zoom_Concat2_itr1
I0814 08:07:37.420275 28599 net.cpp:91] Creating Layer zoom_Concat2_itr1
I0814 08:07:37.420279 28599 net.cpp:432] zoom_Concat2_itr1 <- zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split_1
I0814 08:07:37.420282 28599 net.cpp:432] zoom_Concat2_itr1 <- zoom_deconv2_itr1
I0814 08:07:37.420286 28599 net.cpp:432] zoom_Concat2_itr1 <- upsampled_disparity2x_itr1
I0814 08:07:37.420290 28599 net.cpp:404] zoom_Concat2_itr1 -> zoom_concat2_itr1
I0814 08:07:37.420315 28599 net.cpp:144] Setting up zoom_Concat2_itr1
I0814 08:07:37.420320 28599 net.cpp:151] Top shape: 1 129 192 384 (9510912)
I0814 08:07:37.420322 28599 net.cpp:159] Memory required for data: 477518832
I0814 08:07:37.420325 28599 layer_factory.hpp:77] Creating layer zoom_fused1_itr1
I0814 08:07:37.420332 28599 net.cpp:91] Creating Layer zoom_fused1_itr1
I0814 08:07:37.420336 28599 net.cpp:432] zoom_fused1_itr1 <- zoom_concat2_itr1
I0814 08:07:37.420339 28599 net.cpp:404] zoom_fused1_itr1 -> zoom_fused1_itr1
I0814 08:07:37.423529 28599 net.cpp:144] Setting up zoom_fused1_itr1
I0814 08:07:37.423548 28599 net.cpp:151] Top shape: 1 64 192 384 (4718592)
I0814 08:07:37.423566 28599 net.cpp:159] Memory required for data: 496393200
I0814 08:07:37.423575 28599 layer_factory.hpp:77] Creating layer zoom_deconv1_itr1
I0814 08:07:37.423584 28599 net.cpp:91] Creating Layer zoom_deconv1_itr1
I0814 08:07:37.423588 28599 net.cpp:432] zoom_deconv1_itr1 <- zoom_fused1_itr1
I0814 08:07:37.423594 28599 net.cpp:404] zoom_deconv1_itr1 -> zoom_deconv1_itr1
I0814 08:07:37.424793 28599 net.cpp:144] Setting up zoom_deconv1_itr1
I0814 08:07:37.424818 28599 net.cpp:151] Top shape: 1 32 384 768 (9437184)
I0814 08:07:37.424821 28599 net.cpp:159] Memory required for data: 534141936
I0814 08:07:37.424827 28599 layer_factory.hpp:77] Creating layer zoom_ReLU1p_itr1
I0814 08:07:37.424835 28599 net.cpp:91] Creating Layer zoom_ReLU1p_itr1
I0814 08:07:37.424839 28599 net.cpp:432] zoom_ReLU1p_itr1 <- zoom_deconv1_itr1
I0814 08:07:37.424844 28599 net.cpp:391] zoom_ReLU1p_itr1 -> zoom_deconv1_itr1 (in-place)
I0814 08:07:37.425453 28599 net.cpp:144] Setting up zoom_ReLU1p_itr1
I0814 08:07:37.425464 28599 net.cpp:151] Top shape: 1 32 384 768 (9437184)
I0814 08:07:37.425467 28599 net.cpp:159] Memory required for data: 571890672
I0814 08:07:37.425472 28599 layer_factory.hpp:77] Creating layer zoom_Concat1_itr1
I0814 08:07:37.425477 28599 net.cpp:91] Creating Layer zoom_Concat1_itr1
I0814 08:07:37.425482 28599 net.cpp:432] zoom_Concat1_itr1 <- zoom_conv0_itr1_zoom_ReLU0_itr1_0_split_1
I0814 08:07:37.425487 28599 net.cpp:432] zoom_Concat1_itr1 <- zoom_deconv1_itr1
I0814 08:07:37.425490 28599 net.cpp:432] zoom_Concat1_itr1 <- upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split_0
I0814 08:07:37.425495 28599 net.cpp:404] zoom_Concat1_itr1 -> zoom_concat1_itr1
I0814 08:07:37.425534 28599 net.cpp:144] Setting up zoom_Concat1_itr1
I0814 08:07:37.425539 28599 net.cpp:151] Top shape: 1 65 384 768 (19169280)
I0814 08:07:37.425540 28599 net.cpp:159] Memory required for data: 648567792
I0814 08:07:37.425544 28599 layer_factory.hpp:77] Creating layer zoom_fused0_itr1
I0814 08:07:37.425552 28599 net.cpp:91] Creating Layer zoom_fused0_itr1
I0814 08:07:37.425555 28599 net.cpp:432] zoom_fused0_itr1 <- zoom_concat1_itr1
I0814 08:07:37.425560 28599 net.cpp:404] zoom_fused0_itr1 -> zoom_fused0_itr1
I0814 08:07:37.427413 28599 net.cpp:144] Setting up zoom_fused0_itr1
I0814 08:07:37.427436 28599 net.cpp:151] Top shape: 1 32 384 768 (9437184)
I0814 08:07:37.427439 28599 net.cpp:159] Memory required for data: 686316528
I0814 08:07:37.427448 28599 layer_factory.hpp:77] Creating layer zoom_Convolution0_itr1
I0814 08:07:37.427462 28599 net.cpp:91] Creating Layer zoom_Convolution0_itr1
I0814 08:07:37.427469 28599 net.cpp:432] zoom_Convolution0_itr1 <- zoom_fused0_itr1
I0814 08:07:37.427474 28599 net.cpp:404] zoom_Convolution0_itr1 -> zoom_predict0_itr1
I0814 08:07:37.430027 28599 net.cpp:144] Setting up zoom_Convolution0_itr1
I0814 08:07:37.430050 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.430053 28599 net.cpp:159] Memory required for data: 687496176
I0814 08:07:37.430063 28599 layer_factory.hpp:77] Creating layer zoom_ReLU_predict_itr1
I0814 08:07:37.430071 28599 net.cpp:91] Creating Layer zoom_ReLU_predict_itr1
I0814 08:07:37.430075 28599 net.cpp:432] zoom_ReLU_predict_itr1 <- zoom_predict0_itr1
I0814 08:07:37.430080 28599 net.cpp:404] zoom_ReLU_predict_itr1 -> zoom_predict0n_itr1
I0814 08:07:37.430997 28599 net.cpp:144] Setting up zoom_ReLU_predict_itr1
I0814 08:07:37.431008 28599 net.cpp:151] Top shape: 1 1 384 768 (294912)
I0814 08:07:37.431011 28599 net.cpp:159] Memory required for data: 688675824
I0814 08:07:37.431023 28599 layer_factory.hpp:77] Creating layer zoom_disp_loss0
I0814 08:07:37.431030 28599 net.cpp:91] Creating Layer zoom_disp_loss0
I0814 08:07:37.431033 28599 net.cpp:432] zoom_disp_loss0 <- zoom_predict0n_itr1
I0814 08:07:37.431047 28599 net.cpp:432] zoom_disp_loss0 <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_2
I0814 08:07:37.431051 28599 net.cpp:404] zoom_disp_loss0 -> zoom_disp_loss0
I0814 08:07:37.431126 28599 net.cpp:144] Setting up zoom_disp_loss0
I0814 08:07:37.431131 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.431150 28599 net.cpp:154]     with loss weight 1
I0814 08:07:37.431159 28599 net.cpp:159] Memory required for data: 688675828
I0814 08:07:37.431161 28599 layer_factory.hpp:77] Creating layer zoom_disp_loss0_zoom_disp_loss0_0_split
I0814 08:07:37.431166 28599 net.cpp:91] Creating Layer zoom_disp_loss0_zoom_disp_loss0_0_split
I0814 08:07:37.431169 28599 net.cpp:432] zoom_disp_loss0_zoom_disp_loss0_0_split <- zoom_disp_loss0
I0814 08:07:37.431174 28599 net.cpp:404] zoom_disp_loss0_zoom_disp_loss0_0_split -> zoom_disp_loss0_zoom_disp_loss0_0_split_0
I0814 08:07:37.431179 28599 net.cpp:404] zoom_disp_loss0_zoom_disp_loss0_0_split -> zoom_disp_loss0_zoom_disp_loss0_0_split_1
I0814 08:07:37.431267 28599 net.cpp:144] Setting up zoom_disp_loss0_zoom_disp_loss0_0_split
I0814 08:07:37.431273 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.431285 28599 net.cpp:154]     with loss weight 1
I0814 08:07:37.431289 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.431306 28599 net.cpp:159] Memory required for data: 688675836
I0814 08:07:37.431309 28599 layer_factory.hpp:77] Creating layer down_up_loss
I0814 08:07:37.431314 28599 net.cpp:91] Creating Layer down_up_loss
I0814 08:07:37.431326 28599 net.cpp:432] down_up_loss <- upsampled_disparity_itr1_upsample_disparity_itr1_0_split_1
I0814 08:07:37.431330 28599 net.cpp:432] down_up_loss <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_3
I0814 08:07:37.431334 28599 net.cpp:404] down_up_loss -> down_up_loss
I0814 08:07:37.431388 28599 net.cpp:144] Setting up down_up_loss
I0814 08:07:37.431393 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.431396 28599 net.cpp:159] Memory required for data: 688675840
I0814 08:07:37.431397 28599 layer_factory.hpp:77] Creating layer down_up_half_loss
I0814 08:07:37.431402 28599 net.cpp:91] Creating Layer down_up_half_loss
I0814 08:07:37.431406 28599 net.cpp:432] down_up_half_loss <- upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split_1
I0814 08:07:37.431409 28599 net.cpp:432] down_up_half_loss <- disp_gt_aug_norm_disp_gt_aug_norm_0_split_4
I0814 08:07:37.431412 28599 net.cpp:404] down_up_half_loss -> down_up_half_loss
I0814 08:07:37.431459 28599 net.cpp:144] Setting up down_up_half_loss
I0814 08:07:37.431463 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.431466 28599 net.cpp:159] Memory required for data: 688675844
I0814 08:07:37.431468 28599 layer_factory.hpp:77] Creating layer to_real_disp_loss
I0814 08:07:37.431473 28599 net.cpp:91] Creating Layer to_real_disp_loss
I0814 08:07:37.431475 28599 net.cpp:432] to_real_disp_loss <- zoom_disp_loss0_zoom_disp_loss0_0_split_1
I0814 08:07:37.431479 28599 net.cpp:404] to_real_disp_loss -> zoom_disp_loss0_real
I0814 08:07:37.431484 28599 net.cpp:144] Setting up to_real_disp_loss
I0814 08:07:37.431488 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.431489 28599 net.cpp:159] Memory required for data: 688675848
I0814 08:07:37.431493 28599 layer_factory.hpp:77] Creating layer to_real_downup_loss
I0814 08:07:37.431500 28599 net.cpp:91] Creating Layer to_real_downup_loss
I0814 08:07:37.431504 28599 net.cpp:432] to_real_downup_loss <- down_up_loss
I0814 08:07:37.431507 28599 net.cpp:404] to_real_downup_loss -> down_up_loss_real
I0814 08:07:37.431512 28599 net.cpp:144] Setting up to_real_downup_loss
I0814 08:07:37.431515 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.431517 28599 net.cpp:159] Memory required for data: 688675852
I0814 08:07:37.431520 28599 layer_factory.hpp:77] Creating layer to_real_downup_loss
I0814 08:07:37.431524 28599 net.cpp:91] Creating Layer to_real_downup_loss
I0814 08:07:37.431526 28599 net.cpp:432] to_real_downup_loss <- down_up_half_loss
I0814 08:07:37.431530 28599 net.cpp:404] to_real_downup_loss -> down_up_half_loss_real
I0814 08:07:37.431535 28599 net.cpp:144] Setting up to_real_downup_loss
I0814 08:07:37.431537 28599 net.cpp:151] Top shape: (1)
I0814 08:07:37.431540 28599 net.cpp:159] Memory required for data: 688675856
I0814 08:07:37.431542 28599 net.cpp:224] to_real_downup_loss does not need backward computation.
I0814 08:07:37.431545 28599 net.cpp:224] to_real_downup_loss does not need backward computation.
I0814 08:07:37.431553 28599 net.cpp:224] to_real_disp_loss does not need backward computation.
I0814 08:07:37.431556 28599 net.cpp:224] down_up_half_loss does not need backward computation.
I0814 08:07:37.431558 28599 net.cpp:224] down_up_loss does not need backward computation.
I0814 08:07:37.431561 28599 net.cpp:222] zoom_disp_loss0_zoom_disp_loss0_0_split needs backward computation.
I0814 08:07:37.431565 28599 net.cpp:222] zoom_disp_loss0 needs backward computation.
I0814 08:07:37.431567 28599 net.cpp:222] zoom_ReLU_predict_itr1 needs backward computation.
I0814 08:07:37.431571 28599 net.cpp:222] zoom_Convolution0_itr1 needs backward computation.
I0814 08:07:37.431573 28599 net.cpp:222] zoom_fused0_itr1 needs backward computation.
I0814 08:07:37.431576 28599 net.cpp:222] zoom_Concat1_itr1 needs backward computation.
I0814 08:07:37.431579 28599 net.cpp:222] zoom_ReLU1p_itr1 needs backward computation.
I0814 08:07:37.431581 28599 net.cpp:222] zoom_deconv1_itr1 needs backward computation.
I0814 08:07:37.431584 28599 net.cpp:222] zoom_fused1_itr1 needs backward computation.
I0814 08:07:37.431587 28599 net.cpp:222] zoom_Concat2_itr1 needs backward computation.
I0814 08:07:37.431591 28599 net.cpp:222] zoom_ReLU0p_itr1 needs backward computation.
I0814 08:07:37.431593 28599 net.cpp:222] zoom_deconv2_itr1 needs backward computation.
I0814 08:07:37.431596 28599 net.cpp:222] zoom_ReLU2b_itr1 needs backward computation.
I0814 08:07:37.431599 28599 net.cpp:222] zoom_conv2b_itr1 needs backward computation.
I0814 08:07:37.431602 28599 net.cpp:222] zoom_ReLU2_itr1 needs backward computation.
I0814 08:07:37.431604 28599 net.cpp:222] zoom_conv2_itr1 needs backward computation.
I0814 08:07:37.431607 28599 net.cpp:222] zoom_conv1b_itr1_zoom_ReLU1b_itr1_0_split needs backward computation.
I0814 08:07:37.431610 28599 net.cpp:222] zoom_ReLU1b_itr1 needs backward computation.
I0814 08:07:37.431613 28599 net.cpp:222] zoom_conv1b_itr1 needs backward computation.
I0814 08:07:37.431617 28599 net.cpp:222] zoom_ReLU1_itr1 needs backward computation.
I0814 08:07:37.431619 28599 net.cpp:222] zoom_conv1_itr1 needs backward computation.
I0814 08:07:37.431622 28599 net.cpp:222] zoom_conv0_itr1_zoom_ReLU0_itr1_0_split needs backward computation.
I0814 08:07:37.431624 28599 net.cpp:222] zoom_ReLU0_itr1 needs backward computation.
I0814 08:07:37.431627 28599 net.cpp:222] zoom_conv0_itr1 needs backward computation.
I0814 08:07:37.431634 28599 net.cpp:222] concat_input_itr1 needs backward computation.
I0814 08:07:37.431638 28599 net.cpp:222] upsample_disparity2x_itr1 needs backward computation.
I0814 08:07:37.431641 28599 net.cpp:222] upsampled_disparity_itr1_upsample_disparity_itr1_0_split needs backward computation.
I0814 08:07:37.431643 28599 net.cpp:222] upsample_disparity_itr1 needs backward computation.
I0814 08:07:37.431648 28599 net.cpp:224] Silence6 does not need backward computation.
I0814 08:07:37.431649 28599 net.cpp:224] Silence5 does not need backward computation.
I0814 08:07:37.431653 28599 net.cpp:224] Silence4 does not need backward computation.
I0814 08:07:37.431654 28599 net.cpp:224] Silence3 does not need backward computation.
I0814 08:07:37.431658 28599 net.cpp:224] Silence2 does not need backward computation.
I0814 08:07:37.431659 28599 net.cpp:224] Silence1 does not need backward computation.
I0814 08:07:37.431663 28599 net.cpp:222] upsampled_disparity_itr1_half_upsample_disparity_itr1_half_0_split needs backward computation.
I0814 08:07:37.431665 28599 net.cpp:222] upsample_disparity_itr1_half needs backward computation.
I0814 08:07:37.431673 28599 net.cpp:224] disp_gt_aug_norm_down_half does not need backward computation.
I0814 08:07:37.431676 28599 net.cpp:224] disp_gt_aug_norm_down_disp_gt_aug_norm_down_0_split does not need backward computation.
I0814 08:07:37.431679 28599 net.cpp:224] disp_gt_aug_norm_down does not need backward computation.
I0814 08:07:37.431684 28599 net.cpp:222] disp_gt_aug_norm_disp_gt_aug_norm_0_split needs backward computation.
I0814 08:07:37.431700 28599 net.cpp:222] disp_gt_aug_norm needs backward computation.
I0814 08:07:37.431704 28599 net.cpp:224] ref_disp_scale_half does not need backward computation.
I0814 08:07:37.431706 28599 net.cpp:224] ref_disp_scale does not need backward computation.
I0814 08:07:37.431717 28599 net.cpp:222] Slice1 needs backward computation.
I0814 08:07:37.431721 28599 net.cpp:224] FlowAugmentation1 does not need backward computation.
I0814 08:07:37.431740 28599 net.cpp:224] Concat1 does not need backward computation.
I0814 08:07:37.431746 28599 net.cpp:224] blob9_DummyData2_0_split does not need backward computation.
I0814 08:07:37.431748 28599 net.cpp:224] DummyData2 does not need backward computation.
I0814 08:07:37.431761 28599 net.cpp:224] img1s_aug does not need backward computation.
I0814 08:07:37.431764 28599 net.cpp:224] blob7_aug_params1_0_split does not need backward computation.
I0814 08:07:37.431767 28599 net.cpp:224] aug_params1 does not need backward computation.
I0814 08:07:37.431782 28599 net.cpp:224] blob6_img0s_aug_1_split does not need backward computation.
I0814 08:07:37.431785 28599 net.cpp:222] img0_aug_img0s_aug_0_split needs backward computation.
I0814 08:07:37.431788 28599 net.cpp:222] img0s_aug needs backward computation.
I0814 08:07:37.431792 28599 net.cpp:224] Eltwise2 does not need backward computation.
I0814 08:07:37.431795 28599 net.cpp:224] blob3_Eltwise1_0_split does not need backward computation.
I0814 08:07:37.431798 28599 net.cpp:224] Eltwise1 does not need backward computation.
I0814 08:07:37.431802 28599 net.cpp:224] blob2_CustomData2_2_split does not need backward computation.
I0814 08:07:37.431805 28599 net.cpp:224] blob1_CustomData2_1_split does not need backward computation.
I0814 08:07:37.431809 28599 net.cpp:224] blob0_CustomData2_0_split does not need backward computation.
I0814 08:07:37.431813 28599 net.cpp:224] CustomData2 does not need backward computation.
I0814 08:07:37.431815 28599 net.cpp:266] This network produces output down_up_half_loss_real
I0814 08:07:37.431818 28599 net.cpp:266] This network produces output down_up_loss_real
I0814 08:07:37.431821 28599 net.cpp:266] This network produces output zoom_disp_loss0_real
I0814 08:07:37.431833 28599 net.cpp:266] This network produces output zoom_disp_loss0_zoom_disp_loss0_0_split_0
I0814 08:07:37.431870 28599 net.cpp:279] Network initialization done.
I0814 08:07:37.432044 28599 solver.cpp:60] Solver scaffolding done.
I0814 08:07:37.433051 28599 caffe.cpp:129] Finetuning from ../model_only_zoom_in/GT_pos_no_norm_itr1.caffemodel
I0814 08:07:37.528873 28599 data_augmentation_layer.cpp:165] Data augmentation layer: adjusting mean blobs
I0814 08:07:37.529065 28599 data_augmentation_layer.cpp:173] recovered iteration count 2.69794e+06
I0814 08:07:37.529192 28599 data_augmentation_layer.cpp:182] recovered mean value 0.35372
I0814 08:07:37.529199 28599 data_augmentation_layer.cpp:182] recovered mean value 0.384273
I0814 08:07:37.529202 28599 data_augmentation_layer.cpp:182] recovered mean value 0.405834
I0814 08:07:37.531113 28599 data_augmentation_layer.cpp:165] Data augmentation layer: adjusting mean blobs
I0814 08:07:37.531199 28599 data_augmentation_layer.cpp:173] recovered iteration count 2.69794e+06
I0814 08:07:37.531303 28599 data_augmentation_layer.cpp:182] recovered mean value 0.353581
I0814 08:07:37.531311 28599 data_augmentation_layer.cpp:182] recovered mean value 0.384512
I0814 08:07:37.531313 28599 data_augmentation_layer.cpp:182] recovered mean value 0.406228
I0814 08:07:37.531960 28599 net.cpp:762] Ignoring source layer zoom_conv2b_itr1_zoom_ReLU2b_itr1_0_split
I0814 08:07:37.531967 28599 net.cpp:762] Ignoring source layer zoom_Convolution2_itr1
I0814 08:07:37.532012 28599 net.cpp:762] Ignoring source layer zoom_upsample_2to1_itr1
I0814 08:07:37.532063 28599 net.cpp:762] Ignoring source layer zoom_fused1_itr1_zoom_fused1_itr1_0_split
I0814 08:07:37.532066 28599 net.cpp:762] Ignoring source layer zoom_Convolution1_itr1
I0814 08:07:37.532114 28599 net.cpp:762] Ignoring source layer zoom_upsample_1to0_itr1
I0814 08:07:37.541528 28599 net.cpp:762] Ignoring source layer CustomData1
I0814 08:07:37.541563 28599 net.cpp:762] Ignoring source layer blob0_CustomData1_0_split
I0814 08:07:37.541565 28599 net.cpp:762] Ignoring source layer blob1_CustomData1_1_split
I0814 08:07:37.541568 28599 net.cpp:762] Ignoring source layer blob2_CustomData1_2_split
I0814 08:07:37.543043 28599 data_augmentation_layer.cpp:165] Data augmentation layer: adjusting mean blobs
I0814 08:07:37.543205 28599 data_augmentation_layer.cpp:173] recovered iteration count 2.69794e+06
I0814 08:07:37.543318 28599 data_augmentation_layer.cpp:182] recovered mean value 0.35372
I0814 08:07:37.543324 28599 data_augmentation_layer.cpp:182] recovered mean value 0.384273
I0814 08:07:37.543328 28599 data_augmentation_layer.cpp:182] recovered mean value 0.405834
I0814 08:07:37.545217 28599 data_augmentation_layer.cpp:165] Data augmentation layer: adjusting mean blobs
I0814 08:07:37.545320 28599 data_augmentation_layer.cpp:173] recovered iteration count 2.69794e+06
I0814 08:07:37.545428 28599 data_augmentation_layer.cpp:182] recovered mean value 0.353581
I0814 08:07:37.545434 28599 data_augmentation_layer.cpp:182] recovered mean value 0.384512
I0814 08:07:37.545437 28599 data_augmentation_layer.cpp:182] recovered mean value 0.406228
I0814 08:07:37.545979 28599 net.cpp:762] Ignoring source layer DummyData1
I0814 08:07:37.545986 28599 net.cpp:762] Ignoring source layer blob9_DummyData1_0_split
I0814 08:07:37.546102 28599 net.cpp:762] Ignoring source layer zoom_conv2b_itr1_zoom_ReLU2b_itr1_0_split
I0814 08:07:37.546104 28599 net.cpp:762] Ignoring source layer zoom_Convolution2_itr1
I0814 08:07:37.546170 28599 net.cpp:762] Ignoring source layer zoom_upsample_2to1_itr1
I0814 08:07:37.546250 28599 net.cpp:762] Ignoring source layer zoom_fused1_itr1_zoom_fused1_itr1_0_split
I0814 08:07:37.546254 28599 net.cpp:762] Ignoring source layer zoom_Convolution1_itr1
I0814 08:07:37.546283 28599 net.cpp:762] Ignoring source layer zoom_upsample_1to0_itr1
I0814 08:07:37.546396 28599 caffe.cpp:219] Starting Optimization
I0814 08:07:37.546401 28599 solver.cpp:280] Solving 
I0814 08:07:37.546404 28599 solver.cpp:281] Learning Rate Policy: multistep
I0814 08:07:37.548883 28599 solver.cpp:338] Iteration 0, Testing net (#0)
I0814 08:07:37.548893 28599 net.cpp:694] Ignoring source layer CustomData1
I0814 08:07:37.548897 28599 net.cpp:694] Ignoring source layer blob0_CustomData1_0_split
I0814 08:07:37.548899 28599 net.cpp:694] Ignoring source layer blob1_CustomData1_1_split
I0814 08:07:37.548902 28599 net.cpp:694] Ignoring source layer blob2_CustomData1_2_split
I0814 08:07:37.548979 28599 net.cpp:694] Ignoring source layer DummyData1
I0814 08:07:37.548982 28599 net.cpp:694] Ignoring source layer blob9_DummyData1_0_split
I0814 08:17:18.091059 28599 solver.cpp:406]     Test net output #0: down_up_half_loss_real = 0.0614319
I0814 08:17:18.102987 28599 solver.cpp:406]     Test net output #1: down_up_loss_real = 0.118198
I0814 08:17:18.103003 28599 solver.cpp:406]     Test net output #2: zoom_disp_loss0_real = 4.42161
I0814 08:17:18.103010 28599 solver.cpp:406]     Test net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 4.42161 (* 1 = 4.42161 loss)
I0814 08:17:18.367002 28599 solver.cpp:229] Iteration 0, loss = 11.7973
I0814 08:17:18.523610 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.048261
I0814 08:17:18.523769 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0938261
I0814 08:17:18.523818 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 5.89865
I0814 08:17:18.523836 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 5.89865 (* 1 = 5.89865 loss)
I0814 08:17:18.523859 28599 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I0814 08:17:51.162621 28599 solver.cpp:229] Iteration 50, loss = 10.8175
I0814 08:17:51.322589 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0885786
I0814 08:17:51.322646 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.167111
I0814 08:17:51.322662 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 5.40877
I0814 08:17:51.322671 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 5.40877 (* 1 = 5.40877 loss)
I0814 08:17:51.322679 28599 sgd_solver.cpp:106] Iteration 50, lr = 0.0001
I0814 08:18:24.818858 28599 solver.cpp:229] Iteration 100, loss = 5.2334
I0814 08:18:24.987365 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.124366
I0814 08:18:24.987426 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.239915
I0814 08:18:24.987447 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 2.6167
I0814 08:18:24.987462 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 2.6167 (* 1 = 2.6167 loss)
I0814 08:18:24.987473 28599 sgd_solver.cpp:106] Iteration 100, lr = 0.0001
I0814 08:18:58.443061 28599 solver.cpp:229] Iteration 150, loss = 0.872764
I0814 08:18:58.606431 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.102195
I0814 08:18:58.606483 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.186033
I0814 08:18:58.606499 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.436382
I0814 08:18:58.606509 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.436382 (* 1 = 0.436382 loss)
I0814 08:18:58.606520 28599 sgd_solver.cpp:106] Iteration 150, lr = 0.0001
I0814 08:19:31.951596 28599 solver.cpp:229] Iteration 200, loss = 0.264897
I0814 08:19:32.108835 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0431602
I0814 08:19:32.108896 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0825908
I0814 08:19:32.108911 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.132449
I0814 08:19:32.108922 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.132449 (* 1 = 0.132449 loss)
I0814 08:19:32.108930 28599 sgd_solver.cpp:106] Iteration 200, lr = 0.0001
I0814 08:20:04.619093 28599 solver.cpp:229] Iteration 250, loss = 0.332744
I0814 08:20:04.779960 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0616997
I0814 08:20:04.780012 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.117739
I0814 08:20:04.780037 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.166372
I0814 08:20:04.780047 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.166372 (* 1 = 0.166372 loss)
I0814 08:20:04.780055 28599 sgd_solver.cpp:106] Iteration 250, lr = 0.0001
I0814 08:20:37.872954 28599 solver.cpp:229] Iteration 300, loss = 0.326002
I0814 08:20:38.031389 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0758486
I0814 08:20:38.031450 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.142754
I0814 08:20:38.031472 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.163001
I0814 08:20:38.031483 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.163001 (* 1 = 0.163001 loss)
I0814 08:20:38.031523 28599 sgd_solver.cpp:106] Iteration 300, lr = 0.0001
I0814 08:21:11.233072 28599 solver.cpp:229] Iteration 350, loss = 0.278926
I0814 08:21:11.390627 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.055547
I0814 08:21:11.390686 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.110753
I0814 08:21:11.390707 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.139463
I0814 08:21:11.390717 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.139463 (* 1 = 0.139463 loss)
I0814 08:21:11.390724 28599 sgd_solver.cpp:106] Iteration 350, lr = 0.0001
I0814 08:21:44.609189 28599 solver.cpp:229] Iteration 400, loss = 0.370027
I0814 08:21:44.767698 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.090706
I0814 08:21:44.767755 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.173996
I0814 08:21:44.767772 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.185014
I0814 08:21:44.767810 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.185014 (* 1 = 0.185014 loss)
I0814 08:21:44.767822 28599 sgd_solver.cpp:106] Iteration 400, lr = 0.0001
I0814 08:22:17.736907 28599 solver.cpp:229] Iteration 450, loss = 0.20944
I0814 08:22:17.893499 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0593376
I0814 08:22:17.893549 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.114484
I0814 08:22:17.893565 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.10472
I0814 08:22:17.893576 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.10472 (* 1 = 0.10472 loss)
I0814 08:22:17.893585 28599 sgd_solver.cpp:106] Iteration 450, lr = 0.0001
I0814 08:22:50.639677 28599 solver.cpp:229] Iteration 500, loss = 0.301046
I0814 08:22:50.797210 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0619084
I0814 08:22:50.797264 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.112302
I0814 08:22:50.797281 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.150523
I0814 08:22:50.797291 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.150523 (* 1 = 0.150523 loss)
I0814 08:22:50.797297 28599 sgd_solver.cpp:106] Iteration 500, lr = 0.0001
I0814 08:23:23.936162 28599 solver.cpp:229] Iteration 550, loss = 0.232011
I0814 08:23:24.094369 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0709575
I0814 08:23:24.094437 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.135693
I0814 08:23:24.094460 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.116006
I0814 08:23:24.094476 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.116006 (* 1 = 0.116006 loss)
I0814 08:23:24.094487 28599 sgd_solver.cpp:106] Iteration 550, lr = 0.0001
I0814 08:23:56.763003 28599 solver.cpp:229] Iteration 600, loss = 0.269926
I0814 08:23:56.924062 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0895412
I0814 08:23:56.924115 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.160127
I0814 08:23:56.924134 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.134963
I0814 08:23:56.924144 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.134963 (* 1 = 0.134963 loss)
I0814 08:23:56.924151 28599 sgd_solver.cpp:106] Iteration 600, lr = 0.0001
I0814 08:24:29.948252 28599 solver.cpp:229] Iteration 650, loss = 0.2628
I0814 08:24:30.107590 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.058593
I0814 08:24:30.107659 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.111229
I0814 08:24:30.107684 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.1314
I0814 08:24:30.107700 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.1314 (* 1 = 0.1314 loss)
I0814 08:24:30.107741 28599 sgd_solver.cpp:106] Iteration 650, lr = 0.0001
I0814 08:25:03.808576 28599 solver.cpp:229] Iteration 700, loss = 0.202027
I0814 08:25:03.966127 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0519683
I0814 08:25:03.966183 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.100052
I0814 08:25:03.966199 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.101013
I0814 08:25:03.966210 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.101013 (* 1 = 0.101013 loss)
I0814 08:25:03.966218 28599 sgd_solver.cpp:106] Iteration 700, lr = 0.0001
I0814 08:25:37.083746 28599 solver.cpp:229] Iteration 750, loss = 0.405802
I0814 08:25:37.244163 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.142565
I0814 08:25:37.244213 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.265841
I0814 08:25:37.244236 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.202901
I0814 08:25:37.244256 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.202901 (* 1 = 0.202901 loss)
I0814 08:25:37.244262 28599 sgd_solver.cpp:106] Iteration 750, lr = 0.0001
I0814 08:26:10.621717 28599 solver.cpp:229] Iteration 800, loss = 0.347868
I0814 08:26:10.779376 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.127122
I0814 08:26:10.779429 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.248437
I0814 08:26:10.779445 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.173934
I0814 08:26:10.779458 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.173934 (* 1 = 0.173934 loss)
I0814 08:26:10.779466 28599 sgd_solver.cpp:106] Iteration 800, lr = 0.0001
I0814 08:26:43.767772 28599 solver.cpp:229] Iteration 850, loss = 0.469155
I0814 08:26:43.923784 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.113148
I0814 08:26:43.923848 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.210836
I0814 08:26:43.923871 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.234578
I0814 08:26:43.923887 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.234578 (* 1 = 0.234578 loss)
I0814 08:26:43.923897 28599 sgd_solver.cpp:106] Iteration 850, lr = 0.0001
I0814 08:27:16.247179 28599 solver.cpp:229] Iteration 900, loss = 0.265937
I0814 08:27:16.406153 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0919874
I0814 08:27:16.406203 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.177625
I0814 08:27:16.406219 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.132969
I0814 08:27:16.406231 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.132969 (* 1 = 0.132969 loss)
I0814 08:27:16.406239 28599 sgd_solver.cpp:106] Iteration 900, lr = 0.0001
I0814 08:27:50.013258 28599 solver.cpp:229] Iteration 950, loss = 0.238367
I0814 08:27:50.174418 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.082748
I0814 08:27:50.174476 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.161892
I0814 08:27:50.174494 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.119184
I0814 08:27:50.174504 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.119184 (* 1 = 0.119184 loss)
I0814 08:27:50.174512 28599 sgd_solver.cpp:106] Iteration 950, lr = 0.0001
I0814 08:28:22.913799 28599 solver.cpp:229] Iteration 1000, loss = 0.199177
I0814 08:28:23.071475 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.08409
I0814 08:28:23.071542 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.158906
I0814 08:28:23.071564 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0995885
I0814 08:28:23.071581 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0995885 (* 1 = 0.0995885 loss)
I0814 08:28:23.071593 28599 sgd_solver.cpp:106] Iteration 1000, lr = 0.0001
I0814 08:28:57.159921 28599 solver.cpp:229] Iteration 1050, loss = 0.157711
I0814 08:28:57.321375 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0493022
I0814 08:28:57.321441 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0960915
I0814 08:28:57.321455 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0788552
I0814 08:28:57.321477 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0788552 (* 1 = 0.0788552 loss)
I0814 08:28:57.321486 28599 sgd_solver.cpp:106] Iteration 1050, lr = 0.0001
I0814 08:29:30.344050 28599 solver.cpp:229] Iteration 1100, loss = 0.165985
I0814 08:29:30.504254 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.069056
I0814 08:29:30.504308 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.128048
I0814 08:29:30.504321 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0829925
I0814 08:29:30.504341 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0829925 (* 1 = 0.0829925 loss)
I0814 08:29:30.504348 28599 sgd_solver.cpp:106] Iteration 1100, lr = 0.0001
I0814 08:30:03.732892 28599 solver.cpp:229] Iteration 1150, loss = 0.191664
I0814 08:30:03.892917 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0579794
I0814 08:30:03.892977 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.113259
I0814 08:30:03.892997 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0958317
I0814 08:30:03.893008 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0958317 (* 1 = 0.0958317 loss)
I0814 08:30:03.893018 28599 sgd_solver.cpp:106] Iteration 1150, lr = 0.0001
I0814 08:30:38.350741 28599 solver.cpp:229] Iteration 1200, loss = 0.196425
I0814 08:30:38.521510 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0764866
I0814 08:30:38.521708 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.151305
I0814 08:30:38.521821 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0982125
I0814 08:30:38.521896 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0982125 (* 1 = 0.0982125 loss)
I0814 08:30:38.521950 28599 sgd_solver.cpp:106] Iteration 1200, lr = 0.0001
I0814 08:31:12.985029 28599 solver.cpp:229] Iteration 1250, loss = 0.290054
I0814 08:31:13.144758 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.133951
I0814 08:31:13.144829 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.250582
I0814 08:31:13.144845 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.145027
I0814 08:31:13.144855 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.145027 (* 1 = 0.145027 loss)
I0814 08:31:13.144892 28599 sgd_solver.cpp:106] Iteration 1250, lr = 0.0001
I0814 08:31:45.799053 28599 solver.cpp:229] Iteration 1300, loss = 0.325851
I0814 08:31:45.958487 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.116623
I0814 08:31:45.958540 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.217449
I0814 08:31:45.958557 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.162925
I0814 08:31:45.958567 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.162925 (* 1 = 0.162925 loss)
I0814 08:31:45.958575 28599 sgd_solver.cpp:106] Iteration 1300, lr = 0.0001
I0814 08:32:19.010622 28599 solver.cpp:229] Iteration 1350, loss = 0.294506
I0814 08:32:19.169378 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.115675
I0814 08:32:19.169432 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.206882
I0814 08:32:19.169448 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.147253
I0814 08:32:19.169459 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.147253 (* 1 = 0.147253 loss)
I0814 08:32:19.169467 28599 sgd_solver.cpp:106] Iteration 1350, lr = 0.0001
I0814 08:32:51.948972 28599 solver.cpp:229] Iteration 1400, loss = 0.195981
I0814 08:32:52.107141 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0841999
I0814 08:32:52.107192 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.164768
I0814 08:32:52.107220 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0979902
I0814 08:32:52.107239 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0979902 (* 1 = 0.0979902 loss)
I0814 08:32:52.107255 28599 sgd_solver.cpp:106] Iteration 1400, lr = 0.0001
I0814 08:33:24.597329 28599 solver.cpp:229] Iteration 1450, loss = 0.337772
I0814 08:33:24.750581 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.131565
I0814 08:33:24.750689 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.245428
I0814 08:33:24.750713 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.168886
I0814 08:33:24.750723 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.168886 (* 1 = 0.168886 loss)
I0814 08:33:24.750732 28599 sgd_solver.cpp:106] Iteration 1450, lr = 0.0001
I0814 08:33:59.028899 28599 solver.cpp:229] Iteration 1500, loss = 0.223173
I0814 08:33:59.185395 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0838758
I0814 08:33:59.185459 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.159746
I0814 08:33:59.185482 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.111586
I0814 08:33:59.185497 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.111586 (* 1 = 0.111586 loss)
I0814 08:33:59.185508 28599 sgd_solver.cpp:106] Iteration 1500, lr = 0.0001
I0814 08:34:33.351948 28599 solver.cpp:229] Iteration 1550, loss = 0.140785
I0814 08:34:33.511270 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0576577
I0814 08:34:33.511394 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.110052
I0814 08:34:33.511437 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0703923
I0814 08:34:33.511461 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0703923 (* 1 = 0.0703923 loss)
I0814 08:34:33.511479 28599 sgd_solver.cpp:106] Iteration 1550, lr = 0.0001
I0814 08:35:06.640199 28599 solver.cpp:229] Iteration 1600, loss = 0.279792
I0814 08:35:06.797555 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.125678
I0814 08:35:06.797611 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.24552
I0814 08:35:06.797631 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.139896
I0814 08:35:06.797641 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.139896 (* 1 = 0.139896 loss)
I0814 08:35:06.797650 28599 sgd_solver.cpp:106] Iteration 1600, lr = 0.0001
I0814 08:35:40.257603 28599 solver.cpp:229] Iteration 1650, loss = 0.134675
I0814 08:35:40.416931 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0587265
I0814 08:35:40.416985 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.111881
I0814 08:35:40.416999 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0673375
I0814 08:35:40.417009 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0673375 (* 1 = 0.0673375 loss)
I0814 08:35:40.417016 28599 sgd_solver.cpp:106] Iteration 1650, lr = 0.0001
I0814 08:36:13.919385 28599 solver.cpp:229] Iteration 1700, loss = 0.210242
I0814 08:36:14.074966 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0801459
I0814 08:36:14.075047 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.154075
I0814 08:36:14.075083 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.105121
I0814 08:36:14.075101 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.105121 (* 1 = 0.105121 loss)
I0814 08:36:14.075116 28599 sgd_solver.cpp:106] Iteration 1700, lr = 0.0001
I0814 08:36:47.482836 28599 solver.cpp:229] Iteration 1750, loss = 0.13134
I0814 08:36:47.641263 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0460234
I0814 08:36:47.641330 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0889526
I0814 08:36:47.641358 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0656701
I0814 08:36:47.641373 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0656701 (* 1 = 0.0656701 loss)
I0814 08:36:47.641383 28599 sgd_solver.cpp:106] Iteration 1750, lr = 0.0001
I0814 08:37:20.760526 28599 solver.cpp:229] Iteration 1800, loss = 0.263188
I0814 08:37:20.917949 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0618275
I0814 08:37:20.918002 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.117776
I0814 08:37:20.918017 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.131594
I0814 08:37:20.918028 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.131594 (* 1 = 0.131594 loss)
I0814 08:37:20.918035 28599 sgd_solver.cpp:106] Iteration 1800, lr = 0.0001
I0814 08:37:53.878521 28599 solver.cpp:229] Iteration 1850, loss = 0.170592
I0814 08:37:54.036945 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0784999
I0814 08:37:54.037017 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.149437
I0814 08:37:54.037037 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0852958
I0814 08:37:54.037081 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0852958 (* 1 = 0.0852958 loss)
I0814 08:37:54.037106 28599 sgd_solver.cpp:106] Iteration 1850, lr = 0.0001
I0814 08:38:27.081722 28599 solver.cpp:229] Iteration 1900, loss = 0.178169
I0814 08:38:27.237735 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.072185
I0814 08:38:27.237802 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139214
I0814 08:38:27.237823 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0890844
I0814 08:38:27.237838 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0890844 (* 1 = 0.0890844 loss)
I0814 08:38:27.237848 28599 sgd_solver.cpp:106] Iteration 1900, lr = 0.0001
I0814 08:39:00.351410 28599 solver.cpp:229] Iteration 1950, loss = 0.155218
I0814 08:39:00.508064 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0788628
I0814 08:39:00.508116 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.150995
I0814 08:39:00.508131 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.077609
I0814 08:39:00.508139 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.077609 (* 1 = 0.077609 loss)
I0814 08:39:00.508148 28599 sgd_solver.cpp:106] Iteration 1950, lr = 0.0001
I0814 08:39:33.512576 28599 solver.cpp:229] Iteration 2000, loss = 0.132241
I0814 08:39:33.669724 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.062002
I0814 08:39:33.669829 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.117758
I0814 08:39:33.669878 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0661203
I0814 08:39:33.669899 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0661203 (* 1 = 0.0661203 loss)
I0814 08:39:33.669960 28599 sgd_solver.cpp:106] Iteration 2000, lr = 0.0001
I0814 08:40:06.307950 28599 solver.cpp:229] Iteration 2050, loss = 0.224272
I0814 08:40:06.465193 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0836358
I0814 08:40:06.465247 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.162184
I0814 08:40:06.465265 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.112136
I0814 08:40:06.465275 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.112136 (* 1 = 0.112136 loss)
I0814 08:40:06.465282 28599 sgd_solver.cpp:106] Iteration 2050, lr = 0.0001
I0814 08:40:39.634451 28599 solver.cpp:229] Iteration 2100, loss = 0.250424
I0814 08:40:39.792748 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.124413
I0814 08:40:39.792799 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.239178
I0814 08:40:39.792815 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.125212
I0814 08:40:39.792826 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.125212 (* 1 = 0.125212 loss)
I0814 08:40:39.792834 28599 sgd_solver.cpp:106] Iteration 2100, lr = 0.0001
I0814 08:41:12.773478 28599 solver.cpp:229] Iteration 2150, loss = 0.242063
I0814 08:41:12.929198 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.091222
I0814 08:41:12.929265 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.177608
I0814 08:41:12.929285 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.121031
I0814 08:41:12.929298 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.121031 (* 1 = 0.121031 loss)
I0814 08:41:12.929307 28599 sgd_solver.cpp:106] Iteration 2150, lr = 0.0001
I0814 08:41:45.630396 28599 solver.cpp:229] Iteration 2200, loss = 0.190872
I0814 08:41:45.784785 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0493554
I0814 08:41:45.784945 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0940508
I0814 08:41:45.785009 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.095436
I0814 08:41:45.785048 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.095436 (* 1 = 0.095436 loss)
I0814 08:41:45.785074 28599 sgd_solver.cpp:106] Iteration 2200, lr = 0.0001
I0814 08:42:21.615262 28599 solver.cpp:229] Iteration 2250, loss = 0.132432
I0814 08:42:21.775535 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0441255
I0814 08:42:21.775599 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0837964
I0814 08:42:21.775620 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0662158
I0814 08:42:21.775632 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0662158 (* 1 = 0.0662158 loss)
I0814 08:42:21.775642 28599 sgd_solver.cpp:106] Iteration 2250, lr = 0.0001
I0814 08:42:54.659493 28599 solver.cpp:229] Iteration 2300, loss = 0.144412
I0814 08:42:54.818490 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0617125
I0814 08:42:54.818544 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.116993
I0814 08:42:54.818563 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0722061
I0814 08:42:54.818573 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0722061 (* 1 = 0.0722061 loss)
I0814 08:42:54.818581 28599 sgd_solver.cpp:106] Iteration 2300, lr = 0.0001
I0814 08:43:27.843745 28599 solver.cpp:229] Iteration 2350, loss = 0.208614
I0814 08:43:28.001504 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.093548
I0814 08:43:28.001567 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.178608
I0814 08:43:28.001590 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.104307
I0814 08:43:28.001605 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.104307 (* 1 = 0.104307 loss)
I0814 08:43:28.001616 28599 sgd_solver.cpp:106] Iteration 2350, lr = 0.0001
I0814 08:44:00.981472 28599 solver.cpp:229] Iteration 2400, loss = 0.237977
I0814 08:44:01.142637 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.119261
I0814 08:44:01.142701 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.220397
I0814 08:44:01.142724 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.118989
I0814 08:44:01.142776 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.118989 (* 1 = 0.118989 loss)
I0814 08:44:01.142791 28599 sgd_solver.cpp:106] Iteration 2400, lr = 0.0001
I0814 08:44:34.360441 28599 solver.cpp:229] Iteration 2450, loss = 0.199087
I0814 08:44:34.516381 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0869638
I0814 08:44:34.516453 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.16043
I0814 08:44:34.516476 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0995436
I0814 08:44:34.516494 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0995436 (* 1 = 0.0995436 loss)
I0814 08:44:34.516506 28599 sgd_solver.cpp:106] Iteration 2450, lr = 0.0001
I0814 08:45:07.314049 28599 solver.cpp:229] Iteration 2500, loss = 0.167271
I0814 08:45:07.471861 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0673959
I0814 08:45:07.471930 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.127047
I0814 08:45:07.471956 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0836354
I0814 08:45:07.471969 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0836354 (* 1 = 0.0836354 loss)
I0814 08:45:07.471981 28599 sgd_solver.cpp:106] Iteration 2500, lr = 0.0001
I0814 08:45:40.949390 28599 solver.cpp:229] Iteration 2550, loss = 0.173929
I0814 08:45:41.111099 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0877512
I0814 08:45:41.111165 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.166864
I0814 08:45:41.111187 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0869647
I0814 08:45:41.111202 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0869647 (* 1 = 0.0869647 loss)
I0814 08:45:41.111212 28599 sgd_solver.cpp:106] Iteration 2550, lr = 0.0001
I0814 08:46:13.712159 28599 solver.cpp:229] Iteration 2600, loss = 0.274654
I0814 08:46:13.870275 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.14375
I0814 08:46:13.870349 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.266567
I0814 08:46:13.870424 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.137327
I0814 08:46:13.870443 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.137327 (* 1 = 0.137327 loss)
I0814 08:46:13.870453 28599 sgd_solver.cpp:106] Iteration 2600, lr = 0.0001
I0814 08:46:47.025283 28599 solver.cpp:229] Iteration 2650, loss = 0.185702
I0814 08:46:47.183015 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0859173
I0814 08:46:47.183076 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163611
I0814 08:46:47.183130 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.092851
I0814 08:46:47.183145 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.092851 (* 1 = 0.092851 loss)
I0814 08:46:47.183169 28599 sgd_solver.cpp:106] Iteration 2650, lr = 0.0001
I0814 08:47:20.711300 28599 solver.cpp:229] Iteration 2700, loss = 0.205971
I0814 08:47:20.868109 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0905044
I0814 08:47:20.868178 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.173716
I0814 08:47:20.868202 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.102985
I0814 08:47:20.868221 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.102985 (* 1 = 0.102985 loss)
I0814 08:47:20.868232 28599 sgd_solver.cpp:106] Iteration 2700, lr = 0.0001
I0814 08:47:53.730064 28599 solver.cpp:229] Iteration 2750, loss = 0.161191
I0814 08:47:53.893667 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0528456
I0814 08:47:53.893733 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0945855
I0814 08:47:53.893754 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0805955
I0814 08:47:53.893770 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0805955 (* 1 = 0.0805955 loss)
I0814 08:47:53.893780 28599 sgd_solver.cpp:106] Iteration 2750, lr = 0.0001
I0814 08:48:26.453868 28599 solver.cpp:229] Iteration 2800, loss = 0.195004
I0814 08:48:26.610096 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0789998
I0814 08:48:26.610169 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.145133
I0814 08:48:26.610198 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.097502
I0814 08:48:26.610213 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.097502 (* 1 = 0.097502 loss)
I0814 08:48:26.610225 28599 sgd_solver.cpp:106] Iteration 2800, lr = 0.0001
I0814 08:48:59.915468 28599 solver.cpp:229] Iteration 2850, loss = 0.278688
I0814 08:49:00.074002 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.142303
I0814 08:49:00.074069 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.266764
I0814 08:49:00.074090 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.139344
I0814 08:49:00.074105 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.139344 (* 1 = 0.139344 loss)
I0814 08:49:00.074116 28599 sgd_solver.cpp:106] Iteration 2850, lr = 0.0001
I0814 08:49:33.137845 28599 solver.cpp:229] Iteration 2900, loss = 0.17035
I0814 08:49:33.295222 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0759678
I0814 08:49:33.295289 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.149387
I0814 08:49:33.295312 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0851749
I0814 08:49:33.295327 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0851749 (* 1 = 0.0851749 loss)
I0814 08:49:33.295338 28599 sgd_solver.cpp:106] Iteration 2900, lr = 0.0001
I0814 08:50:06.237644 28599 solver.cpp:229] Iteration 2950, loss = 0.237581
I0814 08:50:06.394520 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.121797
I0814 08:50:06.394634 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.227508
I0814 08:50:06.394654 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.118791
I0814 08:50:06.394670 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.118791 (* 1 = 0.118791 loss)
I0814 08:50:06.394685 28599 sgd_solver.cpp:106] Iteration 2950, lr = 0.0001
I0814 08:50:39.410944 28599 solver.cpp:229] Iteration 3000, loss = 0.134515
I0814 08:50:39.572940 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0620336
I0814 08:50:39.573009 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.116653
I0814 08:50:39.573034 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0672575
I0814 08:50:39.573050 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0672575 (* 1 = 0.0672575 loss)
I0814 08:50:39.573061 28599 sgd_solver.cpp:106] Iteration 3000, lr = 0.0001
I0814 08:51:12.455904 28599 solver.cpp:229] Iteration 3050, loss = 0.302694
I0814 08:51:12.612881 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0963719
I0814 08:51:12.612942 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.155665
I0814 08:51:12.612973 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.151347
I0814 08:51:12.612987 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.151347 (* 1 = 0.151347 loss)
I0814 08:51:12.613001 28599 sgd_solver.cpp:106] Iteration 3050, lr = 0.0001
I0814 08:51:46.297328 28599 solver.cpp:229] Iteration 3100, loss = 0.269013
I0814 08:51:46.456851 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0784484
I0814 08:51:46.456909 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.149311
I0814 08:51:46.456925 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.134507
I0814 08:51:46.456936 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.134507 (* 1 = 0.134507 loss)
I0814 08:51:46.456943 28599 sgd_solver.cpp:106] Iteration 3100, lr = 0.0001
I0814 08:52:19.629276 28599 solver.cpp:229] Iteration 3150, loss = 0.177937
I0814 08:52:19.791577 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0646045
I0814 08:52:19.791640 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.125243
I0814 08:52:19.791661 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0889687
I0814 08:52:19.791672 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0889687 (* 1 = 0.0889687 loss)
I0814 08:52:19.791682 28599 sgd_solver.cpp:106] Iteration 3150, lr = 0.0001
I0814 08:52:52.701411 28599 solver.cpp:229] Iteration 3200, loss = 0.189508
I0814 08:52:52.858028 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0920556
I0814 08:52:52.858103 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.161666
I0814 08:52:52.858132 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.094754
I0814 08:52:52.858148 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.094754 (* 1 = 0.094754 loss)
I0814 08:52:52.858161 28599 sgd_solver.cpp:106] Iteration 3200, lr = 0.0001
I0814 08:53:25.835321 28599 solver.cpp:229] Iteration 3250, loss = 0.208634
I0814 08:53:25.992456 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0953406
I0814 08:53:25.992524 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.177001
I0814 08:53:25.992553 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.104317
I0814 08:53:25.992568 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.104317 (* 1 = 0.104317 loss)
I0814 08:53:25.992579 28599 sgd_solver.cpp:106] Iteration 3250, lr = 0.0001
I0814 08:53:59.459408 28599 solver.cpp:229] Iteration 3300, loss = 0.255396
I0814 08:53:59.616768 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.117703
I0814 08:53:59.616837 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.208306
I0814 08:53:59.616861 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.127698
I0814 08:53:59.616876 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.127698 (* 1 = 0.127698 loss)
I0814 08:53:59.616885 28599 sgd_solver.cpp:106] Iteration 3300, lr = 0.0001
I0814 08:54:32.420606 28599 solver.cpp:229] Iteration 3350, loss = 0.0522717
I0814 08:54:32.578407 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0201217
I0814 08:54:32.578466 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0397929
I0814 08:54:32.578482 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0261359
I0814 08:54:32.578493 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0261359 (* 1 = 0.0261359 loss)
I0814 08:54:32.578502 28599 sgd_solver.cpp:106] Iteration 3350, lr = 0.0001
I0814 08:55:05.589820 28599 solver.cpp:229] Iteration 3400, loss = 0.273819
I0814 08:55:05.746441 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0675926
I0814 08:55:05.746513 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.133087
I0814 08:55:05.746537 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.136909
I0814 08:55:05.746554 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.136909 (* 1 = 0.136909 loss)
I0814 08:55:05.746565 28599 sgd_solver.cpp:106] Iteration 3400, lr = 0.0001
I0814 08:55:39.630797 28599 solver.cpp:229] Iteration 3450, loss = 0.187113
I0814 08:55:39.789597 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0876022
I0814 08:55:39.789655 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163031
I0814 08:55:39.789675 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0935567
I0814 08:55:39.789687 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0935567 (* 1 = 0.0935567 loss)
I0814 08:55:39.789695 28599 sgd_solver.cpp:106] Iteration 3450, lr = 0.0001
I0814 08:56:12.392324 28599 solver.cpp:229] Iteration 3500, loss = 0.2247
I0814 08:56:12.552273 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0841645
I0814 08:56:12.552331 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.157252
I0814 08:56:12.552350 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11235
I0814 08:56:12.552361 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11235 (* 1 = 0.11235 loss)
I0814 08:56:12.552371 28599 sgd_solver.cpp:106] Iteration 3500, lr = 0.0001
I0814 08:56:46.168120 28599 solver.cpp:229] Iteration 3550, loss = 0.289945
I0814 08:56:46.326381 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.151165
I0814 08:56:46.326452 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.289692
I0814 08:56:46.326481 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.144972
I0814 08:56:46.326498 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.144972 (* 1 = 0.144972 loss)
I0814 08:56:46.326511 28599 sgd_solver.cpp:106] Iteration 3550, lr = 0.0001
I0814 08:57:19.635465 28599 solver.cpp:229] Iteration 3600, loss = 0.183149
I0814 08:57:19.793316 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0757921
I0814 08:57:19.793448 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.14523
I0814 08:57:19.793480 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0915746
I0814 08:57:19.793493 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0915746 (* 1 = 0.0915746 loss)
I0814 08:57:19.793503 28599 sgd_solver.cpp:106] Iteration 3600, lr = 0.0001
I0814 08:57:52.715939 28599 solver.cpp:229] Iteration 3650, loss = 0.168389
I0814 08:57:52.875300 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0678931
I0814 08:57:52.875361 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129923
I0814 08:57:52.875385 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0841945
I0814 08:57:52.875398 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0841945 (* 1 = 0.0841945 loss)
I0814 08:57:52.875408 28599 sgd_solver.cpp:106] Iteration 3650, lr = 0.0001
I0814 08:58:26.088390 28599 solver.cpp:229] Iteration 3700, loss = 0.123018
I0814 08:58:26.247750 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0635496
I0814 08:58:26.247817 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.120059
I0814 08:58:26.247839 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0615092
I0814 08:58:26.247853 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0615092 (* 1 = 0.0615092 loss)
I0814 08:58:26.247864 28599 sgd_solver.cpp:106] Iteration 3700, lr = 0.0001
I0814 08:58:59.935457 28599 solver.cpp:229] Iteration 3750, loss = 0.130778
I0814 08:59:00.093230 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.053769
I0814 08:59:00.093288 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.104727
I0814 08:59:00.093308 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0653889
I0814 08:59:00.093322 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0653889 (* 1 = 0.0653889 loss)
I0814 08:59:00.093331 28599 sgd_solver.cpp:106] Iteration 3750, lr = 0.0001
I0814 08:59:33.240749 28599 solver.cpp:229] Iteration 3800, loss = 0.158301
I0814 08:59:33.397250 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0850869
I0814 08:59:33.397333 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.161981
I0814 08:59:33.397367 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0791504
I0814 08:59:33.397387 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0791504 (* 1 = 0.0791504 loss)
I0814 08:59:33.397403 28599 sgd_solver.cpp:106] Iteration 3800, lr = 0.0001
I0814 09:00:06.569133 28599 solver.cpp:229] Iteration 3850, loss = 0.241265
I0814 09:00:06.728667 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.121895
I0814 09:00:06.728742 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.219435
I0814 09:00:06.728770 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.120632
I0814 09:00:06.728786 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.120632 (* 1 = 0.120632 loss)
I0814 09:00:06.728798 28599 sgd_solver.cpp:106] Iteration 3850, lr = 0.0001
I0814 09:00:39.859828 28599 solver.cpp:229] Iteration 3900, loss = 0.145256
I0814 09:00:40.018903 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0751182
I0814 09:00:40.018992 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.142164
I0814 09:00:40.019032 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0726282
I0814 09:00:40.019057 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0726282 (* 1 = 0.0726282 loss)
I0814 09:00:40.019075 28599 sgd_solver.cpp:106] Iteration 3900, lr = 0.0001
I0814 09:01:12.709782 28599 solver.cpp:229] Iteration 3950, loss = 0.185897
I0814 09:01:12.869019 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0934303
I0814 09:01:12.869102 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.179152
I0814 09:01:12.869125 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0929487
I0814 09:01:12.869132 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0929487 (* 1 = 0.0929487 loss)
I0814 09:01:12.869140 28599 sgd_solver.cpp:106] Iteration 3950, lr = 0.0001
I0814 09:01:46.045068 28599 solver.cpp:229] Iteration 4000, loss = 0.234518
I0814 09:01:46.207566 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.108268
I0814 09:01:46.207620 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.20226
I0814 09:01:46.207646 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.117259
I0814 09:01:46.207665 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.117259 (* 1 = 0.117259 loss)
I0814 09:01:46.207681 28599 sgd_solver.cpp:106] Iteration 4000, lr = 0.0001
I0814 09:02:19.725682 28599 solver.cpp:229] Iteration 4050, loss = 0.135546
I0814 09:02:19.882261 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0439356
I0814 09:02:19.882325 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0827508
I0814 09:02:19.882349 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0677729
I0814 09:02:19.882365 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0677729 (* 1 = 0.0677729 loss)
I0814 09:02:19.882377 28599 sgd_solver.cpp:106] Iteration 4050, lr = 0.0001
I0814 09:02:52.998953 28599 solver.cpp:229] Iteration 4100, loss = 0.133783
I0814 09:02:53.156520 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0643564
I0814 09:02:53.156579 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.126579
I0814 09:02:53.156596 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0668914
I0814 09:02:53.156606 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0668914 (* 1 = 0.0668914 loss)
I0814 09:02:53.156615 28599 sgd_solver.cpp:106] Iteration 4100, lr = 0.0001
I0814 09:03:26.428438 28599 solver.cpp:229] Iteration 4150, loss = 0.274395
I0814 09:03:26.585191 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.149912
I0814 09:03:26.585256 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.283831
I0814 09:03:26.585280 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.137197
I0814 09:03:26.585296 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.137197 (* 1 = 0.137197 loss)
I0814 09:03:26.585309 28599 sgd_solver.cpp:106] Iteration 4150, lr = 0.0001
I0814 09:03:59.669663 28599 solver.cpp:229] Iteration 4200, loss = 0.172105
I0814 09:03:59.828213 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.087345
I0814 09:03:59.828279 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.171258
I0814 09:03:59.828306 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0860525
I0814 09:03:59.828320 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0860525 (* 1 = 0.0860525 loss)
I0814 09:03:59.828330 28599 sgd_solver.cpp:106] Iteration 4200, lr = 0.0001
I0814 09:04:33.029106 28599 solver.cpp:229] Iteration 4250, loss = 0.0978374
I0814 09:04:33.189224 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0497985
I0814 09:04:33.189291 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0967518
I0814 09:04:33.189355 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0489187
I0814 09:04:33.189375 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0489187 (* 1 = 0.0489187 loss)
I0814 09:04:33.189404 28599 sgd_solver.cpp:106] Iteration 4250, lr = 0.0001
I0814 09:05:05.672224 28599 solver.cpp:229] Iteration 4300, loss = 0.194411
I0814 09:05:05.830148 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0880477
I0814 09:05:05.830206 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.162218
I0814 09:05:05.830225 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0972055
I0814 09:05:05.830237 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0972055 (* 1 = 0.0972055 loss)
I0814 09:05:05.830246 28599 sgd_solver.cpp:106] Iteration 4300, lr = 0.0001
I0814 09:05:38.827580 28599 solver.cpp:229] Iteration 4350, loss = 0.09589
I0814 09:05:38.985530 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0485166
I0814 09:05:38.985586 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0942474
I0814 09:05:38.985605 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.047945
I0814 09:05:38.985617 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.047945 (* 1 = 0.047945 loss)
I0814 09:05:38.985627 28599 sgd_solver.cpp:106] Iteration 4350, lr = 0.0001
I0814 09:06:12.834475 28599 solver.cpp:229] Iteration 4400, loss = 0.215892
I0814 09:06:12.993311 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.117552
I0814 09:06:12.993372 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.224272
I0814 09:06:12.993392 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.107946
I0814 09:06:12.993402 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.107946 (* 1 = 0.107946 loss)
I0814 09:06:12.993412 28599 sgd_solver.cpp:106] Iteration 4400, lr = 0.0001
I0814 09:06:45.550544 28599 solver.cpp:229] Iteration 4450, loss = 0.226534
I0814 09:06:45.710942 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.124795
I0814 09:06:45.710999 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.235052
I0814 09:06:45.711017 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.113267
I0814 09:06:45.711028 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.113267 (* 1 = 0.113267 loss)
I0814 09:06:45.711036 28599 sgd_solver.cpp:106] Iteration 4450, lr = 0.0001
I0814 09:07:19.594781 28599 solver.cpp:229] Iteration 4500, loss = 0.197009
I0814 09:07:19.754082 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0983752
I0814 09:07:19.754148 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.186188
I0814 09:07:19.754170 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0985045
I0814 09:07:19.754184 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0985045 (* 1 = 0.0985045 loss)
I0814 09:07:19.754194 28599 sgd_solver.cpp:106] Iteration 4500, lr = 0.0001
I0814 09:07:53.060225 28599 solver.cpp:229] Iteration 4550, loss = 0.336305
I0814 09:07:53.220913 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.189106
I0814 09:07:53.220975 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.366272
I0814 09:07:53.220999 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.168153
I0814 09:07:53.221014 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.168153 (* 1 = 0.168153 loss)
I0814 09:07:53.221024 28599 sgd_solver.cpp:106] Iteration 4550, lr = 0.0001
I0814 09:08:27.632699 28599 solver.cpp:229] Iteration 4600, loss = 0.366583
I0814 09:08:27.793222 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.137606
I0814 09:08:27.793350 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.263625
I0814 09:08:27.793411 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.183292
I0814 09:08:27.793457 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.183292 (* 1 = 0.183292 loss)
I0814 09:08:27.793488 28599 sgd_solver.cpp:106] Iteration 4600, lr = 0.0001
I0814 09:09:05.598325 28599 solver.cpp:229] Iteration 4650, loss = 0.162167
I0814 09:09:05.760030 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0848258
I0814 09:09:05.760169 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163325
I0814 09:09:05.760232 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0810837
I0814 09:09:05.760268 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0810837 (* 1 = 0.0810837 loss)
I0814 09:09:05.760296 28599 sgd_solver.cpp:106] Iteration 4650, lr = 0.0001
I0814 09:09:40.514861 28599 solver.cpp:229] Iteration 4700, loss = 0.239566
I0814 09:09:40.673264 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.114823
I0814 09:09:40.673316 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.219863
I0814 09:09:40.673331 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.119783
I0814 09:09:40.673341 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.119783 (* 1 = 0.119783 loss)
I0814 09:09:40.673346 28599 sgd_solver.cpp:106] Iteration 4700, lr = 0.0001
I0814 09:10:19.657171 28599 solver.cpp:229] Iteration 4750, loss = 0.16541
I0814 09:10:19.820628 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0857465
I0814 09:10:19.820720 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.164149
I0814 09:10:19.820787 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0827051
I0814 09:10:19.820842 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0827051 (* 1 = 0.0827051 loss)
I0814 09:10:19.820873 28599 sgd_solver.cpp:106] Iteration 4750, lr = 0.0001
I0814 09:10:56.192577 28599 solver.cpp:229] Iteration 4800, loss = 0.279031
I0814 09:10:56.353317 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.139328
I0814 09:10:56.353395 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.26766
I0814 09:10:56.353422 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.139516
I0814 09:10:56.353441 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.139516 (* 1 = 0.139516 loss)
I0814 09:10:56.353456 28599 sgd_solver.cpp:106] Iteration 4800, lr = 0.0001
I0814 09:11:29.723877 28599 solver.cpp:229] Iteration 4850, loss = 0.154283
I0814 09:11:29.886222 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0882661
I0814 09:11:29.886281 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.166228
I0814 09:11:29.886298 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0771414
I0814 09:11:29.886310 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0771414 (* 1 = 0.0771414 loss)
I0814 09:11:29.886319 28599 sgd_solver.cpp:106] Iteration 4850, lr = 0.0001
I0814 09:12:03.234053 28599 solver.cpp:229] Iteration 4900, loss = 0.275182
I0814 09:12:03.396389 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.138672
I0814 09:12:03.396445 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.264387
I0814 09:12:03.396462 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.137591
I0814 09:12:03.396474 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.137591 (* 1 = 0.137591 loss)
I0814 09:12:03.396483 28599 sgd_solver.cpp:106] Iteration 4900, lr = 0.0001
I0814 09:12:38.498478 28599 solver.cpp:229] Iteration 4950, loss = 0.136168
I0814 09:12:38.661815 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0714732
I0814 09:12:38.661959 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.141615
I0814 09:12:38.662032 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0680839
I0814 09:12:38.662081 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0680839 (* 1 = 0.0680839 loss)
I0814 09:12:38.662116 28599 sgd_solver.cpp:106] Iteration 4950, lr = 0.0001
I0814 09:13:18.453420 28599 solver.cpp:229] Iteration 5000, loss = 0.107283
I0814 09:13:18.607427 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0449829
I0814 09:13:18.607568 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0860004
I0814 09:13:18.607648 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0536416
I0814 09:13:18.607698 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0536416 (* 1 = 0.0536416 loss)
I0814 09:13:18.607733 28599 sgd_solver.cpp:106] Iteration 5000, lr = 0.0001
I0814 09:13:58.325551 28599 solver.cpp:229] Iteration 5050, loss = 0.17164
I0814 09:13:58.480676 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0828916
I0814 09:13:58.480741 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.156027
I0814 09:13:58.480762 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0858198
I0814 09:13:58.480778 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0858198 (* 1 = 0.0858198 loss)
I0814 09:13:58.480788 28599 sgd_solver.cpp:106] Iteration 5050, lr = 0.0001
I0814 09:14:32.593376 28599 solver.cpp:229] Iteration 5100, loss = 0.897915
I0814 09:14:32.759069 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.130966
I0814 09:14:32.759214 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.244167
I0814 09:14:32.759253 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.448957
I0814 09:14:32.759268 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.448957 (* 1 = 0.448957 loss)
I0814 09:14:32.759282 28599 sgd_solver.cpp:106] Iteration 5100, lr = 0.0001
I0814 09:15:10.975697 28599 solver.cpp:229] Iteration 5150, loss = 0.140055
I0814 09:15:11.131973 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0605677
I0814 09:15:11.132045 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.117583
I0814 09:15:11.132077 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0700275
I0814 09:15:11.132102 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0700275 (* 1 = 0.0700275 loss)
I0814 09:15:11.132114 28599 sgd_solver.cpp:106] Iteration 5150, lr = 0.0001
I0814 09:15:44.794785 28599 solver.cpp:229] Iteration 5200, loss = 0.238975
I0814 09:15:44.954074 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.129794
I0814 09:15:44.954143 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.250074
I0814 09:15:44.954156 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.119488
I0814 09:15:44.954176 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.119488 (* 1 = 0.119488 loss)
I0814 09:15:44.954185 28599 sgd_solver.cpp:106] Iteration 5200, lr = 0.0001
I0814 09:16:18.503999 28599 solver.cpp:229] Iteration 5250, loss = 0.118327
I0814 09:16:18.660516 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0645049
I0814 09:16:18.660570 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.123162
I0814 09:16:18.660593 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0591637
I0814 09:16:18.660604 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0591637 (* 1 = 0.0591637 loss)
I0814 09:16:18.660612 28599 sgd_solver.cpp:106] Iteration 5250, lr = 0.0001
I0814 09:16:52.488186 28599 solver.cpp:229] Iteration 5300, loss = 0.235145
I0814 09:16:52.643612 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.130774
I0814 09:16:52.643663 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.241126
I0814 09:16:52.643677 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.117573
I0814 09:16:52.643687 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.117573 (* 1 = 0.117573 loss)
I0814 09:16:52.643694 28599 sgd_solver.cpp:106] Iteration 5300, lr = 0.0001
I0814 09:17:26.233393 28599 solver.cpp:229] Iteration 5350, loss = 0.197644
I0814 09:17:26.391096 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0954954
I0814 09:17:26.391155 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.177927
I0814 09:17:26.391172 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0988223
I0814 09:17:26.391209 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0988223 (* 1 = 0.0988223 loss)
I0814 09:17:26.391229 28599 sgd_solver.cpp:106] Iteration 5350, lr = 0.0001
I0814 09:17:59.595660 28599 solver.cpp:229] Iteration 5400, loss = 0.0955583
I0814 09:17:59.753944 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.050999
I0814 09:17:59.753994 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0961923
I0814 09:17:59.754009 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0477792
I0814 09:17:59.754029 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0477792 (* 1 = 0.0477792 loss)
I0814 09:17:59.754045 28599 sgd_solver.cpp:106] Iteration 5400, lr = 0.0001
I0814 09:18:33.473130 28599 solver.cpp:229] Iteration 5450, loss = 0.138971
I0814 09:18:33.632000 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0766857
I0814 09:18:33.632160 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.147382
I0814 09:18:33.632218 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0694854
I0814 09:18:33.632253 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0694854 (* 1 = 0.0694854 loss)
I0814 09:18:33.632277 28599 sgd_solver.cpp:106] Iteration 5450, lr = 0.0001
I0814 09:19:06.278031 28599 solver.cpp:456] Snapshotting to binary proto file norm_-0.01_iter_5500.caffemodel
I0814 09:19:06.334594 28599 sgd_solver.cpp:273] Snapshotting solver state to binary proto file norm_-0.01_iter_5500.solverstate
I0814 09:19:06.357890 28599 solver.cpp:338] Iteration 5500, Testing net (#0)
I0814 09:19:06.357919 28599 net.cpp:694] Ignoring source layer CustomData1
I0814 09:19:06.357924 28599 net.cpp:694] Ignoring source layer blob0_CustomData1_0_split
I0814 09:19:06.357928 28599 net.cpp:694] Ignoring source layer blob1_CustomData1_1_split
I0814 09:19:06.357933 28599 net.cpp:694] Ignoring source layer blob2_CustomData1_2_split
I0814 09:19:06.357940 28599 net.cpp:694] Ignoring source layer DummyData1
I0814 09:19:06.357944 28599 net.cpp:694] Ignoring source layer blob9_DummyData1_0_split
I0814 09:29:31.533785 28599 solver.cpp:406]     Test net output #0: down_up_half_loss_real = 0.0614319
I0814 09:29:31.533912 28599 solver.cpp:406]     Test net output #1: down_up_loss_real = 0.118198
I0814 09:29:31.533921 28599 solver.cpp:406]     Test net output #2: zoom_disp_loss0_real = 0.0573968
I0814 09:29:31.533927 28599 solver.cpp:406]     Test net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0573968 (* 1 = 0.0573968 loss)
I0814 09:29:31.696621 28599 solver.cpp:229] Iteration 5500, loss = 0.135618
I0814 09:29:31.862818 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0673772
I0814 09:29:31.862871 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.130094
I0814 09:29:31.862884 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0678089
I0814 09:29:31.862905 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0678089 (* 1 = 0.0678089 loss)
I0814 09:29:31.862911 28599 sgd_solver.cpp:106] Iteration 5500, lr = 0.0001
I0814 09:30:05.755446 28599 solver.cpp:229] Iteration 5550, loss = 0.158542
I0814 09:30:05.913372 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0805875
I0814 09:30:05.913437 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.149627
I0814 09:30:05.913461 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0792708
I0814 09:30:05.913473 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0792708 (* 1 = 0.0792708 loss)
I0814 09:30:05.913485 28599 sgd_solver.cpp:106] Iteration 5550, lr = 0.0001
I0814 09:30:38.915963 28599 solver.cpp:229] Iteration 5600, loss = 0.245437
I0814 09:30:39.077271 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0708353
I0814 09:30:39.077322 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.134451
I0814 09:30:39.077347 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.122718
I0814 09:30:39.077354 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.122718 (* 1 = 0.122718 loss)
I0814 09:30:39.077364 28599 sgd_solver.cpp:106] Iteration 5600, lr = 0.0001
I0814 09:31:12.162956 28599 solver.cpp:229] Iteration 5650, loss = 0.178947
I0814 09:31:12.327111 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0702802
I0814 09:31:12.327172 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129175
I0814 09:31:12.327198 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0894734
I0814 09:31:12.327210 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0894734 (* 1 = 0.0894734 loss)
I0814 09:31:12.327219 28599 sgd_solver.cpp:106] Iteration 5650, lr = 0.0001
I0814 09:31:45.621127 28599 solver.cpp:229] Iteration 5700, loss = 0.086908
I0814 09:31:45.779100 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0354965
I0814 09:31:45.779167 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0698019
I0814 09:31:45.779189 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0434541
I0814 09:31:45.779238 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0434541 (* 1 = 0.0434541 loss)
I0814 09:31:45.779268 28599 sgd_solver.cpp:106] Iteration 5700, lr = 0.0001
I0814 09:32:18.623706 28599 solver.cpp:229] Iteration 5750, loss = 0.126454
I0814 09:32:18.779754 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.051833
I0814 09:32:18.779826 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0989657
I0814 09:32:18.779848 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0632272
I0814 09:32:18.779865 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0632272 (* 1 = 0.0632272 loss)
I0814 09:32:18.779875 28599 sgd_solver.cpp:106] Iteration 5750, lr = 0.0001
I0814 09:32:51.725473 28599 solver.cpp:229] Iteration 5800, loss = 0.159392
I0814 09:32:51.882365 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0815318
I0814 09:32:51.882432 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.156879
I0814 09:32:51.882462 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.079696
I0814 09:32:51.882508 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.079696 (* 1 = 0.079696 loss)
I0814 09:32:51.882520 28599 sgd_solver.cpp:106] Iteration 5800, lr = 0.0001
I0814 09:33:25.128969 28599 solver.cpp:229] Iteration 5850, loss = 0.0892324
I0814 09:33:25.290285 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0490052
I0814 09:33:25.290352 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0977078
I0814 09:33:25.290377 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0446163
I0814 09:33:25.290395 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0446163 (* 1 = 0.0446163 loss)
I0814 09:33:25.290405 28599 sgd_solver.cpp:106] Iteration 5850, lr = 0.0001
I0814 09:33:58.442158 28599 solver.cpp:229] Iteration 5900, loss = 0.204652
I0814 09:33:58.599393 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0841999
I0814 09:33:58.599445 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163295
I0814 09:33:58.599462 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.102326
I0814 09:33:58.599472 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.102326 (* 1 = 0.102326 loss)
I0814 09:33:58.599479 28599 sgd_solver.cpp:106] Iteration 5900, lr = 0.0001
I0814 09:34:32.060219 28599 solver.cpp:229] Iteration 5950, loss = 0.138209
I0814 09:34:32.218382 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0645978
I0814 09:34:32.218442 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122995
I0814 09:34:32.218461 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0691048
I0814 09:34:32.218472 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0691048 (* 1 = 0.0691048 loss)
I0814 09:34:32.218482 28599 sgd_solver.cpp:106] Iteration 5950, lr = 0.0001
I0814 09:35:05.951625 28599 solver.cpp:229] Iteration 6000, loss = 0.120036
I0814 09:35:06.108749 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0599584
I0814 09:35:06.108816 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.112103
I0814 09:35:06.108839 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.060018
I0814 09:35:06.108853 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.060018 (* 1 = 0.060018 loss)
I0814 09:35:06.108865 28599 sgd_solver.cpp:106] Iteration 6000, lr = 0.0001
I0814 09:35:39.375407 28599 solver.cpp:229] Iteration 6050, loss = 0.152909
I0814 09:35:39.531185 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0787522
I0814 09:35:39.531250 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.15205
I0814 09:35:39.531299 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0764547
I0814 09:35:39.531327 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0764547 (* 1 = 0.0764547 loss)
I0814 09:35:39.531344 28599 sgd_solver.cpp:106] Iteration 6050, lr = 0.0001
I0814 09:36:12.663527 28599 solver.cpp:229] Iteration 6100, loss = 0.133371
I0814 09:36:12.820993 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0691192
I0814 09:36:12.821121 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.127325
I0814 09:36:12.821161 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0666855
I0814 09:36:12.821184 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0666855 (* 1 = 0.0666855 loss)
I0814 09:36:12.821200 28599 sgd_solver.cpp:106] Iteration 6100, lr = 0.0001
I0814 09:36:45.777508 28599 solver.cpp:229] Iteration 6150, loss = 0.167861
I0814 09:36:45.933176 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.074313
I0814 09:36:45.933235 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.143895
I0814 09:36:45.933256 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0839304
I0814 09:36:45.933267 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0839304 (* 1 = 0.0839304 loss)
I0814 09:36:45.933277 28599 sgd_solver.cpp:106] Iteration 6150, lr = 0.0001
I0814 09:37:19.464174 28599 solver.cpp:229] Iteration 6200, loss = 0.157548
I0814 09:37:19.625260 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0803759
I0814 09:37:19.625331 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.151695
I0814 09:37:19.625357 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.078774
I0814 09:37:19.625375 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.078774 (* 1 = 0.078774 loss)
I0814 09:37:19.625387 28599 sgd_solver.cpp:106] Iteration 6200, lr = 0.0001
I0814 09:37:53.116066 28599 solver.cpp:229] Iteration 6250, loss = 0.279089
I0814 09:37:53.272445 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.17495
I0814 09:37:53.272545 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.328005
I0814 09:37:53.272583 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.139545
I0814 09:37:53.272608 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.139545 (* 1 = 0.139545 loss)
I0814 09:37:53.272627 28599 sgd_solver.cpp:106] Iteration 6250, lr = 0.0001
I0814 09:38:26.701393 28599 solver.cpp:229] Iteration 6300, loss = 0.194744
I0814 09:38:26.856485 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0950377
I0814 09:38:26.856582 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.183472
I0814 09:38:26.856616 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0973721
I0814 09:38:26.856642 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0973721 (* 1 = 0.0973721 loss)
I0814 09:38:26.856657 28599 sgd_solver.cpp:106] Iteration 6300, lr = 0.0001
I0814 09:39:00.630575 28599 solver.cpp:229] Iteration 6350, loss = 0.168532
I0814 09:39:00.791062 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0824653
I0814 09:39:00.791116 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.151931
I0814 09:39:00.791132 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0842658
I0814 09:39:00.791144 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0842658 (* 1 = 0.0842658 loss)
I0814 09:39:00.791152 28599 sgd_solver.cpp:106] Iteration 6350, lr = 0.0001
I0814 09:39:33.329083 28599 solver.cpp:229] Iteration 6400, loss = 0.164604
I0814 09:39:33.489526 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0928957
I0814 09:39:33.489583 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.179018
I0814 09:39:33.489603 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.082302
I0814 09:39:33.489614 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.082302 (* 1 = 0.082302 loss)
I0814 09:39:33.489624 28599 sgd_solver.cpp:106] Iteration 6400, lr = 0.0001
I0814 09:40:06.574169 28599 solver.cpp:229] Iteration 6450, loss = 0.231739
I0814 09:40:06.730351 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0952997
I0814 09:40:06.730422 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.185557
I0814 09:40:06.730446 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11587
I0814 09:40:06.730463 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11587 (* 1 = 0.11587 loss)
I0814 09:40:06.730474 28599 sgd_solver.cpp:106] Iteration 6450, lr = 0.0001
I0814 09:40:40.164098 28599 solver.cpp:229] Iteration 6500, loss = 0.327969
I0814 09:40:40.320969 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.134114
I0814 09:40:40.321028 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.220964
I0814 09:40:40.321050 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.163984
I0814 09:40:40.321063 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.163984 (* 1 = 0.163984 loss)
I0814 09:40:40.321072 28599 sgd_solver.cpp:106] Iteration 6500, lr = 0.0001
I0814 09:41:14.313491 28599 solver.cpp:229] Iteration 6550, loss = 0.203602
I0814 09:41:14.470113 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0743978
I0814 09:41:14.470168 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.144244
I0814 09:41:14.470188 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.101801
I0814 09:41:14.470201 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.101801 (* 1 = 0.101801 loss)
I0814 09:41:14.470209 28599 sgd_solver.cpp:106] Iteration 6550, lr = 0.0001
I0814 09:41:47.865110 28599 solver.cpp:229] Iteration 6600, loss = 0.12065
I0814 09:41:48.022801 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0687975
I0814 09:41:48.022871 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.131394
I0814 09:41:48.022899 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0603252
I0814 09:41:48.022915 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0603252 (* 1 = 0.0603252 loss)
I0814 09:41:48.022927 28599 sgd_solver.cpp:106] Iteration 6600, lr = 0.0001
I0814 09:42:21.035004 28599 solver.cpp:229] Iteration 6650, loss = 0.161122
I0814 09:42:21.193332 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0680337
I0814 09:42:21.193385 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.131468
I0814 09:42:21.193403 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.080561
I0814 09:42:21.193415 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.080561 (* 1 = 0.080561 loss)
I0814 09:42:21.193424 28599 sgd_solver.cpp:106] Iteration 6650, lr = 0.0001
I0814 09:42:54.607810 28599 solver.cpp:229] Iteration 6700, loss = 0.109306
I0814 09:42:54.766643 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0599928
I0814 09:42:54.766695 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.118869
I0814 09:42:54.766715 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.054653
I0814 09:42:54.766747 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.054653 (* 1 = 0.054653 loss)
I0814 09:42:54.766758 28599 sgd_solver.cpp:106] Iteration 6700, lr = 0.0001
I0814 09:43:28.434059 28599 solver.cpp:229] Iteration 6750, loss = 0.212506
I0814 09:43:28.593178 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.120986
I0814 09:43:28.593250 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.22642
I0814 09:43:28.593272 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.106253
I0814 09:43:28.593287 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.106253 (* 1 = 0.106253 loss)
I0814 09:43:28.593297 28599 sgd_solver.cpp:106] Iteration 6750, lr = 0.0001
I0814 09:44:01.543558 28599 solver.cpp:229] Iteration 6800, loss = 0.174965
I0814 09:44:01.705036 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0885579
I0814 09:44:01.705092 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.167802
I0814 09:44:01.705111 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0874825
I0814 09:44:01.705122 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0874825 (* 1 = 0.0874825 loss)
I0814 09:44:01.705135 28599 sgd_solver.cpp:106] Iteration 6800, lr = 0.0001
I0814 09:44:34.950844 28599 solver.cpp:229] Iteration 6850, loss = 0.208807
I0814 09:44:35.103281 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.110061
I0814 09:44:35.103349 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.203526
I0814 09:44:35.103376 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.104403
I0814 09:44:35.103394 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.104403 (* 1 = 0.104403 loss)
I0814 09:44:35.103406 28599 sgd_solver.cpp:106] Iteration 6850, lr = 0.0001
I0814 09:45:08.991677 28599 solver.cpp:229] Iteration 6900, loss = 0.158852
I0814 09:45:09.147318 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.084111
I0814 09:45:09.147435 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.164099
I0814 09:45:09.147464 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0794258
I0814 09:45:09.147477 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0794258 (* 1 = 0.0794258 loss)
I0814 09:45:09.147490 28599 sgd_solver.cpp:106] Iteration 6900, lr = 0.0001
I0814 09:45:42.133641 28599 solver.cpp:229] Iteration 6950, loss = 0.249017
I0814 09:45:42.287850 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.123327
I0814 09:45:42.287914 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.226727
I0814 09:45:42.287936 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.124509
I0814 09:45:42.287950 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.124509 (* 1 = 0.124509 loss)
I0814 09:45:42.287961 28599 sgd_solver.cpp:106] Iteration 6950, lr = 0.0001
I0814 09:46:15.593753 28599 solver.cpp:229] Iteration 7000, loss = 0.156022
I0814 09:46:15.752935 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0907973
I0814 09:46:15.752996 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.172674
I0814 09:46:15.753013 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0780113
I0814 09:46:15.753051 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0780113 (* 1 = 0.0780113 loss)
I0814 09:46:15.753072 28599 sgd_solver.cpp:106] Iteration 7000, lr = 0.0001
I0814 09:46:48.988051 28599 solver.cpp:229] Iteration 7050, loss = 0.176546
I0814 09:46:49.143867 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.092144
I0814 09:46:49.143923 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.178281
I0814 09:46:49.143942 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.088273
I0814 09:46:49.143952 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.088273 (* 1 = 0.088273 loss)
I0814 09:46:49.143962 28599 sgd_solver.cpp:106] Iteration 7050, lr = 0.0001
I0814 09:47:21.603175 28599 solver.cpp:229] Iteration 7100, loss = 0.250084
I0814 09:47:21.759404 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.121398
I0814 09:47:21.759465 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.241522
I0814 09:47:21.759485 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.125042
I0814 09:47:21.759496 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.125042 (* 1 = 0.125042 loss)
I0814 09:47:21.759505 28599 sgd_solver.cpp:106] Iteration 7100, lr = 0.0001
I0814 09:47:55.722235 28599 solver.cpp:229] Iteration 7150, loss = 0.117033
I0814 09:47:55.878657 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0519101
I0814 09:47:55.878718 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0987057
I0814 09:47:55.878738 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0585165
I0814 09:47:55.878779 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0585165 (* 1 = 0.0585165 loss)
I0814 09:47:55.878803 28599 sgd_solver.cpp:106] Iteration 7150, lr = 0.0001
I0814 09:48:29.851485 28599 solver.cpp:229] Iteration 7200, loss = 0.186435
I0814 09:48:30.008471 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.108849
I0814 09:48:30.008527 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.20822
I0814 09:48:30.008544 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0932177
I0814 09:48:30.008556 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0932177 (* 1 = 0.0932177 loss)
I0814 09:48:30.008565 28599 sgd_solver.cpp:106] Iteration 7200, lr = 0.0001
I0814 09:49:03.673735 28599 solver.cpp:229] Iteration 7250, loss = 0.123265
I0814 09:49:03.829495 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0603242
I0814 09:49:03.829566 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.117362
I0814 09:49:03.829630 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0616324
I0814 09:49:03.829650 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0616324 (* 1 = 0.0616324 loss)
I0814 09:49:03.829660 28599 sgd_solver.cpp:106] Iteration 7250, lr = 0.0001
I0814 09:49:37.327553 28599 solver.cpp:229] Iteration 7300, loss = 0.142079
I0814 09:49:37.488211 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0675086
I0814 09:49:37.488277 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.127562
I0814 09:49:37.488306 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0710398
I0814 09:49:37.488319 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0710398 (* 1 = 0.0710398 loss)
I0814 09:49:37.488327 28599 sgd_solver.cpp:106] Iteration 7300, lr = 0.0001
I0814 09:50:10.111985 28599 solver.cpp:229] Iteration 7350, loss = 0.13539
I0814 09:50:10.267225 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0794305
I0814 09:50:10.267287 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.150539
I0814 09:50:10.267313 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0676951
I0814 09:50:10.267328 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0676951 (* 1 = 0.0676951 loss)
I0814 09:50:10.267339 28599 sgd_solver.cpp:106] Iteration 7350, lr = 0.0001
I0814 09:50:43.962232 28599 solver.cpp:229] Iteration 7400, loss = 0.107302
I0814 09:50:44.119228 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0560834
I0814 09:50:44.119294 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.106938
I0814 09:50:44.119318 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0536513
I0814 09:50:44.119330 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0536513 (* 1 = 0.0536513 loss)
I0814 09:50:44.119340 28599 sgd_solver.cpp:106] Iteration 7400, lr = 0.0001
I0814 09:51:16.876732 28599 solver.cpp:229] Iteration 7450, loss = 0.0971317
I0814 09:51:17.034023 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0543515
I0814 09:51:17.034082 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.104014
I0814 09:51:17.034103 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.048566
I0814 09:51:17.034116 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.048566 (* 1 = 0.048566 loss)
I0814 09:51:17.034123 28599 sgd_solver.cpp:106] Iteration 7450, lr = 0.0001
I0814 09:51:50.293788 28599 solver.cpp:229] Iteration 7500, loss = 0.0966152
I0814 09:51:50.450603 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0557485
I0814 09:51:50.450655 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.105911
I0814 09:51:50.450672 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0483077
I0814 09:51:50.450682 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0483077 (* 1 = 0.0483077 loss)
I0814 09:51:50.450690 28599 sgd_solver.cpp:106] Iteration 7500, lr = 0.0001
I0814 09:52:23.195953 28599 solver.cpp:229] Iteration 7550, loss = 0.176059
I0814 09:52:23.356315 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0794496
I0814 09:52:23.356427 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.157091
I0814 09:52:23.356467 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0880295
I0814 09:52:23.356487 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0880295 (* 1 = 0.0880295 loss)
I0814 09:52:23.356504 28599 sgd_solver.cpp:106] Iteration 7550, lr = 0.0001
I0814 09:52:56.850455 28599 solver.cpp:229] Iteration 7600, loss = 0.184013
I0814 09:52:57.005686 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.107001
I0814 09:52:57.005758 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.204467
I0814 09:52:57.005784 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0920067
I0814 09:52:57.005798 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0920067 (* 1 = 0.0920067 loss)
I0814 09:52:57.005812 28599 sgd_solver.cpp:106] Iteration 7600, lr = 0.0001
I0814 09:53:29.971206 28599 solver.cpp:229] Iteration 7650, loss = 0.178541
I0814 09:53:30.127215 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0962177
I0814 09:53:30.127274 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.186873
I0814 09:53:30.127290 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0892705
I0814 09:53:30.127302 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0892705 (* 1 = 0.0892705 loss)
I0814 09:53:30.127310 28599 sgd_solver.cpp:106] Iteration 7650, lr = 0.0001
I0814 09:54:02.998211 28599 solver.cpp:229] Iteration 7700, loss = 0.127797
I0814 09:54:03.155840 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0547017
I0814 09:54:03.155902 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.108154
I0814 09:54:03.155922 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0638986
I0814 09:54:03.155937 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0638986 (* 1 = 0.0638986 loss)
I0814 09:54:03.155947 28599 sgd_solver.cpp:106] Iteration 7700, lr = 0.0001
I0814 09:54:36.119083 28599 solver.cpp:229] Iteration 7750, loss = 0.150022
I0814 09:54:36.275864 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0722867
I0814 09:54:36.275936 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.136667
I0814 09:54:36.275967 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0750111
I0814 09:54:36.275985 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0750111 (* 1 = 0.0750111 loss)
I0814 09:54:36.275998 28599 sgd_solver.cpp:106] Iteration 7750, lr = 0.0001
I0814 09:55:09.628855 28599 solver.cpp:229] Iteration 7800, loss = 0.161462
I0814 09:55:09.783232 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0612814
I0814 09:55:09.783365 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.11563
I0814 09:55:09.783439 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0807309
I0814 09:55:09.783478 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0807309 (* 1 = 0.0807309 loss)
I0814 09:55:09.783504 28599 sgd_solver.cpp:106] Iteration 7800, lr = 0.0001
I0814 09:55:43.224383 28599 solver.cpp:229] Iteration 7850, loss = 0.219895
I0814 09:55:43.380740 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.112758
I0814 09:55:43.380806 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.211765
I0814 09:55:43.380833 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.109948
I0814 09:55:43.380851 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.109948 (* 1 = 0.109948 loss)
I0814 09:55:43.380864 28599 sgd_solver.cpp:106] Iteration 7850, lr = 0.0001
I0814 09:56:16.749387 28599 solver.cpp:229] Iteration 7900, loss = 0.296315
I0814 09:56:16.909529 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.163829
I0814 09:56:16.909600 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.312196
I0814 09:56:16.909623 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.148158
I0814 09:56:16.909638 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.148158 (* 1 = 0.148158 loss)
I0814 09:56:16.909649 28599 sgd_solver.cpp:106] Iteration 7900, lr = 0.0001
I0814 09:56:49.542726 28599 solver.cpp:229] Iteration 7950, loss = 0.141356
I0814 09:56:49.701795 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0633931
I0814 09:56:49.701849 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.123061
I0814 09:56:49.701867 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0706784
I0814 09:56:49.701877 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0706784 (* 1 = 0.0706784 loss)
I0814 09:56:49.701885 28599 sgd_solver.cpp:106] Iteration 7950, lr = 0.0001
I0814 09:57:23.231776 28599 solver.cpp:229] Iteration 8000, loss = 0.0987858
I0814 09:57:23.390895 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0505869
I0814 09:57:23.390952 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.099378
I0814 09:57:23.390970 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.049393
I0814 09:57:23.390981 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.049393 (* 1 = 0.049393 loss)
I0814 09:57:23.390990 28599 sgd_solver.cpp:106] Iteration 8000, lr = 0.0001
I0814 09:57:56.291072 28599 solver.cpp:229] Iteration 8050, loss = 0.131272
I0814 09:57:56.446478 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0691496
I0814 09:57:56.446539 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132936
I0814 09:57:56.446560 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0656362
I0814 09:57:56.446576 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0656362 (* 1 = 0.0656362 loss)
I0814 09:57:56.446586 28599 sgd_solver.cpp:106] Iteration 8050, lr = 0.0001
I0814 09:58:29.126453 28599 solver.cpp:229] Iteration 8100, loss = 0.169135
I0814 09:58:29.285802 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0940281
I0814 09:58:29.285862 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.176133
I0814 09:58:29.285884 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0845678
I0814 09:58:29.285895 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0845678 (* 1 = 0.0845678 loss)
I0814 09:58:29.285903 28599 sgd_solver.cpp:106] Iteration 8100, lr = 0.0001
I0814 09:59:03.546221 28599 solver.cpp:229] Iteration 8150, loss = 0.18765
I0814 09:59:03.706079 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0986365
I0814 09:59:03.706148 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.187234
I0814 09:59:03.706172 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.093825
I0814 09:59:03.706185 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.093825 (* 1 = 0.093825 loss)
I0814 09:59:03.706194 28599 sgd_solver.cpp:106] Iteration 8150, lr = 0.0001
I0814 09:59:37.134016 28599 solver.cpp:229] Iteration 8200, loss = 0.14416
I0814 09:59:37.290875 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0839825
I0814 09:59:37.290930 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163191
I0814 09:59:37.290949 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0720803
I0814 09:59:37.290961 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0720803 (* 1 = 0.0720803 loss)
I0814 09:59:37.290971 28599 sgd_solver.cpp:106] Iteration 8200, lr = 0.0001
I0814 10:00:10.559731 28599 solver.cpp:229] Iteration 8250, loss = 0.145179
I0814 10:00:10.718776 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.091239
I0814 10:00:10.718852 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.172622
I0814 10:00:10.718883 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0725897
I0814 10:00:10.718907 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0725897 (* 1 = 0.0725897 loss)
I0814 10:00:10.718920 28599 sgd_solver.cpp:106] Iteration 8250, lr = 0.0001
I0814 10:00:43.834273 28599 solver.cpp:229] Iteration 8300, loss = 0.110999
I0814 10:00:43.991619 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0571774
I0814 10:00:43.991678 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.107451
I0814 10:00:43.991703 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0554994
I0814 10:00:43.991716 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0554994 (* 1 = 0.0554994 loss)
I0814 10:00:43.991726 28599 sgd_solver.cpp:106] Iteration 8300, lr = 0.0001
I0814 10:01:17.226511 28599 solver.cpp:229] Iteration 8350, loss = 0.264876
I0814 10:01:17.385665 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.117118
I0814 10:01:17.385749 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.217915
I0814 10:01:17.385782 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.132438
I0814 10:01:17.385794 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.132438 (* 1 = 0.132438 loss)
I0814 10:01:17.385802 28599 sgd_solver.cpp:106] Iteration 8350, lr = 0.0001
I0814 10:01:51.112591 28599 solver.cpp:229] Iteration 8400, loss = 0.131453
I0814 10:01:51.268476 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0729925
I0814 10:01:51.268541 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.140179
I0814 10:01:51.268564 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0657265
I0814 10:01:51.268577 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0657265 (* 1 = 0.0657265 loss)
I0814 10:01:51.268590 28599 sgd_solver.cpp:106] Iteration 8400, lr = 0.0001
I0814 10:02:24.081156 28599 solver.cpp:229] Iteration 8450, loss = 0.216327
I0814 10:02:24.241250 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.132976
I0814 10:02:24.241304 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.2516
I0814 10:02:24.241322 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.108164
I0814 10:02:24.241336 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.108164 (* 1 = 0.108164 loss)
I0814 10:02:24.241344 28599 sgd_solver.cpp:106] Iteration 8450, lr = 0.0001
I0814 10:02:57.107430 28599 solver.cpp:229] Iteration 8500, loss = 0.12881
I0814 10:02:57.259212 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0661373
I0814 10:02:57.259277 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.12477
I0814 10:02:57.259300 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.064405
I0814 10:02:57.259315 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.064405 (* 1 = 0.064405 loss)
I0814 10:02:57.259323 28599 sgd_solver.cpp:106] Iteration 8500, lr = 0.0001
I0814 10:03:30.446928 28599 solver.cpp:229] Iteration 8550, loss = 0.264393
I0814 10:03:30.605752 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.119875
I0814 10:03:30.605813 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.213615
I0814 10:03:30.605834 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.132196
I0814 10:03:30.605849 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.132196 (* 1 = 0.132196 loss)
I0814 10:03:30.605859 28599 sgd_solver.cpp:106] Iteration 8550, lr = 0.0001
I0814 10:04:03.261865 28599 solver.cpp:229] Iteration 8600, loss = 0.16121
I0814 10:04:03.419808 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0895413
I0814 10:04:03.419859 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.175264
I0814 10:04:03.419875 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.080605
I0814 10:04:03.419886 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.080605 (* 1 = 0.080605 loss)
I0814 10:04:03.419893 28599 sgd_solver.cpp:106] Iteration 8600, lr = 0.0001
I0814 10:04:37.061990 28599 solver.cpp:229] Iteration 8650, loss = 0.138311
I0814 10:04:37.222296 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0707377
I0814 10:04:37.222363 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.136968
I0814 10:04:37.222389 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0691557
I0814 10:04:37.222404 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0691557 (* 1 = 0.0691557 loss)
I0814 10:04:37.222414 28599 sgd_solver.cpp:106] Iteration 8650, lr = 0.0001
I0814 10:05:10.039050 28599 solver.cpp:229] Iteration 8700, loss = 0.145461
I0814 10:05:10.198498 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0747073
I0814 10:05:10.198555 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.128372
I0814 10:05:10.198606 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0727305
I0814 10:05:10.198635 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0727305 (* 1 = 0.0727305 loss)
I0814 10:05:10.198654 28599 sgd_solver.cpp:106] Iteration 8700, lr = 0.0001
I0814 10:05:43.237206 28599 solver.cpp:229] Iteration 8750, loss = 0.130622
I0814 10:05:43.391592 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0680622
I0814 10:05:43.391661 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.128535
I0814 10:05:43.391685 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0653112
I0814 10:05:43.391732 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0653112 (* 1 = 0.0653112 loss)
I0814 10:05:43.391746 28599 sgd_solver.cpp:106] Iteration 8750, lr = 0.0001
I0814 10:06:16.479317 28599 solver.cpp:229] Iteration 8800, loss = 0.216317
I0814 10:06:16.639180 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.118541
I0814 10:06:16.639243 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.214809
I0814 10:06:16.639266 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.108159
I0814 10:06:16.639277 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.108159 (* 1 = 0.108159 loss)
I0814 10:06:16.639286 28599 sgd_solver.cpp:106] Iteration 8800, lr = 0.0001
I0814 10:06:49.809950 28599 solver.cpp:229] Iteration 8850, loss = 0.086598
I0814 10:06:49.964026 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0359577
I0814 10:06:49.964155 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0695286
I0814 10:06:49.964215 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0432991
I0814 10:06:49.964253 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0432991 (* 1 = 0.0432991 loss)
I0814 10:06:49.964282 28599 sgd_solver.cpp:106] Iteration 8850, lr = 0.0001
I0814 10:07:23.665868 28599 solver.cpp:229] Iteration 8900, loss = 0.176569
I0814 10:07:23.824671 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0918585
I0814 10:07:23.824723 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.179982
I0814 10:07:23.824741 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0882846
I0814 10:07:23.824753 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0882846 (* 1 = 0.0882846 loss)
I0814 10:07:23.824761 28599 sgd_solver.cpp:106] Iteration 8900, lr = 0.0001
I0814 10:07:56.691720 28599 solver.cpp:229] Iteration 8950, loss = 0.182033
I0814 10:07:56.847682 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0888694
I0814 10:07:56.847738 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.167406
I0814 10:07:56.847757 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0910164
I0814 10:07:56.847769 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0910164 (* 1 = 0.0910164 loss)
I0814 10:07:56.847775 28599 sgd_solver.cpp:106] Iteration 8950, lr = 0.0001
I0814 10:08:30.592195 28599 solver.cpp:229] Iteration 9000, loss = 0.220945
I0814 10:08:30.747998 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.120082
I0814 10:08:30.748070 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.216248
I0814 10:08:30.748091 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.110473
I0814 10:08:30.748129 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.110473 (* 1 = 0.110473 loss)
I0814 10:08:30.748149 28599 sgd_solver.cpp:106] Iteration 9000, lr = 0.0001
I0814 10:09:03.832176 28599 solver.cpp:229] Iteration 9050, loss = 0.302375
I0814 10:09:03.991504 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.140356
I0814 10:09:03.991562 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.256965
I0814 10:09:03.991582 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.151188
I0814 10:09:03.991593 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.151188 (* 1 = 0.151188 loss)
I0814 10:09:03.991603 28599 sgd_solver.cpp:106] Iteration 9050, lr = 0.0001
I0814 10:09:37.955982 28599 solver.cpp:229] Iteration 9100, loss = 0.131289
I0814 10:09:38.112821 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.06531
I0814 10:09:38.112956 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.12352
I0814 10:09:38.113008 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0656447
I0814 10:09:38.113044 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0656447 (* 1 = 0.0656447 loss)
I0814 10:09:38.113070 28599 sgd_solver.cpp:106] Iteration 9100, lr = 0.0001
I0814 10:10:13.394071 28599 solver.cpp:229] Iteration 9150, loss = 0.162233
I0814 10:10:13.551903 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0756028
I0814 10:10:13.551967 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.140207
I0814 10:10:13.551986 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0811165
I0814 10:10:13.551996 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0811165 (* 1 = 0.0811165 loss)
I0814 10:10:13.552003 28599 sgd_solver.cpp:106] Iteration 9150, lr = 0.0001
I0814 10:10:47.133277 28599 solver.cpp:229] Iteration 9200, loss = 0.118983
I0814 10:10:47.290555 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0590463
I0814 10:10:47.290623 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.10689
I0814 10:10:47.290645 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0594917
I0814 10:10:47.290663 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0594917 (* 1 = 0.0594917 loss)
I0814 10:10:47.290709 28599 sgd_solver.cpp:106] Iteration 9200, lr = 0.0001
I0814 10:11:20.797880 28599 solver.cpp:229] Iteration 9250, loss = 0.0923179
I0814 10:11:20.955605 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0480463
I0814 10:11:20.955677 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0930144
I0814 10:11:20.955698 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.046159
I0814 10:11:20.955708 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.046159 (* 1 = 0.046159 loss)
I0814 10:11:20.955716 28599 sgd_solver.cpp:106] Iteration 9250, lr = 0.0001
I0814 10:11:54.834314 28599 solver.cpp:229] Iteration 9300, loss = 0.189095
I0814 10:11:54.990828 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.105166
I0814 10:11:54.990952 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.199292
I0814 10:11:54.991039 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0945478
I0814 10:11:54.991061 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0945478 (* 1 = 0.0945478 loss)
I0814 10:11:54.991083 28599 sgd_solver.cpp:106] Iteration 9300, lr = 0.0001
I0814 10:12:27.495175 28599 solver.cpp:229] Iteration 9350, loss = 0.29728
I0814 10:12:27.654165 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.154479
I0814 10:12:27.654245 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.290566
I0814 10:12:27.654345 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.14864
I0814 10:12:27.654377 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.14864 (* 1 = 0.14864 loss)
I0814 10:12:27.654393 28599 sgd_solver.cpp:106] Iteration 9350, lr = 0.0001
I0814 10:13:01.191670 28599 solver.cpp:229] Iteration 9400, loss = 0.134673
I0814 10:13:01.348244 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0701062
I0814 10:13:01.348301 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.134432
I0814 10:13:01.348320 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0673368
I0814 10:13:01.348331 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0673368 (* 1 = 0.0673368 loss)
I0814 10:13:01.348340 28599 sgd_solver.cpp:106] Iteration 9400, lr = 0.0001
I0814 10:13:35.069303 28599 solver.cpp:229] Iteration 9450, loss = 0.141843
I0814 10:13:35.226505 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0772391
I0814 10:13:35.226570 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.147781
I0814 10:13:35.226590 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0709217
I0814 10:13:35.226604 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0709217 (* 1 = 0.0709217 loss)
I0814 10:13:35.226614 28599 sgd_solver.cpp:106] Iteration 9450, lr = 0.0001
I0814 10:14:08.886639 28599 solver.cpp:229] Iteration 9500, loss = 0.138064
I0814 10:14:09.040290 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0627148
I0814 10:14:09.040351 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.119533
I0814 10:14:09.040370 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0690319
I0814 10:14:09.040382 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0690319 (* 1 = 0.0690319 loss)
I0814 10:14:09.040391 28599 sgd_solver.cpp:106] Iteration 9500, lr = 0.0001
I0814 10:14:42.392523 28599 solver.cpp:229] Iteration 9550, loss = 0.0767736
I0814 10:14:42.551287 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0381552
I0814 10:14:42.551342 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0724895
I0814 10:14:42.551359 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0383869
I0814 10:14:42.551370 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0383869 (* 1 = 0.0383869 loss)
I0814 10:14:42.551379 28599 sgd_solver.cpp:106] Iteration 9550, lr = 0.0001
I0814 10:15:16.819624 28599 solver.cpp:229] Iteration 9600, loss = 0.194861
I0814 10:15:16.977628 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.102219
I0814 10:15:16.977689 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.19484
I0814 10:15:16.977710 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0974306
I0814 10:15:16.977720 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0974306 (* 1 = 0.0974306 loss)
I0814 10:15:16.977730 28599 sgd_solver.cpp:106] Iteration 9600, lr = 0.0001
I0814 10:15:50.041899 28599 solver.cpp:229] Iteration 9650, loss = 0.301579
I0814 10:15:50.201666 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.168922
I0814 10:15:50.201721 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.329007
I0814 10:15:50.201740 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.15079
I0814 10:15:50.201751 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.15079 (* 1 = 0.15079 loss)
I0814 10:15:50.201761 28599 sgd_solver.cpp:106] Iteration 9650, lr = 0.0001
I0814 10:16:23.674695 28599 solver.cpp:229] Iteration 9700, loss = 0.20954
I0814 10:16:23.836125 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0962408
I0814 10:16:23.836180 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.185983
I0814 10:16:23.836197 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.10477
I0814 10:16:23.836208 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.10477 (* 1 = 0.10477 loss)
I0814 10:16:23.836215 28599 sgd_solver.cpp:106] Iteration 9700, lr = 0.0001
I0814 10:16:56.532899 28599 solver.cpp:229] Iteration 9750, loss = 0.104551
I0814 10:16:56.692107 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0567951
I0814 10:16:56.692169 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.112107
I0814 10:16:56.692189 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0522758
I0814 10:16:56.692201 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0522758 (* 1 = 0.0522758 loss)
I0814 10:16:56.692212 28599 sgd_solver.cpp:106] Iteration 9750, lr = 0.0001
I0814 10:17:30.104531 28599 solver.cpp:229] Iteration 9800, loss = 0.158422
I0814 10:17:30.260920 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0902107
I0814 10:17:30.260995 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.168927
I0814 10:17:30.261029 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.079211
I0814 10:17:30.261047 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.079211 (* 1 = 0.079211 loss)
I0814 10:17:30.261060 28599 sgd_solver.cpp:106] Iteration 9800, lr = 0.0001
I0814 10:18:04.957739 28599 solver.cpp:229] Iteration 9850, loss = 0.102943
I0814 10:18:05.114707 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0470962
I0814 10:18:05.114848 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0924379
I0814 10:18:05.114908 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0514715
I0814 10:18:05.114941 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0514715 (* 1 = 0.0514715 loss)
I0814 10:18:05.114969 28599 sgd_solver.cpp:106] Iteration 9850, lr = 0.0001
I0814 10:18:39.923490 28599 solver.cpp:229] Iteration 9900, loss = 0.17802
I0814 10:18:40.078884 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0984523
I0814 10:18:40.078935 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.186458
I0814 10:18:40.078975 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.08901
I0814 10:18:40.078989 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.08901 (* 1 = 0.08901 loss)
I0814 10:18:40.079007 28599 sgd_solver.cpp:106] Iteration 9900, lr = 0.0001
I0814 10:19:13.399778 28599 solver.cpp:229] Iteration 9950, loss = 0.175138
I0814 10:19:13.556735 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.102974
I0814 10:19:13.556792 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.195741
I0814 10:19:13.556810 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0875691
I0814 10:19:13.556820 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0875691 (* 1 = 0.0875691 loss)
I0814 10:19:13.556829 28599 sgd_solver.cpp:106] Iteration 9950, lr = 0.0001
I0814 10:19:47.382010 28599 solver.cpp:229] Iteration 10000, loss = 0.181449
I0814 10:19:47.541357 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.10135
I0814 10:19:47.541422 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.190903
I0814 10:19:47.541447 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0907246
I0814 10:19:47.541460 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0907246 (* 1 = 0.0907246 loss)
I0814 10:19:47.541471 28599 sgd_solver.cpp:106] Iteration 10000, lr = 0.0001
I0814 10:20:21.229219 28599 solver.cpp:229] Iteration 10050, loss = 0.22883
I0814 10:20:21.389464 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.131651
I0814 10:20:21.389523 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.254933
I0814 10:20:21.389541 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.114415
I0814 10:20:21.389555 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.114415 (* 1 = 0.114415 loss)
I0814 10:20:21.389564 28599 sgd_solver.cpp:106] Iteration 10050, lr = 0.0001
I0814 10:20:54.729176 28599 solver.cpp:229] Iteration 10100, loss = 0.29524
I0814 10:20:54.890141 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0979638
I0814 10:20:54.890213 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.18899
I0814 10:20:54.890236 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.14762
I0814 10:20:54.890254 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.14762 (* 1 = 0.14762 loss)
I0814 10:20:54.890265 28599 sgd_solver.cpp:106] Iteration 10100, lr = 0.0001
I0814 10:21:28.217831 28599 solver.cpp:229] Iteration 10150, loss = 0.184347
I0814 10:21:28.378175 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0929816
I0814 10:21:28.378242 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.178216
I0814 10:21:28.378264 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0921733
I0814 10:21:28.378278 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0921733 (* 1 = 0.0921733 loss)
I0814 10:21:28.378289 28599 sgd_solver.cpp:106] Iteration 10150, lr = 0.0001
I0814 10:22:01.479430 28599 solver.cpp:229] Iteration 10200, loss = 0.123514
I0814 10:22:01.637230 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0732627
I0814 10:22:01.637290 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.141218
I0814 10:22:01.637310 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0617569
I0814 10:22:01.637322 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0617569 (* 1 = 0.0617569 loss)
I0814 10:22:01.637331 28599 sgd_solver.cpp:106] Iteration 10200, lr = 0.0001
I0814 10:22:34.771034 28599 solver.cpp:229] Iteration 10250, loss = 0.166807
I0814 10:22:34.928027 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0959685
I0814 10:22:34.928088 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.18
I0814 10:22:34.928109 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0834036
I0814 10:22:34.928122 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0834036 (* 1 = 0.0834036 loss)
I0814 10:22:34.928130 28599 sgd_solver.cpp:106] Iteration 10250, lr = 0.0001
I0814 10:23:08.626456 28599 solver.cpp:229] Iteration 10300, loss = 0.202105
I0814 10:23:08.784760 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.100416
I0814 10:23:08.784816 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.195747
I0814 10:23:08.784837 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.101052
I0814 10:23:08.784848 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.101052 (* 1 = 0.101052 loss)
I0814 10:23:08.784855 28599 sgd_solver.cpp:106] Iteration 10300, lr = 0.0001
I0814 10:23:41.942746 28599 solver.cpp:229] Iteration 10350, loss = 0.183394
I0814 10:23:42.101877 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0975351
I0814 10:23:42.101936 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.177237
I0814 10:23:42.101958 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.091697
I0814 10:23:42.101969 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.091697 (* 1 = 0.091697 loss)
I0814 10:23:42.101979 28599 sgd_solver.cpp:106] Iteration 10350, lr = 0.0001
I0814 10:24:15.521446 28599 solver.cpp:229] Iteration 10400, loss = 0.286073
I0814 10:24:15.678016 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.151801
I0814 10:24:15.678122 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.290357
I0814 10:24:15.678216 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.143036
I0814 10:24:15.678269 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.143036 (* 1 = 0.143036 loss)
I0814 10:24:15.678287 28599 sgd_solver.cpp:106] Iteration 10400, lr = 0.0001
I0814 10:24:48.676664 28599 solver.cpp:229] Iteration 10450, loss = 0.205653
I0814 10:24:48.834436 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.106216
I0814 10:24:48.834508 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.211879
I0814 10:24:48.834535 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.102827
I0814 10:24:48.834550 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.102827 (* 1 = 0.102827 loss)
I0814 10:24:48.834563 28599 sgd_solver.cpp:106] Iteration 10450, lr = 0.0001
I0814 10:25:21.703632 28599 solver.cpp:229] Iteration 10500, loss = 0.0736207
I0814 10:25:21.860350 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0368869
I0814 10:25:21.860409 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0714664
I0814 10:25:21.860427 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0368104
I0814 10:25:21.860441 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0368104 (* 1 = 0.0368104 loss)
I0814 10:25:21.860451 28599 sgd_solver.cpp:106] Iteration 10500, lr = 0.0001
I0814 10:25:55.707991 28599 solver.cpp:229] Iteration 10550, loss = 0.128258
I0814 10:25:55.863145 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0755604
I0814 10:25:55.863199 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.140189
I0814 10:25:55.863250 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.064129
I0814 10:25:55.863265 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.064129 (* 1 = 0.064129 loss)
I0814 10:25:55.863272 28599 sgd_solver.cpp:106] Iteration 10550, lr = 0.0001
I0814 10:26:29.335680 28599 solver.cpp:229] Iteration 10600, loss = 0.190546
I0814 10:26:29.493542 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.102075
I0814 10:26:29.493645 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.193537
I0814 10:26:29.493686 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0952728
I0814 10:26:29.493775 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0952728 (* 1 = 0.0952728 loss)
I0814 10:26:29.493799 28599 sgd_solver.cpp:106] Iteration 10600, lr = 0.0001
I0814 10:27:03.565776 28599 solver.cpp:229] Iteration 10650, loss = 0.153342
I0814 10:27:03.725530 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0864068
I0814 10:27:03.725594 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.167784
I0814 10:27:03.725616 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0766709
I0814 10:27:03.725632 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0766709 (* 1 = 0.0766709 loss)
I0814 10:27:03.725643 28599 sgd_solver.cpp:106] Iteration 10650, lr = 0.0001
I0814 10:27:36.571250 28599 solver.cpp:229] Iteration 10700, loss = 0.207527
I0814 10:27:36.727421 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.119294
I0814 10:27:36.727484 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.228249
I0814 10:27:36.727504 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.103764
I0814 10:27:36.727517 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.103764 (* 1 = 0.103764 loss)
I0814 10:27:36.727526 28599 sgd_solver.cpp:106] Iteration 10700, lr = 0.0001
I0814 10:28:11.046560 28599 solver.cpp:229] Iteration 10750, loss = 0.110641
I0814 10:28:11.205379 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0653062
I0814 10:28:11.205442 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.124028
I0814 10:28:11.205463 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0553207
I0814 10:28:11.205476 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0553207 (* 1 = 0.0553207 loss)
I0814 10:28:11.205487 28599 sgd_solver.cpp:106] Iteration 10750, lr = 0.0001
I0814 10:28:44.530369 28599 solver.cpp:229] Iteration 10800, loss = 0.210424
I0814 10:28:44.690403 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.123657
I0814 10:28:44.690459 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.221783
I0814 10:28:44.690476 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.105212
I0814 10:28:44.690486 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.105212 (* 1 = 0.105212 loss)
I0814 10:28:44.690493 28599 sgd_solver.cpp:106] Iteration 10800, lr = 0.0001
I0814 10:29:19.044427 28599 solver.cpp:229] Iteration 10850, loss = 0.125665
I0814 10:29:19.203336 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0676541
I0814 10:29:19.203392 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.128343
I0814 10:29:19.203408 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0628324
I0814 10:29:19.203418 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0628324 (* 1 = 0.0628324 loss)
I0814 10:29:19.203426 28599 sgd_solver.cpp:106] Iteration 10850, lr = 0.0001
I0814 10:29:52.619827 28599 solver.cpp:229] Iteration 10900, loss = 0.101706
I0814 10:29:52.776792 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0597862
I0814 10:29:52.776856 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.110426
I0814 10:29:52.776877 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0508533
I0814 10:29:52.776888 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0508533 (* 1 = 0.0508533 loss)
I0814 10:29:52.776896 28599 sgd_solver.cpp:106] Iteration 10900, lr = 0.0001
I0814 10:30:29.711879 28599 solver.cpp:229] Iteration 10950, loss = 0.101183
I0814 10:30:29.867810 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0525675
I0814 10:30:29.867905 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.101967
I0814 10:30:29.867995 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0505915
I0814 10:30:29.868024 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0505915 (* 1 = 0.0505915 loss)
I0814 10:30:29.868037 28599 sgd_solver.cpp:106] Iteration 10950, lr = 0.0001
I0814 10:31:02.825660 28599 solver.cpp:456] Snapshotting to binary proto file norm_-0.01_iter_11000.caffemodel
I0814 10:31:02.856822 28599 sgd_solver.cpp:273] Snapshotting solver state to binary proto file norm_-0.01_iter_11000.solverstate
I0814 10:31:02.878255 28599 solver.cpp:338] Iteration 11000, Testing net (#0)
I0814 10:31:02.878304 28599 net.cpp:694] Ignoring source layer CustomData1
I0814 10:31:02.878312 28599 net.cpp:694] Ignoring source layer blob0_CustomData1_0_split
I0814 10:31:02.878316 28599 net.cpp:694] Ignoring source layer blob1_CustomData1_1_split
I0814 10:31:02.878324 28599 net.cpp:694] Ignoring source layer blob2_CustomData1_2_split
I0814 10:31:02.878340 28599 net.cpp:694] Ignoring source layer DummyData1
I0814 10:31:02.878351 28599 net.cpp:694] Ignoring source layer blob9_DummyData1_0_split
I0814 10:41:04.537276 28599 solver.cpp:406]     Test net output #0: down_up_half_loss_real = 0.0614319
I0814 10:41:04.537398 28599 solver.cpp:406]     Test net output #1: down_up_loss_real = 0.118198
I0814 10:41:04.537415 28599 solver.cpp:406]     Test net output #2: zoom_disp_loss0_real = 0.0499946
I0814 10:41:04.537422 28599 solver.cpp:406]     Test net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0499946 (* 1 = 0.0499946 loss)
I0814 10:41:04.701390 28599 solver.cpp:229] Iteration 11000, loss = 0.125772
I0814 10:41:04.866166 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0771338
I0814 10:41:04.866293 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.147022
I0814 10:41:04.866350 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.062886
I0814 10:41:04.866386 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.062886 (* 1 = 0.062886 loss)
I0814 10:41:04.866406 28599 sgd_solver.cpp:106] Iteration 11000, lr = 0.0001
I0814 10:41:37.944152 28599 solver.cpp:229] Iteration 11050, loss = 0.157948
I0814 10:41:38.106367 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0759979
I0814 10:41:38.106446 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.144811
I0814 10:41:38.106473 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0789742
I0814 10:41:38.106530 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0789742 (* 1 = 0.0789742 loss)
I0814 10:41:38.106570 28599 sgd_solver.cpp:106] Iteration 11050, lr = 0.0001
I0814 10:42:12.066462 28599 solver.cpp:229] Iteration 11100, loss = 0.135231
I0814 10:42:12.230412 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0701635
I0814 10:42:12.230478 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.13465
I0814 10:42:12.230497 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0676156
I0814 10:42:12.230509 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0676156 (* 1 = 0.0676156 loss)
I0814 10:42:12.230543 28599 sgd_solver.cpp:106] Iteration 11100, lr = 0.0001
I0814 10:42:45.402869 28599 solver.cpp:229] Iteration 11150, loss = 0.237788
I0814 10:42:45.567821 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.130981
I0814 10:42:45.568032 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.231958
I0814 10:42:45.568084 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.118894
I0814 10:42:45.568145 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.118894 (* 1 = 0.118894 loss)
I0814 10:42:45.568177 28599 sgd_solver.cpp:106] Iteration 11150, lr = 0.0001
I0814 10:43:18.749646 28599 solver.cpp:229] Iteration 11200, loss = 0.0573735
I0814 10:43:18.912994 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0265157
I0814 10:43:18.913067 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0512382
I0814 10:43:18.913096 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0286868
I0814 10:43:18.913111 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0286868 (* 1 = 0.0286868 loss)
I0814 10:43:18.913121 28599 sgd_solver.cpp:106] Iteration 11200, lr = 0.0001
I0814 10:43:52.040628 28599 solver.cpp:229] Iteration 11250, loss = 0.0939573
I0814 10:43:52.202072 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0535799
I0814 10:43:52.202128 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.103158
I0814 10:43:52.202148 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0469788
I0814 10:43:52.202158 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0469788 (* 1 = 0.0469788 loss)
I0814 10:43:52.202167 28599 sgd_solver.cpp:106] Iteration 11250, lr = 0.0001
I0814 10:44:25.706275 28599 solver.cpp:229] Iteration 11300, loss = 0.161324
I0814 10:44:25.868988 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0727606
I0814 10:44:25.869051 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.13899
I0814 10:44:25.869073 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0806619
I0814 10:44:25.869086 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0806619 (* 1 = 0.0806619 loss)
I0814 10:44:25.869093 28599 sgd_solver.cpp:106] Iteration 11300, lr = 0.0001
I0814 10:44:59.249464 28599 solver.cpp:229] Iteration 11350, loss = 0.137581
I0814 10:44:59.407486 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0694122
I0814 10:44:59.407547 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.137659
I0814 10:44:59.407570 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0687905
I0814 10:44:59.407585 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0687905 (* 1 = 0.0687905 loss)
I0814 10:44:59.407598 28599 sgd_solver.cpp:106] Iteration 11350, lr = 0.0001
I0814 10:45:32.878686 28599 solver.cpp:229] Iteration 11400, loss = 0.225254
I0814 10:45:33.038693 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0997922
I0814 10:45:33.038748 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.190545
I0814 10:45:33.038765 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.112627
I0814 10:45:33.038776 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.112627 (* 1 = 0.112627 loss)
I0814 10:45:33.038785 28599 sgd_solver.cpp:106] Iteration 11400, lr = 0.0001
I0814 10:46:05.933274 28599 solver.cpp:229] Iteration 11450, loss = 0.141106
I0814 10:46:06.092928 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0590294
I0814 10:46:06.092985 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.113623
I0814 10:46:06.093003 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0705529
I0814 10:46:06.093013 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0705529 (* 1 = 0.0705529 loss)
I0814 10:46:06.093019 28599 sgd_solver.cpp:106] Iteration 11450, lr = 0.0001
I0814 10:46:39.308321 28599 solver.cpp:229] Iteration 11500, loss = 0.211347
I0814 10:46:39.462642 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.105222
I0814 10:46:39.462714 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.181179
I0814 10:46:39.462741 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.105673
I0814 10:46:39.462759 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.105673 (* 1 = 0.105673 loss)
I0814 10:46:39.462771 28599 sgd_solver.cpp:106] Iteration 11500, lr = 0.0001
I0814 10:47:12.826956 28599 solver.cpp:229] Iteration 11550, loss = 0.153061
I0814 10:47:12.983350 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0644564
I0814 10:47:12.983489 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.123254
I0814 10:47:12.983602 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0765306
I0814 10:47:12.983659 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0765306 (* 1 = 0.0765306 loss)
I0814 10:47:12.983700 28599 sgd_solver.cpp:106] Iteration 11550, lr = 0.0001
I0814 10:47:46.744462 28599 solver.cpp:229] Iteration 11600, loss = 0.171923
I0814 10:47:46.905118 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0893493
I0814 10:47:46.905179 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.160471
I0814 10:47:46.905200 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0859617
I0814 10:47:46.905213 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0859617 (* 1 = 0.0859617 loss)
I0814 10:47:46.905221 28599 sgd_solver.cpp:106] Iteration 11600, lr = 0.0001
I0814 10:48:20.163946 28599 solver.cpp:229] Iteration 11650, loss = 0.123376
I0814 10:48:20.325083 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0748113
I0814 10:48:20.325184 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.142483
I0814 10:48:20.325207 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0616882
I0814 10:48:20.325225 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0616882 (* 1 = 0.0616882 loss)
I0814 10:48:20.325239 28599 sgd_solver.cpp:106] Iteration 11650, lr = 0.0001
I0814 10:48:53.677486 28599 solver.cpp:229] Iteration 11700, loss = 0.122932
I0814 10:48:53.838279 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0717348
I0814 10:48:53.838346 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.135705
I0814 10:48:53.838371 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0614662
I0814 10:48:53.838388 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0614662 (* 1 = 0.0614662 loss)
I0814 10:48:53.838399 28599 sgd_solver.cpp:106] Iteration 11700, lr = 0.0001
I0814 10:49:27.789611 28599 solver.cpp:229] Iteration 11750, loss = 0.1913
I0814 10:49:27.943847 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.104351
I0814 10:49:27.943953 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.196381
I0814 10:49:27.943990 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0956503
I0814 10:49:27.944015 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0956503 (* 1 = 0.0956503 loss)
I0814 10:49:27.944031 28599 sgd_solver.cpp:106] Iteration 11750, lr = 0.0001
I0814 10:50:01.368695 28599 solver.cpp:229] Iteration 11800, loss = 0.130849
I0814 10:50:01.527815 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0830406
I0814 10:50:01.527897 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.160942
I0814 10:50:01.527932 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0654248
I0814 10:50:01.527951 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0654248 (* 1 = 0.0654248 loss)
I0814 10:50:01.527966 28599 sgd_solver.cpp:106] Iteration 11800, lr = 0.0001
I0814 10:50:35.293447 28599 solver.cpp:229] Iteration 11850, loss = 0.243677
I0814 10:50:35.452245 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.117958
I0814 10:50:35.452303 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.218304
I0814 10:50:35.452322 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.121839
I0814 10:50:35.452334 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.121839 (* 1 = 0.121839 loss)
I0814 10:50:35.452347 28599 sgd_solver.cpp:106] Iteration 11850, lr = 0.0001
I0814 10:51:08.852202 28599 solver.cpp:229] Iteration 11900, loss = 0.240379
I0814 10:51:09.010872 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.140924
I0814 10:51:09.010939 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.263866
I0814 10:51:09.010962 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.12019
I0814 10:51:09.010978 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.12019 (* 1 = 0.12019 loss)
I0814 10:51:09.010988 28599 sgd_solver.cpp:106] Iteration 11900, lr = 0.0001
I0814 10:51:42.787967 28599 solver.cpp:229] Iteration 11950, loss = 0.113491
I0814 10:51:42.945124 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0658576
I0814 10:51:42.945184 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.126393
I0814 10:51:42.945205 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0567456
I0814 10:51:42.945216 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0567456 (* 1 = 0.0567456 loss)
I0814 10:51:42.945225 28599 sgd_solver.cpp:106] Iteration 11950, lr = 0.0001
I0814 10:52:17.582398 28599 solver.cpp:229] Iteration 12000, loss = 0.133191
I0814 10:52:17.741343 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0723356
I0814 10:52:17.741410 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.138634
I0814 10:52:17.741436 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0665956
I0814 10:52:17.741451 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0665956 (* 1 = 0.0665956 loss)
I0814 10:52:17.741462 28599 sgd_solver.cpp:106] Iteration 12000, lr = 0.0001
I0814 10:52:51.075711 28599 solver.cpp:229] Iteration 12050, loss = 0.319711
I0814 10:52:51.232216 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0700384
I0814 10:52:51.232278 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.138558
I0814 10:52:51.232300 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.159856
I0814 10:52:51.232316 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.159856 (* 1 = 0.159856 loss)
I0814 10:52:51.232326 28599 sgd_solver.cpp:106] Iteration 12050, lr = 0.0001
I0814 10:53:24.225682 28599 solver.cpp:229] Iteration 12100, loss = 0.151256
I0814 10:53:24.380501 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0815123
I0814 10:53:24.380626 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.155263
I0814 10:53:24.380661 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0756282
I0814 10:53:24.380676 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0756282 (* 1 = 0.0756282 loss)
I0814 10:53:24.380686 28599 sgd_solver.cpp:106] Iteration 12100, lr = 0.0001
I0814 10:53:58.078706 28599 solver.cpp:229] Iteration 12150, loss = 0.121944
I0814 10:53:58.236455 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0552673
I0814 10:53:58.236529 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.109203
I0814 10:53:58.236603 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0609719
I0814 10:53:58.236650 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0609719 (* 1 = 0.0609719 loss)
I0814 10:53:58.236680 28599 sgd_solver.cpp:106] Iteration 12150, lr = 0.0001
I0814 10:54:31.177510 28599 solver.cpp:229] Iteration 12200, loss = 0.121089
I0814 10:54:31.332496 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0683991
I0814 10:54:31.332552 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132059
I0814 10:54:31.332574 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0605447
I0814 10:54:31.332584 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0605447 (* 1 = 0.0605447 loss)
I0814 10:54:31.332592 28599 sgd_solver.cpp:106] Iteration 12200, lr = 0.0001
I0814 10:55:04.650754 28599 solver.cpp:229] Iteration 12250, loss = 0.22712
I0814 10:55:04.807252 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.123691
I0814 10:55:04.807365 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.233424
I0814 10:55:04.807421 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11356
I0814 10:55:04.807458 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11356 (* 1 = 0.11356 loss)
I0814 10:55:04.807485 28599 sgd_solver.cpp:106] Iteration 12250, lr = 0.0001
I0814 10:55:38.508386 28599 solver.cpp:229] Iteration 12300, loss = 0.164468
I0814 10:55:38.663336 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.10655
I0814 10:55:38.663396 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.19708
I0814 10:55:38.663419 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0822342
I0814 10:55:38.663429 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0822342 (* 1 = 0.0822342 loss)
I0814 10:55:38.663439 28599 sgd_solver.cpp:106] Iteration 12300, lr = 0.0001
I0814 10:56:12.203048 28599 solver.cpp:229] Iteration 12350, loss = 0.227185
I0814 10:56:12.359637 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.135568
I0814 10:56:12.359705 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.244454
I0814 10:56:12.359720 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.113593
I0814 10:56:12.359730 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.113593 (* 1 = 0.113593 loss)
I0814 10:56:12.359740 28599 sgd_solver.cpp:106] Iteration 12350, lr = 0.0001
I0814 10:56:45.567873 28599 solver.cpp:229] Iteration 12400, loss = 0.207878
I0814 10:56:45.722944 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.105567
I0814 10:56:45.723014 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.206389
I0814 10:56:45.723042 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.103939
I0814 10:56:45.723060 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.103939 (* 1 = 0.103939 loss)
I0814 10:56:45.723074 28599 sgd_solver.cpp:106] Iteration 12400, lr = 0.0001
I0814 10:57:18.357798 28599 solver.cpp:229] Iteration 12450, loss = 0.286974
I0814 10:57:18.514287 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.165881
I0814 10:57:18.514358 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.304149
I0814 10:57:18.514446 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.143487
I0814 10:57:18.514469 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.143487 (* 1 = 0.143487 loss)
I0814 10:57:18.514482 28599 sgd_solver.cpp:106] Iteration 12450, lr = 0.0001
I0814 10:57:52.400017 28599 solver.cpp:229] Iteration 12500, loss = 0.0743455
I0814 10:57:52.556043 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0435029
I0814 10:57:52.556098 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0842542
I0814 10:57:52.556118 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0371729
I0814 10:57:52.556128 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0371729 (* 1 = 0.0371729 loss)
I0814 10:57:52.556134 28599 sgd_solver.cpp:106] Iteration 12500, lr = 0.0001
I0814 10:58:26.418525 28599 solver.cpp:229] Iteration 12550, loss = 0.139997
I0814 10:58:26.579303 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0765119
I0814 10:58:26.579362 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.147474
I0814 10:58:26.579382 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0699986
I0814 10:58:26.579393 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0699986 (* 1 = 0.0699986 loss)
I0814 10:58:26.579404 28599 sgd_solver.cpp:106] Iteration 12550, lr = 0.0001
I0814 10:59:00.360258 28599 solver.cpp:229] Iteration 12600, loss = 0.207054
I0814 10:59:00.519472 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0949696
I0814 10:59:00.519527 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.185758
I0814 10:59:00.519543 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.103527
I0814 10:59:00.519556 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.103527 (* 1 = 0.103527 loss)
I0814 10:59:00.519563 28599 sgd_solver.cpp:106] Iteration 12600, lr = 0.0001
I0814 10:59:33.187849 28599 solver.cpp:229] Iteration 12650, loss = 0.145926
I0814 10:59:33.339861 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0629342
I0814 10:59:33.339938 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122655
I0814 10:59:33.339969 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.072963
I0814 10:59:33.339984 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.072963 (* 1 = 0.072963 loss)
I0814 10:59:33.339996 28599 sgd_solver.cpp:106] Iteration 12650, lr = 0.0001
I0814 11:00:06.867669 28599 solver.cpp:229] Iteration 12700, loss = 0.142679
I0814 11:00:07.023332 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0830559
I0814 11:00:07.023396 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.161731
I0814 11:00:07.023414 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0713396
I0814 11:00:07.023427 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0713396 (* 1 = 0.0713396 loss)
I0814 11:00:07.023437 28599 sgd_solver.cpp:106] Iteration 12700, lr = 0.0001
I0814 11:00:40.989071 28599 solver.cpp:229] Iteration 12750, loss = 0.123599
I0814 11:00:41.141960 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0670686
I0814 11:00:41.142022 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129052
I0814 11:00:41.142041 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0617998
I0814 11:00:41.142055 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0617998 (* 1 = 0.0617998 loss)
I0814 11:00:41.142062 28599 sgd_solver.cpp:106] Iteration 12750, lr = 0.0001
I0814 11:01:14.008599 28599 solver.cpp:229] Iteration 12800, loss = 0.116127
I0814 11:01:14.163692 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0691445
I0814 11:01:14.163821 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.131681
I0814 11:01:14.163875 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0580634
I0814 11:01:14.163911 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0580634 (* 1 = 0.0580634 loss)
I0814 11:01:14.163938 28599 sgd_solver.cpp:106] Iteration 12800, lr = 0.0001
I0814 11:01:47.754845 28599 solver.cpp:229] Iteration 12850, loss = 0.213373
I0814 11:01:47.907670 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.103641
I0814 11:01:47.907727 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.196408
I0814 11:01:47.907747 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.106687
I0814 11:01:47.907757 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.106687 (* 1 = 0.106687 loss)
I0814 11:01:47.907765 28599 sgd_solver.cpp:106] Iteration 12850, lr = 0.0001
I0814 11:02:21.812603 28599 solver.cpp:229] Iteration 12900, loss = 0.151172
I0814 11:02:21.971127 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0804007
I0814 11:02:21.971185 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.155425
I0814 11:02:21.971209 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0755861
I0814 11:02:21.971220 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0755861 (* 1 = 0.0755861 loss)
I0814 11:02:21.971230 28599 sgd_solver.cpp:106] Iteration 12900, lr = 0.0001
I0814 11:02:55.348660 28599 solver.cpp:229] Iteration 12950, loss = 0.101308
I0814 11:02:55.506765 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.055174
I0814 11:02:55.506824 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.105146
I0814 11:02:55.506846 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.050654
I0814 11:02:55.506858 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.050654 (* 1 = 0.050654 loss)
I0814 11:02:55.506867 28599 sgd_solver.cpp:106] Iteration 12950, lr = 0.0001
I0814 11:03:29.263067 28599 solver.cpp:229] Iteration 13000, loss = 0.130087
I0814 11:03:29.420852 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0769547
I0814 11:03:29.420924 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.1445
I0814 11:03:29.420950 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0650436
I0814 11:03:29.420964 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0650436 (* 1 = 0.0650436 loss)
I0814 11:03:29.420974 28599 sgd_solver.cpp:106] Iteration 13000, lr = 0.0001
I0814 11:04:02.454987 28599 solver.cpp:229] Iteration 13050, loss = 0.214617
I0814 11:04:02.609661 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.113739
I0814 11:04:02.609720 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.209681
I0814 11:04:02.609738 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.107309
I0814 11:04:02.609779 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.107309 (* 1 = 0.107309 loss)
I0814 11:04:02.609802 28599 sgd_solver.cpp:106] Iteration 13050, lr = 0.0001
I0814 11:04:35.744915 28599 solver.cpp:229] Iteration 13100, loss = 0.234311
I0814 11:04:35.903995 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.133827
I0814 11:04:35.904064 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.256309
I0814 11:04:35.904093 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.117155
I0814 11:04:35.904111 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.117155 (* 1 = 0.117155 loss)
I0814 11:04:35.904124 28599 sgd_solver.cpp:106] Iteration 13100, lr = 0.0001
I0814 11:05:09.188011 28599 solver.cpp:229] Iteration 13150, loss = 0.122901
I0814 11:05:09.342592 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0704047
I0814 11:05:09.342658 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.135141
I0814 11:05:09.342677 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0614508
I0814 11:05:09.342691 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0614508 (* 1 = 0.0614508 loss)
I0814 11:05:09.342700 28599 sgd_solver.cpp:106] Iteration 13150, lr = 0.0001
I0814 11:05:42.472611 28599 solver.cpp:229] Iteration 13200, loss = 0.0934477
I0814 11:05:42.628427 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0512618
I0814 11:05:42.628496 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.101431
I0814 11:05:42.628520 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0467239
I0814 11:05:42.628569 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0467239 (* 1 = 0.0467239 loss)
I0814 11:05:42.628603 28599 sgd_solver.cpp:106] Iteration 13200, lr = 0.0001
I0814 11:06:16.246299 28599 solver.cpp:229] Iteration 13250, loss = 0.0978157
I0814 11:06:16.401957 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0489717
I0814 11:06:16.402032 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0921263
I0814 11:06:16.402103 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0489079
I0814 11:06:16.402122 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0489079 (* 1 = 0.0489079 loss)
I0814 11:06:16.402132 28599 sgd_solver.cpp:106] Iteration 13250, lr = 0.0001
I0814 11:06:49.881069 28599 solver.cpp:229] Iteration 13300, loss = 0.213106
I0814 11:06:50.032130 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.113094
I0814 11:06:50.032196 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.214048
I0814 11:06:50.032222 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.106553
I0814 11:06:50.032238 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.106553 (* 1 = 0.106553 loss)
I0814 11:06:50.032249 28599 sgd_solver.cpp:106] Iteration 13300, lr = 0.0001
I0814 11:07:24.526836 28599 solver.cpp:229] Iteration 13350, loss = 0.172058
I0814 11:07:24.695292 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.103736
I0814 11:07:24.695376 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.200056
I0814 11:07:24.695459 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0860291
I0814 11:07:24.695487 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0860291 (* 1 = 0.0860291 loss)
I0814 11:07:24.695498 28599 sgd_solver.cpp:106] Iteration 13350, lr = 0.0001
I0814 11:07:57.613345 28599 solver.cpp:229] Iteration 13400, loss = 0.152617
I0814 11:07:57.764300 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0870159
I0814 11:07:57.764358 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.162768
I0814 11:07:57.764379 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0763087
I0814 11:07:57.764389 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0763087 (* 1 = 0.0763087 loss)
I0814 11:07:57.764397 28599 sgd_solver.cpp:106] Iteration 13400, lr = 0.0001
I0814 11:08:30.308848 28599 solver.cpp:229] Iteration 13450, loss = 0.148641
I0814 11:08:30.459684 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.085701
I0814 11:08:30.459750 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.158178
I0814 11:08:30.459774 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0743208
I0814 11:08:30.459790 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0743208 (* 1 = 0.0743208 loss)
I0814 11:08:30.459805 28599 sgd_solver.cpp:106] Iteration 13450, lr = 0.0001
I0814 11:09:03.702689 28599 solver.cpp:229] Iteration 13500, loss = 0.104485
I0814 11:09:03.854173 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0420149
I0814 11:09:03.854224 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0823227
I0814 11:09:03.854240 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0522424
I0814 11:09:03.854251 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0522424 (* 1 = 0.0522424 loss)
I0814 11:09:03.854259 28599 sgd_solver.cpp:106] Iteration 13500, lr = 0.0001
I0814 11:09:36.545032 28599 solver.cpp:229] Iteration 13550, loss = 0.140857
I0814 11:09:36.695595 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0769402
I0814 11:09:36.695667 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.148531
I0814 11:09:36.695694 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0704285
I0814 11:09:36.695713 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0704285 (* 1 = 0.0704285 loss)
I0814 11:09:36.695725 28599 sgd_solver.cpp:106] Iteration 13550, lr = 0.0001
I0814 11:10:09.725128 28599 solver.cpp:229] Iteration 13600, loss = 0.192159
I0814 11:10:09.880019 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.118679
I0814 11:10:09.880095 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.224361
I0814 11:10:09.880120 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0960796
I0814 11:10:09.880136 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0960796 (* 1 = 0.0960796 loss)
I0814 11:10:09.880147 28599 sgd_solver.cpp:106] Iteration 13600, lr = 0.0001
I0814 11:10:43.724467 28599 solver.cpp:229] Iteration 13650, loss = 0.145468
I0814 11:10:43.877225 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0913084
I0814 11:10:43.877293 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.173804
I0814 11:10:43.877317 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.072734
I0814 11:10:43.877332 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.072734 (* 1 = 0.072734 loss)
I0814 11:10:43.877341 28599 sgd_solver.cpp:106] Iteration 13650, lr = 0.0001
I0814 11:11:16.479380 28599 solver.cpp:229] Iteration 13700, loss = 0.149249
I0814 11:11:16.638214 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.054086
I0814 11:11:16.638286 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.105304
I0814 11:11:16.638315 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0746245
I0814 11:11:16.638332 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0746245 (* 1 = 0.0746245 loss)
I0814 11:11:16.638345 28599 sgd_solver.cpp:106] Iteration 13700, lr = 0.0001
I0814 11:11:49.759743 28599 solver.cpp:229] Iteration 13750, loss = 0.114833
I0814 11:11:49.911125 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0617575
I0814 11:11:49.911183 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.108007
I0814 11:11:49.911203 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0574166
I0814 11:11:49.911219 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0574166 (* 1 = 0.0574166 loss)
I0814 11:11:49.911228 28599 sgd_solver.cpp:106] Iteration 13750, lr = 0.0001
I0814 11:12:23.087493 28599 solver.cpp:229] Iteration 13800, loss = 0.150219
I0814 11:12:23.240093 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0792859
I0814 11:12:23.240160 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.147454
I0814 11:12:23.240183 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0751098
I0814 11:12:23.240196 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0751098 (* 1 = 0.0751098 loss)
I0814 11:12:23.240206 28599 sgd_solver.cpp:106] Iteration 13800, lr = 0.0001
I0814 11:12:56.445346 28599 solver.cpp:229] Iteration 13850, loss = 0.234273
I0814 11:12:56.596945 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.13614
I0814 11:12:56.597003 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.253094
I0814 11:12:56.597025 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.117136
I0814 11:12:56.597036 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.117136 (* 1 = 0.117136 loss)
I0814 11:12:56.597045 28599 sgd_solver.cpp:106] Iteration 13850, lr = 0.0001
I0814 11:13:30.439580 28599 solver.cpp:229] Iteration 13900, loss = 0.18515
I0814 11:13:30.600633 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0983839
I0814 11:13:30.600699 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.185062
I0814 11:13:30.600723 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.092575
I0814 11:13:30.600739 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.092575 (* 1 = 0.092575 loss)
I0814 11:13:30.600750 28599 sgd_solver.cpp:106] Iteration 13900, lr = 0.0001
I0814 11:14:03.417439 28599 solver.cpp:229] Iteration 13950, loss = 0.21143
I0814 11:14:03.569058 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.122313
I0814 11:14:03.569119 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.23221
I0814 11:14:03.569139 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.105715
I0814 11:14:03.569151 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.105715 (* 1 = 0.105715 loss)
I0814 11:14:03.569161 28599 sgd_solver.cpp:106] Iteration 13950, lr = 0.0001
I0814 11:14:36.499511 28599 solver.cpp:229] Iteration 14000, loss = 0.105634
I0814 11:14:36.649981 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.060261
I0814 11:14:36.650060 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.110116
I0814 11:14:36.650092 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.052817
I0814 11:14:36.650112 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.052817 (* 1 = 0.052817 loss)
I0814 11:14:36.650126 28599 sgd_solver.cpp:106] Iteration 14000, lr = 0.0001
I0814 11:15:12.324767 28599 solver.cpp:229] Iteration 14050, loss = 0.366574
I0814 11:15:12.476894 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.146536
I0814 11:15:12.476953 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.250206
I0814 11:15:12.476968 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.183287
I0814 11:15:12.476999 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.183287 (* 1 = 0.183287 loss)
I0814 11:15:12.477016 28599 sgd_solver.cpp:106] Iteration 14050, lr = 0.0001
I0814 11:15:44.850915 28599 solver.cpp:229] Iteration 14100, loss = 0.150867
I0814 11:15:45.001389 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0848429
I0814 11:15:45.001447 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.161359
I0814 11:15:45.001466 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0754338
I0814 11:15:45.001479 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0754338 (* 1 = 0.0754338 loss)
I0814 11:15:45.001489 28599 sgd_solver.cpp:106] Iteration 14100, lr = 0.0001
I0814 11:16:17.871706 28599 solver.cpp:229] Iteration 14150, loss = 0.13792
I0814 11:16:18.024955 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0804062
I0814 11:16:18.025012 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.157281
I0814 11:16:18.025028 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.06896
I0814 11:16:18.025038 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.06896 (* 1 = 0.06896 loss)
I0814 11:16:18.025046 28599 sgd_solver.cpp:106] Iteration 14150, lr = 0.0001
I0814 11:16:50.941666 28599 solver.cpp:229] Iteration 14200, loss = 0.144446
I0814 11:16:51.093914 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0872691
I0814 11:16:51.093963 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.157531
I0814 11:16:51.093977 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0722233
I0814 11:16:51.093997 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0722233 (* 1 = 0.0722233 loss)
I0814 11:16:51.094013 28599 sgd_solver.cpp:106] Iteration 14200, lr = 0.0001
I0814 11:17:24.269950 28599 solver.cpp:229] Iteration 14250, loss = 0.11441
I0814 11:17:24.422617 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0615822
I0814 11:17:24.422677 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.119455
I0814 11:17:24.422699 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0572053
I0814 11:17:24.422709 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0572053 (* 1 = 0.0572053 loss)
I0814 11:17:24.422719 28599 sgd_solver.cpp:106] Iteration 14250, lr = 0.0001
I0814 11:17:57.692848 28599 solver.cpp:229] Iteration 14300, loss = 0.15079
I0814 11:17:57.854586 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0737165
I0814 11:17:57.854641 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.133848
I0814 11:17:57.854658 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0753949
I0814 11:17:57.854670 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0753949 (* 1 = 0.0753949 loss)
I0814 11:17:57.854678 28599 sgd_solver.cpp:106] Iteration 14300, lr = 0.0001
I0814 11:18:31.720384 28599 solver.cpp:229] Iteration 14350, loss = 0.0562282
I0814 11:18:31.882730 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0331715
I0814 11:18:31.882792 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0645693
I0814 11:18:31.882814 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0281142
I0814 11:18:31.882827 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0281142 (* 1 = 0.0281142 loss)
I0814 11:18:31.882834 28599 sgd_solver.cpp:106] Iteration 14350, lr = 0.0001
I0814 11:19:05.719060 28599 solver.cpp:229] Iteration 14400, loss = 0.194326
I0814 11:19:05.880910 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0926912
I0814 11:19:05.880971 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.182849
I0814 11:19:05.880996 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0971633
I0814 11:19:05.881007 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0971633 (* 1 = 0.0971633 loss)
I0814 11:19:05.881016 28599 sgd_solver.cpp:106] Iteration 14400, lr = 0.0001
I0814 11:19:38.845471 28599 solver.cpp:229] Iteration 14450, loss = 0.13203
I0814 11:19:38.997009 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0705892
I0814 11:19:38.997076 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.130602
I0814 11:19:38.997102 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0660151
I0814 11:19:38.997115 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0660151 (* 1 = 0.0660151 loss)
I0814 11:19:38.997125 28599 sgd_solver.cpp:106] Iteration 14450, lr = 0.0001
I0814 11:20:11.504416 28599 solver.cpp:229] Iteration 14500, loss = 0.209331
I0814 11:20:11.656116 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.110024
I0814 11:20:11.656180 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.205287
I0814 11:20:11.656203 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.104666
I0814 11:20:11.656219 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.104666 (* 1 = 0.104666 loss)
I0814 11:20:11.656230 28599 sgd_solver.cpp:106] Iteration 14500, lr = 0.0001
I0814 11:20:46.838948 28599 solver.cpp:229] Iteration 14550, loss = 0.224682
I0814 11:20:47.001464 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.148635
I0814 11:20:47.001543 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.282092
I0814 11:20:47.001611 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.112341
I0814 11:20:47.001658 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.112341 (* 1 = 0.112341 loss)
I0814 11:20:47.001688 28599 sgd_solver.cpp:106] Iteration 14550, lr = 0.0001
I0814 11:21:21.569136 28599 solver.cpp:229] Iteration 14600, loss = 0.143063
I0814 11:21:21.737036 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0735249
I0814 11:21:21.737123 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139873
I0814 11:21:21.737160 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0715317
I0814 11:21:21.737181 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0715317 (* 1 = 0.0715317 loss)
I0814 11:21:21.737196 28599 sgd_solver.cpp:106] Iteration 14600, lr = 0.0001
I0814 11:21:56.549631 28599 solver.cpp:229] Iteration 14650, loss = 0.111575
I0814 11:21:56.700244 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0669304
I0814 11:21:56.700320 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.121878
I0814 11:21:56.700350 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0557874
I0814 11:21:56.700369 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0557874 (* 1 = 0.0557874 loss)
I0814 11:21:56.700382 28599 sgd_solver.cpp:106] Iteration 14650, lr = 0.0001
I0814 11:22:30.021735 28599 solver.cpp:229] Iteration 14700, loss = 0.0716341
I0814 11:22:30.172871 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0380758
I0814 11:22:30.172922 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0768159
I0814 11:22:30.172950 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0358172
I0814 11:22:30.172960 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0358172 (* 1 = 0.0358172 loss)
I0814 11:22:30.172967 28599 sgd_solver.cpp:106] Iteration 14700, lr = 0.0001
I0814 11:23:03.361080 28599 solver.cpp:229] Iteration 14750, loss = 0.111816
I0814 11:23:03.525198 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0674795
I0814 11:23:03.525269 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.127727
I0814 11:23:03.525290 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0559083
I0814 11:23:03.525305 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0559083 (* 1 = 0.0559083 loss)
I0814 11:23:03.525315 28599 sgd_solver.cpp:106] Iteration 14750, lr = 0.0001
I0814 11:23:36.425467 28599 solver.cpp:229] Iteration 14800, loss = 0.131445
I0814 11:23:36.576347 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0726208
I0814 11:23:36.576412 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139039
I0814 11:23:36.576434 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0657224
I0814 11:23:36.576449 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0657224 (* 1 = 0.0657224 loss)
I0814 11:23:36.576459 28599 sgd_solver.cpp:106] Iteration 14800, lr = 0.0001
I0814 11:24:09.763389 28599 solver.cpp:229] Iteration 14850, loss = 0.17624
I0814 11:24:09.918496 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.101245
I0814 11:24:09.918556 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.182701
I0814 11:24:09.918576 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0881201
I0814 11:24:09.918588 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0881201 (* 1 = 0.0881201 loss)
I0814 11:24:09.918597 28599 sgd_solver.cpp:106] Iteration 14850, lr = 0.0001
I0814 11:24:43.275660 28599 solver.cpp:229] Iteration 14900, loss = 0.164146
I0814 11:24:43.427225 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.090299
I0814 11:24:43.427284 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.170557
I0814 11:24:43.427301 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.082073
I0814 11:24:43.427311 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.082073 (* 1 = 0.082073 loss)
I0814 11:24:43.427323 28599 sgd_solver.cpp:106] Iteration 14900, lr = 0.0001
I0814 11:25:19.129264 28599 solver.cpp:229] Iteration 14950, loss = 0.0946673
I0814 11:25:19.278843 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0598161
I0814 11:25:19.278991 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.115549
I0814 11:25:19.279134 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0473337
I0814 11:25:19.279181 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0473337 (* 1 = 0.0473337 loss)
I0814 11:25:19.279253 28599 sgd_solver.cpp:106] Iteration 14950, lr = 0.0001
I0814 11:25:54.203362 28599 solver.cpp:229] Iteration 15000, loss = 0.144503
I0814 11:25:54.362685 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0649107
I0814 11:25:54.362751 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.121782
I0814 11:25:54.362774 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0722515
I0814 11:25:54.362788 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0722515 (* 1 = 0.0722515 loss)
I0814 11:25:54.362800 28599 sgd_solver.cpp:106] Iteration 15000, lr = 0.0001
I0814 11:26:27.474493 28599 solver.cpp:229] Iteration 15050, loss = 0.0885966
I0814 11:26:27.626354 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0484595
I0814 11:26:27.626407 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0915973
I0814 11:26:27.626422 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0442984
I0814 11:26:27.626432 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0442984 (* 1 = 0.0442984 loss)
I0814 11:26:27.626441 28599 sgd_solver.cpp:106] Iteration 15050, lr = 0.0001
I0814 11:27:00.701550 28599 solver.cpp:229] Iteration 15100, loss = 0.109777
I0814 11:27:00.853399 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0617657
I0814 11:27:00.853452 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.120408
I0814 11:27:00.853469 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0548884
I0814 11:27:00.853478 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0548884 (* 1 = 0.0548884 loss)
I0814 11:27:00.853487 28599 sgd_solver.cpp:106] Iteration 15100, lr = 0.0001
I0814 11:27:34.548887 28599 solver.cpp:229] Iteration 15150, loss = 0.226799
I0814 11:27:34.700467 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.141282
I0814 11:27:34.700529 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.262906
I0814 11:27:34.700554 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.113399
I0814 11:27:34.700568 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.113399 (* 1 = 0.113399 loss)
I0814 11:27:34.700578 28599 sgd_solver.cpp:106] Iteration 15150, lr = 0.0001
I0814 11:28:07.997025 28599 solver.cpp:229] Iteration 15200, loss = 0.213508
I0814 11:28:08.147810 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.105425
I0814 11:28:08.147861 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.201588
I0814 11:28:08.147889 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.106754
I0814 11:28:08.147899 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.106754 (* 1 = 0.106754 loss)
I0814 11:28:08.147907 28599 sgd_solver.cpp:106] Iteration 15200, lr = 0.0001
I0814 11:28:41.463933 28599 solver.cpp:229] Iteration 15250, loss = 0.107035
I0814 11:28:41.615226 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0586716
I0814 11:28:41.615289 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.113538
I0814 11:28:41.615311 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0535176
I0814 11:28:41.615324 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0535176 (* 1 = 0.0535176 loss)
I0814 11:28:41.615334 28599 sgd_solver.cpp:106] Iteration 15250, lr = 0.0001
I0814 11:29:15.493882 28599 solver.cpp:229] Iteration 15300, loss = 0.169458
I0814 11:29:15.644953 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0917464
I0814 11:29:15.645017 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.172724
I0814 11:29:15.645042 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0847288
I0814 11:29:15.645056 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0847288 (* 1 = 0.0847288 loss)
I0814 11:29:15.645066 28599 sgd_solver.cpp:106] Iteration 15300, lr = 0.0001
I0814 11:29:49.715198 28599 solver.cpp:229] Iteration 15350, loss = 0.11071
I0814 11:29:49.865465 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0577836
I0814 11:29:49.865533 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.112533
I0814 11:29:49.865558 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0553552
I0814 11:29:49.865573 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0553552 (* 1 = 0.0553552 loss)
I0814 11:29:49.865583 28599 sgd_solver.cpp:106] Iteration 15350, lr = 0.0001
I0814 11:30:23.630041 28599 solver.cpp:229] Iteration 15400, loss = 0.180218
I0814 11:30:23.781461 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.108513
I0814 11:30:23.781529 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.204176
I0814 11:30:23.781553 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0901093
I0814 11:30:23.781569 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0901093 (* 1 = 0.0901093 loss)
I0814 11:30:23.781579 28599 sgd_solver.cpp:106] Iteration 15400, lr = 0.0001
I0814 11:30:57.256172 28599 solver.cpp:229] Iteration 15450, loss = 0.157234
I0814 11:30:57.407363 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0906842
I0814 11:30:57.407429 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.170622
I0814 11:30:57.407451 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0786171
I0814 11:30:57.407466 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0786171 (* 1 = 0.0786171 loss)
I0814 11:30:57.407480 28599 sgd_solver.cpp:106] Iteration 15450, lr = 0.0001
I0814 11:31:31.594044 28599 solver.cpp:229] Iteration 15500, loss = 0.169824
I0814 11:31:31.744961 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0698446
I0814 11:31:31.745028 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132821
I0814 11:31:31.745048 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0849122
I0814 11:31:31.745061 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0849122 (* 1 = 0.0849122 loss)
I0814 11:31:31.745072 28599 sgd_solver.cpp:106] Iteration 15500, lr = 0.0001
I0814 11:32:05.398568 28599 solver.cpp:229] Iteration 15550, loss = 0.291693
I0814 11:32:05.549461 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.161955
I0814 11:32:05.549526 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.313679
I0814 11:32:05.549549 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.145847
I0814 11:32:05.549563 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.145847 (* 1 = 0.145847 loss)
I0814 11:32:05.549573 28599 sgd_solver.cpp:106] Iteration 15550, lr = 0.0001
I0814 11:32:38.421058 28599 solver.cpp:229] Iteration 15600, loss = 0.215987
I0814 11:32:38.573230 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.124117
I0814 11:32:38.573279 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.240047
I0814 11:32:38.573304 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.107994
I0814 11:32:38.573313 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.107994 (* 1 = 0.107994 loss)
I0814 11:32:38.573324 28599 sgd_solver.cpp:106] Iteration 15600, lr = 0.0001
I0814 11:33:11.477620 28599 solver.cpp:229] Iteration 15650, loss = 0.158402
I0814 11:33:11.629596 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0707668
I0814 11:33:11.629647 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.135742
I0814 11:33:11.629665 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0792012
I0814 11:33:11.629676 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0792012 (* 1 = 0.0792012 loss)
I0814 11:33:11.629684 28599 sgd_solver.cpp:106] Iteration 15650, lr = 0.0001
I0814 11:33:44.700325 28599 solver.cpp:229] Iteration 15700, loss = 0.142604
I0814 11:33:44.851544 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.090147
I0814 11:33:44.851610 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.173157
I0814 11:33:44.851632 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0713021
I0814 11:33:44.851647 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0713021 (* 1 = 0.0713021 loss)
I0814 11:33:44.851660 28599 sgd_solver.cpp:106] Iteration 15700, lr = 0.0001
I0814 11:34:17.994959 28599 solver.cpp:229] Iteration 15750, loss = 0.134247
I0814 11:34:18.147120 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0843763
I0814 11:34:18.147172 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.160983
I0814 11:34:18.147191 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0671235
I0814 11:34:18.147202 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0671235 (* 1 = 0.0671235 loss)
I0814 11:34:18.147209 28599 sgd_solver.cpp:106] Iteration 15750, lr = 0.0001
I0814 11:34:51.101343 28599 solver.cpp:229] Iteration 15800, loss = 0.174371
I0814 11:34:51.252313 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.106109
I0814 11:34:51.252370 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.206144
I0814 11:34:51.252388 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0871856
I0814 11:34:51.252399 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0871856 (* 1 = 0.0871856 loss)
I0814 11:34:51.252411 28599 sgd_solver.cpp:106] Iteration 15800, lr = 0.0001
I0814 11:35:24.151242 28599 solver.cpp:229] Iteration 15850, loss = 0.192173
I0814 11:35:24.300616 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.114458
I0814 11:35:24.300679 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.206768
I0814 11:35:24.300706 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0960866
I0814 11:35:24.300724 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0960866 (* 1 = 0.0960866 loss)
I0814 11:35:24.300735 28599 sgd_solver.cpp:106] Iteration 15850, lr = 0.0001
I0814 11:35:57.284680 28599 solver.cpp:229] Iteration 15900, loss = 0.255779
I0814 11:35:57.435132 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.119749
I0814 11:35:57.435201 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.235697
I0814 11:35:57.435230 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.12789
I0814 11:35:57.435245 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.12789 (* 1 = 0.12789 loss)
I0814 11:35:57.435257 28599 sgd_solver.cpp:106] Iteration 15900, lr = 0.0001
I0814 11:36:29.861167 28599 solver.cpp:229] Iteration 15950, loss = 0.155538
I0814 11:36:30.013238 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0852415
I0814 11:36:30.013293 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.16917
I0814 11:36:30.013311 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0777691
I0814 11:36:30.013322 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0777691 (* 1 = 0.0777691 loss)
I0814 11:36:30.013330 28599 sgd_solver.cpp:106] Iteration 15950, lr = 0.0001
I0814 11:37:03.769196 28599 solver.cpp:229] Iteration 16000, loss = 0.0697301
I0814 11:37:03.920688 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0377773
I0814 11:37:03.920756 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0747624
I0814 11:37:03.920781 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.034865
I0814 11:37:03.920794 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.034865 (* 1 = 0.034865 loss)
I0814 11:37:03.920804 28599 sgd_solver.cpp:106] Iteration 16000, lr = 0.0001
I0814 11:37:37.674625 28599 solver.cpp:229] Iteration 16050, loss = 0.177894
I0814 11:37:37.825803 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.105067
I0814 11:37:37.825871 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.197735
I0814 11:37:37.825897 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0889471
I0814 11:37:37.825911 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0889471 (* 1 = 0.0889471 loss)
I0814 11:37:37.825923 28599 sgd_solver.cpp:106] Iteration 16050, lr = 0.0001
I0814 11:38:11.736331 28599 solver.cpp:229] Iteration 16100, loss = 0.198445
I0814 11:38:11.886487 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.109634
I0814 11:38:11.886553 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.206518
I0814 11:38:11.886577 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0992223
I0814 11:38:11.886590 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0992223 (* 1 = 0.0992223 loss)
I0814 11:38:11.886600 28599 sgd_solver.cpp:106] Iteration 16100, lr = 0.0001
I0814 11:38:46.416448 28599 solver.cpp:229] Iteration 16150, loss = 0.197287
I0814 11:38:46.567606 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.112775
I0814 11:38:46.567678 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.221562
I0814 11:38:46.567703 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0986436
I0814 11:38:46.567718 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0986436 (* 1 = 0.0986436 loss)
I0814 11:38:46.567730 28599 sgd_solver.cpp:106] Iteration 16150, lr = 0.0001
I0814 11:39:20.657380 28599 solver.cpp:229] Iteration 16200, loss = 0.219518
I0814 11:39:20.809270 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.122157
I0814 11:39:20.809340 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.236592
I0814 11:39:20.809357 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.109759
I0814 11:39:20.809370 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.109759 (* 1 = 0.109759 loss)
I0814 11:39:20.809378 28599 sgd_solver.cpp:106] Iteration 16200, lr = 0.0001
I0814 11:39:53.025684 28599 solver.cpp:229] Iteration 16250, loss = 0.139008
I0814 11:39:53.177371 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0797522
I0814 11:39:53.177431 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.150761
I0814 11:39:53.177459 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.069504
I0814 11:39:53.177477 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.069504 (* 1 = 0.069504 loss)
I0814 11:39:53.177495 28599 sgd_solver.cpp:106] Iteration 16250, lr = 0.0001
I0814 11:40:26.080215 28599 solver.cpp:229] Iteration 16300, loss = 0.156557
I0814 11:40:26.231751 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0871589
I0814 11:40:26.231811 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.16639
I0814 11:40:26.231832 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0782784
I0814 11:40:26.231843 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0782784 (* 1 = 0.0782784 loss)
I0814 11:40:26.231853 28599 sgd_solver.cpp:106] Iteration 16300, lr = 0.0001
I0814 11:40:59.788660 28599 solver.cpp:229] Iteration 16350, loss = 0.231644
I0814 11:40:59.939227 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.130931
I0814 11:40:59.939288 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.234382
I0814 11:40:59.939307 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.115822
I0814 11:40:59.939318 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.115822 (* 1 = 0.115822 loss)
I0814 11:40:59.939327 28599 sgd_solver.cpp:106] Iteration 16350, lr = 0.0001
I0814 11:41:32.683217 28599 solver.cpp:229] Iteration 16400, loss = 0.132914
I0814 11:41:32.835108 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.053405
I0814 11:41:32.835158 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.101587
I0814 11:41:32.835186 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0664571
I0814 11:41:32.835196 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0664571 (* 1 = 0.0664571 loss)
I0814 11:41:32.835202 28599 sgd_solver.cpp:106] Iteration 16400, lr = 0.0001
I0814 11:42:05.805203 28599 solver.cpp:229] Iteration 16450, loss = 0.175139
I0814 11:42:05.958202 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.100869
I0814 11:42:05.958273 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.192978
I0814 11:42:05.958298 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0875692
I0814 11:42:05.958314 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0875692 (* 1 = 0.0875692 loss)
I0814 11:42:05.958325 28599 sgd_solver.cpp:106] Iteration 16450, lr = 0.0001
I0814 11:42:38.827082 28599 solver.cpp:456] Snapshotting to binary proto file norm_-0.01_iter_16500.caffemodel
I0814 11:42:38.869328 28599 sgd_solver.cpp:273] Snapshotting solver state to binary proto file norm_-0.01_iter_16500.solverstate
I0814 11:42:38.896633 28599 solver.cpp:338] Iteration 16500, Testing net (#0)
I0814 11:42:38.896661 28599 net.cpp:694] Ignoring source layer CustomData1
I0814 11:42:38.896667 28599 net.cpp:694] Ignoring source layer blob0_CustomData1_0_split
I0814 11:42:38.896670 28599 net.cpp:694] Ignoring source layer blob1_CustomData1_1_split
I0814 11:42:38.896674 28599 net.cpp:694] Ignoring source layer blob2_CustomData1_2_split
I0814 11:42:38.896682 28599 net.cpp:694] Ignoring source layer DummyData1
I0814 11:42:38.896684 28599 net.cpp:694] Ignoring source layer blob9_DummyData1_0_split
I0814 11:52:40.666303 28599 solver.cpp:406]     Test net output #0: down_up_half_loss_real = 0.0614319
I0814 11:52:40.666434 28599 solver.cpp:406]     Test net output #1: down_up_loss_real = 0.118198
I0814 11:52:40.666443 28599 solver.cpp:406]     Test net output #2: zoom_disp_loss0_real = 0.065765
I0814 11:52:40.666450 28599 solver.cpp:406]     Test net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.065765 (* 1 = 0.065765 loss)
I0814 11:52:40.820477 28599 solver.cpp:229] Iteration 16500, loss = 0.145184
I0814 11:52:40.976899 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0639738
I0814 11:52:40.976954 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.125126
I0814 11:52:40.976977 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.072592
I0814 11:52:40.976996 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.072592 (* 1 = 0.072592 loss)
I0814 11:52:40.977005 28599 sgd_solver.cpp:106] Iteration 16500, lr = 0.0001
I0814 11:53:13.673982 28599 solver.cpp:229] Iteration 16550, loss = 0.184708
I0814 11:53:13.826256 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.10239
I0814 11:53:13.826325 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.196247
I0814 11:53:13.826352 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0923543
I0814 11:53:13.826365 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0923543 (* 1 = 0.0923543 loss)
I0814 11:53:13.826375 28599 sgd_solver.cpp:106] Iteration 16550, lr = 0.0001
I0814 11:53:48.115878 28599 solver.cpp:229] Iteration 16600, loss = 0.0922778
I0814 11:53:48.273731 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0625817
I0814 11:53:48.273800 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.118157
I0814 11:53:48.273823 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0461389
I0814 11:53:48.273836 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0461389 (* 1 = 0.0461389 loss)
I0814 11:53:48.273847 28599 sgd_solver.cpp:106] Iteration 16600, lr = 0.0001
I0814 11:54:21.171679 28599 solver.cpp:229] Iteration 16650, loss = 0.273297
I0814 11:54:21.329561 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.132917
I0814 11:54:21.329614 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.233035
I0814 11:54:21.329632 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.136648
I0814 11:54:21.329640 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.136648 (* 1 = 0.136648 loss)
I0814 11:54:21.329648 28599 sgd_solver.cpp:106] Iteration 16650, lr = 0.0001
I0814 11:54:54.978426 28599 solver.cpp:229] Iteration 16700, loss = 0.109197
I0814 11:54:55.135952 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0494985
I0814 11:54:55.136009 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0965853
I0814 11:54:55.136027 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0545986
I0814 11:54:55.136037 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0545986 (* 1 = 0.0545986 loss)
I0814 11:54:55.136045 28599 sgd_solver.cpp:106] Iteration 16700, lr = 0.0001
I0814 11:55:28.011291 28599 solver.cpp:229] Iteration 16750, loss = 0.100145
I0814 11:55:28.167376 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0542287
I0814 11:55:28.167431 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.103038
I0814 11:55:28.167449 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0500724
I0814 11:55:28.167459 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0500724 (* 1 = 0.0500724 loss)
I0814 11:55:28.167467 28599 sgd_solver.cpp:106] Iteration 16750, lr = 0.0001
I0814 11:56:01.174154 28599 solver.cpp:229] Iteration 16800, loss = 0.113997
I0814 11:56:01.326225 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0576969
I0814 11:56:01.326284 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.109575
I0814 11:56:01.326301 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0569983
I0814 11:56:01.326313 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0569983 (* 1 = 0.0569983 loss)
I0814 11:56:01.326320 28599 sgd_solver.cpp:106] Iteration 16800, lr = 0.0001
I0814 11:56:34.744459 28599 solver.cpp:229] Iteration 16850, loss = 0.0817487
I0814 11:56:34.896872 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0478619
I0814 11:56:34.896937 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0917767
I0814 11:56:34.896958 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0408743
I0814 11:56:34.896973 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0408743 (* 1 = 0.0408743 loss)
I0814 11:56:34.896983 28599 sgd_solver.cpp:106] Iteration 16850, lr = 0.0001
I0814 11:57:08.469805 28599 solver.cpp:229] Iteration 16900, loss = 0.180702
I0814 11:57:08.621610 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.106349
I0814 11:57:08.621665 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.206086
I0814 11:57:08.621683 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.090351
I0814 11:57:08.621695 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.090351 (* 1 = 0.090351 loss)
I0814 11:57:08.621702 28599 sgd_solver.cpp:106] Iteration 16900, lr = 0.0001
I0814 11:57:41.379423 28599 solver.cpp:229] Iteration 16950, loss = 0.112201
I0814 11:57:41.533655 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0622833
I0814 11:57:41.533715 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.116387
I0814 11:57:41.533735 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0561005
I0814 11:57:41.533745 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0561005 (* 1 = 0.0561005 loss)
I0814 11:57:41.533752 28599 sgd_solver.cpp:106] Iteration 16950, lr = 0.0001
I0814 11:58:14.738422 28599 solver.cpp:229] Iteration 17000, loss = 0.0984325
I0814 11:58:14.891249 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0581674
I0814 11:58:14.891315 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.111378
I0814 11:58:14.891337 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0492162
I0814 11:58:14.891350 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0492162 (* 1 = 0.0492162 loss)
I0814 11:58:14.891366 28599 sgd_solver.cpp:106] Iteration 17000, lr = 0.0001
I0814 11:58:48.376240 28599 solver.cpp:229] Iteration 17050, loss = 0.121657
I0814 11:58:48.529417 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0693734
I0814 11:58:48.529475 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.133995
I0814 11:58:48.529492 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0608283
I0814 11:58:48.529503 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0608283 (* 1 = 0.0608283 loss)
I0814 11:58:48.529512 28599 sgd_solver.cpp:106] Iteration 17050, lr = 0.0001
I0814 11:59:21.819329 28599 solver.cpp:229] Iteration 17100, loss = 0.174565
I0814 11:59:21.970595 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0952594
I0814 11:59:21.970664 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.159327
I0814 11:59:21.970686 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0872822
I0814 11:59:21.970698 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0872822 (* 1 = 0.0872822 loss)
I0814 11:59:21.970707 28599 sgd_solver.cpp:106] Iteration 17100, lr = 0.0001
I0814 11:59:55.008770 28599 solver.cpp:229] Iteration 17150, loss = 0.13593
I0814 11:59:55.160686 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0695823
I0814 11:59:55.160748 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132732
I0814 11:59:55.160768 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0679652
I0814 11:59:55.160781 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0679652 (* 1 = 0.0679652 loss)
I0814 11:59:55.160790 28599 sgd_solver.cpp:106] Iteration 17150, lr = 0.0001
I0814 12:00:28.336899 28599 solver.cpp:229] Iteration 17200, loss = 0.137817
I0814 12:00:28.487673 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0666471
I0814 12:00:28.487740 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.126795
I0814 12:00:28.487766 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0689086
I0814 12:00:28.487782 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0689086 (* 1 = 0.0689086 loss)
I0814 12:00:28.487792 28599 sgd_solver.cpp:106] Iteration 17200, lr = 0.0001
I0814 12:01:01.513108 28599 solver.cpp:229] Iteration 17250, loss = 0.136884
I0814 12:01:01.663846 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0754208
I0814 12:01:01.663908 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.141875
I0814 12:01:01.663931 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0684419
I0814 12:01:01.663945 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0684419 (* 1 = 0.0684419 loss)
I0814 12:01:01.663956 28599 sgd_solver.cpp:106] Iteration 17250, lr = 0.0001
I0814 12:01:35.689290 28599 solver.cpp:229] Iteration 17300, loss = 0.115589
I0814 12:01:35.840731 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0792416
I0814 12:01:35.840798 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.151619
I0814 12:01:35.840821 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0577943
I0814 12:01:35.840832 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0577943 (* 1 = 0.0577943 loss)
I0814 12:01:35.840843 28599 sgd_solver.cpp:106] Iteration 17300, lr = 0.0001
I0814 12:02:10.129520 28599 solver.cpp:229] Iteration 17350, loss = 0.174239
I0814 12:02:10.278746 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0980147
I0814 12:02:10.278815 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.176471
I0814 12:02:10.278836 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0871193
I0814 12:02:10.278851 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0871193 (* 1 = 0.0871193 loss)
I0814 12:02:10.278862 28599 sgd_solver.cpp:106] Iteration 17350, lr = 0.0001
I0814 12:02:43.906499 28599 solver.cpp:229] Iteration 17400, loss = 0.213188
I0814 12:02:44.057869 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.135162
I0814 12:02:44.057937 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.259376
I0814 12:02:44.057962 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.106594
I0814 12:02:44.057976 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.106594 (* 1 = 0.106594 loss)
I0814 12:02:44.057986 28599 sgd_solver.cpp:106] Iteration 17400, lr = 0.0001
I0814 12:03:17.832566 28599 solver.cpp:229] Iteration 17450, loss = 0.183574
I0814 12:03:17.983294 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.102565
I0814 12:03:17.983381 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.193892
I0814 12:03:17.983430 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0917869
I0814 12:03:17.983460 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0917869 (* 1 = 0.0917869 loss)
I0814 12:03:17.983480 28599 sgd_solver.cpp:106] Iteration 17450, lr = 0.0001
I0814 12:03:52.425911 28599 solver.cpp:229] Iteration 17500, loss = 0.149922
I0814 12:03:52.577936 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0801422
I0814 12:03:52.578004 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.154081
I0814 12:03:52.578029 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0749608
I0814 12:03:52.578042 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0749608 (* 1 = 0.0749608 loss)
I0814 12:03:52.578053 28599 sgd_solver.cpp:106] Iteration 17500, lr = 0.0001
I0814 12:04:25.546461 28599 solver.cpp:229] Iteration 17550, loss = 0.0750854
I0814 12:04:25.698513 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0411814
I0814 12:04:25.698576 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0807489
I0814 12:04:25.698592 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0375427
I0814 12:04:25.698601 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0375427 (* 1 = 0.0375427 loss)
I0814 12:04:25.698608 28599 sgd_solver.cpp:106] Iteration 17550, lr = 0.0001
I0814 12:04:59.070124 28599 solver.cpp:229] Iteration 17600, loss = 0.154285
I0814 12:04:59.221424 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.086053
I0814 12:04:59.221485 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.164748
I0814 12:04:59.221504 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0771423
I0814 12:04:59.221516 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0771423 (* 1 = 0.0771423 loss)
I0814 12:04:59.221524 28599 sgd_solver.cpp:106] Iteration 17600, lr = 0.0001
I0814 12:05:32.331771 28599 solver.cpp:229] Iteration 17650, loss = 0.136991
I0814 12:05:32.483629 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0643143
I0814 12:05:32.483697 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.12467
I0814 12:05:32.483722 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0684954
I0814 12:05:32.483736 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0684954 (* 1 = 0.0684954 loss)
I0814 12:05:32.483745 28599 sgd_solver.cpp:106] Iteration 17650, lr = 0.0001
I0814 12:06:04.833626 28599 solver.cpp:229] Iteration 17700, loss = 0.109764
I0814 12:06:04.985985 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0614825
I0814 12:06:04.986037 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.12012
I0814 12:06:04.986053 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0548822
I0814 12:06:04.986063 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0548822 (* 1 = 0.0548822 loss)
I0814 12:06:04.986071 28599 sgd_solver.cpp:106] Iteration 17700, lr = 0.0001
I0814 12:06:38.218624 28599 solver.cpp:229] Iteration 17750, loss = 0.132776
I0814 12:06:38.370348 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0838699
I0814 12:06:38.370416 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.157577
I0814 12:06:38.370440 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0663878
I0814 12:06:38.370457 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0663878 (* 1 = 0.0663878 loss)
I0814 12:06:38.370470 28599 sgd_solver.cpp:106] Iteration 17750, lr = 0.0001
I0814 12:07:11.428871 28599 solver.cpp:229] Iteration 17800, loss = 0.118847
I0814 12:07:11.580360 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0739482
I0814 12:07:11.580418 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.142369
I0814 12:07:11.580437 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0594236
I0814 12:07:11.580451 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0594236 (* 1 = 0.0594236 loss)
I0814 12:07:11.580458 28599 sgd_solver.cpp:106] Iteration 17800, lr = 0.0001
I0814 12:07:44.295881 28599 solver.cpp:229] Iteration 17850, loss = 0.152877
I0814 12:07:44.447417 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0994427
I0814 12:07:44.447479 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.191165
I0814 12:07:44.447506 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0764384
I0814 12:07:44.447520 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0764384 (* 1 = 0.0764384 loss)
I0814 12:07:44.447531 28599 sgd_solver.cpp:106] Iteration 17850, lr = 0.0001
I0814 12:08:17.276108 28599 solver.cpp:229] Iteration 17900, loss = 0.100777
I0814 12:08:17.427633 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0625627
I0814 12:08:17.427692 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122185
I0814 12:08:17.427712 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0503886
I0814 12:08:17.427724 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0503886 (* 1 = 0.0503886 loss)
I0814 12:08:17.427733 28599 sgd_solver.cpp:106] Iteration 17900, lr = 0.0001
I0814 12:08:50.425598 28599 solver.cpp:229] Iteration 17950, loss = 0.222092
I0814 12:08:50.576823 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.123672
I0814 12:08:50.576891 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.225535
I0814 12:08:50.576913 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.111046
I0814 12:08:50.576926 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.111046 (* 1 = 0.111046 loss)
I0814 12:08:50.576936 28599 sgd_solver.cpp:106] Iteration 17950, lr = 0.0001
I0814 12:09:23.900497 28599 solver.cpp:229] Iteration 18000, loss = 0.0921481
I0814 12:09:24.051702 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0591042
I0814 12:09:24.051767 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.113284
I0814 12:09:24.051789 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.046074
I0814 12:09:24.051800 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.046074 (* 1 = 0.046074 loss)
I0814 12:09:24.051808 28599 sgd_solver.cpp:106] Iteration 18000, lr = 0.0001
I0814 12:09:56.986548 28599 solver.cpp:229] Iteration 18050, loss = 0.148757
I0814 12:09:57.137995 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0916166
I0814 12:09:57.138048 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.172694
I0814 12:09:57.138065 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0743787
I0814 12:09:57.138077 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0743787 (* 1 = 0.0743787 loss)
I0814 12:09:57.138083 28599 sgd_solver.cpp:106] Iteration 18050, lr = 0.0001
I0814 12:10:29.519697 28599 solver.cpp:229] Iteration 18100, loss = 0.214827
I0814 12:10:29.671231 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.124872
I0814 12:10:29.671285 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.238321
I0814 12:10:29.671303 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.107413
I0814 12:10:29.671314 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.107413 (* 1 = 0.107413 loss)
I0814 12:10:29.671321 28599 sgd_solver.cpp:106] Iteration 18100, lr = 0.0001
I0814 12:11:02.517163 28599 solver.cpp:229] Iteration 18150, loss = 0.192263
I0814 12:11:02.668700 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0922239
I0814 12:11:02.668757 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.171152
I0814 12:11:02.668776 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0961312
I0814 12:11:02.668787 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0961312 (* 1 = 0.0961312 loss)
I0814 12:11:02.668795 28599 sgd_solver.cpp:106] Iteration 18150, lr = 0.0001
I0814 12:11:35.522317 28599 solver.cpp:229] Iteration 18200, loss = 0.209635
I0814 12:11:35.674037 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.133302
I0814 12:11:35.674103 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.255422
I0814 12:11:35.674123 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.104817
I0814 12:11:35.674134 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.104817 (* 1 = 0.104817 loss)
I0814 12:11:35.674145 28599 sgd_solver.cpp:106] Iteration 18200, lr = 0.0001
I0814 12:12:08.470938 28599 solver.cpp:229] Iteration 18250, loss = 0.129285
I0814 12:12:08.622941 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0730509
I0814 12:12:08.622990 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.141627
I0814 12:12:08.623008 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0646423
I0814 12:12:08.623018 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0646423 (* 1 = 0.0646423 loss)
I0814 12:12:08.623025 28599 sgd_solver.cpp:106] Iteration 18250, lr = 0.0001
I0814 12:12:42.514807 28599 solver.cpp:229] Iteration 18300, loss = 0.111587
I0814 12:12:42.665099 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0644015
I0814 12:12:42.665156 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122286
I0814 12:12:42.665176 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0557932
I0814 12:12:42.665189 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0557932 (* 1 = 0.0557932 loss)
I0814 12:12:42.665199 28599 sgd_solver.cpp:106] Iteration 18300, lr = 0.0001
I0814 12:13:15.612872 28599 solver.cpp:229] Iteration 18350, loss = 0.195375
I0814 12:13:15.764155 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.115725
I0814 12:13:15.764215 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.216844
I0814 12:13:15.764236 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0976874
I0814 12:13:15.764247 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0976874 (* 1 = 0.0976874 loss)
I0814 12:13:15.764256 28599 sgd_solver.cpp:106] Iteration 18350, lr = 0.0001
I0814 12:13:48.125989 28599 solver.cpp:229] Iteration 18400, loss = 0.081922
I0814 12:13:48.278007 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0455114
I0814 12:13:48.278060 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0878248
I0814 12:13:48.278080 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0409609
I0814 12:13:48.278090 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0409609 (* 1 = 0.0409609 loss)
I0814 12:13:48.278101 28599 sgd_solver.cpp:106] Iteration 18400, lr = 0.0001
I0814 12:14:21.050076 28599 solver.cpp:229] Iteration 18450, loss = 0.124182
I0814 12:14:21.201854 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0612022
I0814 12:14:21.201906 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.117937
I0814 12:14:21.201933 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0620908
I0814 12:14:21.201946 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0620908 (* 1 = 0.0620908 loss)
I0814 12:14:21.201954 28599 sgd_solver.cpp:106] Iteration 18450, lr = 0.0001
I0814 12:14:53.720300 28599 solver.cpp:229] Iteration 18500, loss = 0.0935205
I0814 12:14:53.871942 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0565075
I0814 12:14:53.871997 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.105599
I0814 12:14:53.872015 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0467602
I0814 12:14:53.872027 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0467602 (* 1 = 0.0467602 loss)
I0814 12:14:53.872035 28599 sgd_solver.cpp:106] Iteration 18500, lr = 0.0001
I0814 12:15:27.253468 28599 solver.cpp:229] Iteration 18550, loss = 0.147395
I0814 12:15:27.404376 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0780568
I0814 12:15:27.404433 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.145894
I0814 12:15:27.404453 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0736973
I0814 12:15:27.404464 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0736973 (* 1 = 0.0736973 loss)
I0814 12:15:27.404474 28599 sgd_solver.cpp:106] Iteration 18550, lr = 0.0001
I0814 12:16:00.476310 28599 solver.cpp:229] Iteration 18600, loss = 0.23828
I0814 12:16:00.628212 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.133975
I0814 12:16:00.628268 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.252293
I0814 12:16:00.628283 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11914
I0814 12:16:00.628293 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11914 (* 1 = 0.11914 loss)
I0814 12:16:00.628304 28599 sgd_solver.cpp:106] Iteration 18600, lr = 0.0001
I0814 12:16:33.694849 28599 solver.cpp:229] Iteration 18650, loss = 0.117795
I0814 12:16:33.870893 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0719836
I0814 12:16:33.871027 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.140951
I0814 12:16:33.871145 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0588975
I0814 12:16:33.871206 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0588975 (* 1 = 0.0588975 loss)
I0814 12:16:33.871245 28599 sgd_solver.cpp:106] Iteration 18650, lr = 0.0001
I0814 12:17:07.710047 28599 solver.cpp:229] Iteration 18700, loss = 0.117039
I0814 12:17:07.861830 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0598468
I0814 12:17:07.861898 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.114888
I0814 12:17:07.861918 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0585196
I0814 12:17:07.861964 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0585196 (* 1 = 0.0585196 loss)
I0814 12:17:07.861977 28599 sgd_solver.cpp:106] Iteration 18700, lr = 0.0001
I0814 12:17:40.536438 28599 solver.cpp:229] Iteration 18750, loss = 0.0623877
I0814 12:17:40.685475 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0321889
I0814 12:17:40.685533 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0635195
I0814 12:17:40.685551 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0311938
I0814 12:17:40.685564 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0311938 (* 1 = 0.0311938 loss)
I0814 12:17:40.685575 28599 sgd_solver.cpp:106] Iteration 18750, lr = 0.0001
I0814 12:18:13.800480 28599 solver.cpp:229] Iteration 18800, loss = 0.131903
I0814 12:18:13.949622 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0687562
I0814 12:18:13.949678 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132223
I0814 12:18:13.949692 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0659517
I0814 12:18:13.949702 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0659517 (* 1 = 0.0659517 loss)
I0814 12:18:13.949710 28599 sgd_solver.cpp:106] Iteration 18800, lr = 0.0001
I0814 12:18:47.271677 28599 solver.cpp:229] Iteration 18850, loss = 0.254101
I0814 12:18:47.420825 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.147109
I0814 12:18:47.420876 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.28528
I0814 12:18:47.420892 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.12705
I0814 12:18:47.420902 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.12705 (* 1 = 0.12705 loss)
I0814 12:18:47.420909 28599 sgd_solver.cpp:106] Iteration 18850, lr = 0.0001
I0814 12:19:20.651036 28599 solver.cpp:229] Iteration 18900, loss = 0.145468
I0814 12:19:20.799305 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0895538
I0814 12:19:20.799362 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.168254
I0814 12:19:20.799381 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0727339
I0814 12:19:20.799392 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0727339 (* 1 = 0.0727339 loss)
I0814 12:19:20.799401 28599 sgd_solver.cpp:106] Iteration 18900, lr = 0.0001
I0814 12:19:53.843583 28599 solver.cpp:229] Iteration 18950, loss = 0.106386
I0814 12:19:53.992722 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0630867
I0814 12:19:53.992780 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.120869
I0814 12:19:53.992799 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0531931
I0814 12:19:53.992811 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0531931 (* 1 = 0.0531931 loss)
I0814 12:19:53.992820 28599 sgd_solver.cpp:106] Iteration 18950, lr = 0.0001
I0814 12:20:26.705375 28599 solver.cpp:229] Iteration 19000, loss = 0.118506
I0814 12:20:26.854346 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0553607
I0814 12:20:26.854406 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.107099
I0814 12:20:26.854426 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0592531
I0814 12:20:26.854468 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0592531 (* 1 = 0.0592531 loss)
I0814 12:20:26.854480 28599 sgd_solver.cpp:106] Iteration 19000, lr = 0.0001
I0814 12:20:59.901402 28599 solver.cpp:229] Iteration 19050, loss = 0.165876
I0814 12:21:00.050770 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0656639
I0814 12:21:00.050832 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.126928
I0814 12:21:00.050855 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0829377
I0814 12:21:00.050873 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0829377 (* 1 = 0.0829377 loss)
I0814 12:21:00.050884 28599 sgd_solver.cpp:106] Iteration 19050, lr = 0.0001
I0814 12:21:32.838131 28599 solver.cpp:229] Iteration 19100, loss = 0.236873
I0814 12:21:32.987527 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.147962
I0814 12:21:32.987583 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.274206
I0814 12:21:32.987601 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.118436
I0814 12:21:32.987612 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.118436 (* 1 = 0.118436 loss)
I0814 12:21:32.987622 28599 sgd_solver.cpp:106] Iteration 19100, lr = 0.0001
I0814 12:22:06.641728 28599 solver.cpp:229] Iteration 19150, loss = 0.143784
I0814 12:22:06.790181 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0901648
I0814 12:22:06.790249 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.171099
I0814 12:22:06.790271 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0718918
I0814 12:22:06.790282 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0718918 (* 1 = 0.0718918 loss)
I0814 12:22:06.790292 28599 sgd_solver.cpp:106] Iteration 19150, lr = 0.0001
I0814 12:22:39.969745 28599 solver.cpp:229] Iteration 19200, loss = 0.0974908
I0814 12:22:40.119213 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0551308
I0814 12:22:40.119277 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.106561
I0814 12:22:40.119295 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0487453
I0814 12:22:40.119307 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0487453 (* 1 = 0.0487453 loss)
I0814 12:22:40.119315 28599 sgd_solver.cpp:106] Iteration 19200, lr = 0.0001
I0814 12:23:13.429333 28599 solver.cpp:229] Iteration 19250, loss = 0.11058
I0814 12:23:13.578279 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0656702
I0814 12:23:13.578397 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.119929
I0814 12:23:13.578434 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0552901
I0814 12:23:13.578459 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0552901 (* 1 = 0.0552901 loss)
I0814 12:23:13.578478 28599 sgd_solver.cpp:106] Iteration 19250, lr = 0.0001
I0814 12:23:46.688809 28599 solver.cpp:229] Iteration 19300, loss = 0.0948516
I0814 12:23:46.837584 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0574552
I0814 12:23:46.837646 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.112504
I0814 12:23:46.837667 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0474258
I0814 12:23:46.837683 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0474258 (* 1 = 0.0474258 loss)
I0814 12:23:46.837694 28599 sgd_solver.cpp:106] Iteration 19300, lr = 0.0001
I0814 12:24:20.657449 28599 solver.cpp:229] Iteration 19350, loss = 0.284699
I0814 12:24:20.806237 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.169135
I0814 12:24:20.806294 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.316619
I0814 12:24:20.806313 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.14235
I0814 12:24:20.806327 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.14235 (* 1 = 0.14235 loss)
I0814 12:24:20.806337 28599 sgd_solver.cpp:106] Iteration 19350, lr = 0.0001
I0814 12:24:53.797065 28599 solver.cpp:229] Iteration 19400, loss = 0.162225
I0814 12:24:53.945721 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0893488
I0814 12:24:53.945789 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.172281
I0814 12:24:53.945813 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0811126
I0814 12:24:53.945832 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0811126 (* 1 = 0.0811126 loss)
I0814 12:24:53.945844 28599 sgd_solver.cpp:106] Iteration 19400, lr = 0.0001
I0814 12:25:26.924860 28599 solver.cpp:229] Iteration 19450, loss = 0.195205
I0814 12:25:27.072098 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.106219
I0814 12:25:27.072150 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.198772
I0814 12:25:27.072165 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0976027
I0814 12:25:27.072175 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0976027 (* 1 = 0.0976027 loss)
I0814 12:25:27.072183 28599 sgd_solver.cpp:106] Iteration 19450, lr = 0.0001
I0814 12:25:59.537515 28599 solver.cpp:229] Iteration 19500, loss = 0.157664
I0814 12:25:59.686959 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0634705
I0814 12:25:59.687007 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.118183
I0814 12:25:59.687021 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0788317
I0814 12:25:59.687037 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0788317 (* 1 = 0.0788317 loss)
I0814 12:25:59.687055 28599 sgd_solver.cpp:106] Iteration 19500, lr = 0.0001
I0814 12:26:32.640190 28599 solver.cpp:229] Iteration 19550, loss = 0.330193
I0814 12:26:32.788635 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.178491
I0814 12:26:32.788692 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.302238
I0814 12:26:32.788709 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.165096
I0814 12:26:32.788722 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.165096 (* 1 = 0.165096 loss)
I0814 12:26:32.788729 28599 sgd_solver.cpp:106] Iteration 19550, lr = 0.0001
I0814 12:27:05.857782 28599 solver.cpp:229] Iteration 19600, loss = 0.20357
I0814 12:27:06.007225 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0889236
I0814 12:27:06.007280 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.170655
I0814 12:27:06.007299 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.101785
I0814 12:27:06.007313 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.101785 (* 1 = 0.101785 loss)
I0814 12:27:06.007323 28599 sgd_solver.cpp:106] Iteration 19600, lr = 0.0001
I0814 12:27:38.897807 28599 solver.cpp:229] Iteration 19650, loss = 0.109435
I0814 12:27:39.046789 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0628343
I0814 12:27:39.046893 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122915
I0814 12:27:39.046918 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0547173
I0814 12:27:39.046929 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0547173 (* 1 = 0.0547173 loss)
I0814 12:27:39.046938 28599 sgd_solver.cpp:106] Iteration 19650, lr = 0.0001
I0814 12:28:12.272653 28599 solver.cpp:229] Iteration 19700, loss = 0.129338
I0814 12:28:12.421948 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0805266
I0814 12:28:12.422008 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.144817
I0814 12:28:12.422026 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0646689
I0814 12:28:12.422037 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0646689 (* 1 = 0.0646689 loss)
I0814 12:28:12.422046 28599 sgd_solver.cpp:106] Iteration 19700, lr = 0.0001
I0814 12:28:45.626631 28599 solver.cpp:229] Iteration 19750, loss = 0.24059
I0814 12:28:45.775373 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.123062
I0814 12:28:45.775447 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.21479
I0814 12:28:45.775477 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.120295
I0814 12:28:45.775494 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.120295 (* 1 = 0.120295 loss)
I0814 12:28:45.775506 28599 sgd_solver.cpp:106] Iteration 19750, lr = 0.0001
I0814 12:29:18.389482 28599 solver.cpp:229] Iteration 19800, loss = 0.175496
I0814 12:29:18.538894 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.100782
I0814 12:29:18.538955 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.180942
I0814 12:29:18.538982 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0877482
I0814 12:29:18.538993 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0877482 (* 1 = 0.0877482 loss)
I0814 12:29:18.539001 28599 sgd_solver.cpp:106] Iteration 19800, lr = 0.0001
I0814 12:29:51.591053 28599 solver.cpp:229] Iteration 19850, loss = 0.0587681
I0814 12:29:51.738276 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0299722
I0814 12:29:51.738376 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0593045
I0814 12:29:51.738415 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.029384
I0814 12:29:51.738440 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.029384 (* 1 = 0.029384 loss)
I0814 12:29:51.738459 28599 sgd_solver.cpp:106] Iteration 19850, lr = 0.0001
I0814 12:30:25.513625 28599 solver.cpp:229] Iteration 19900, loss = 0.116597
I0814 12:30:25.661924 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0777237
I0814 12:30:25.661981 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.147599
I0814 12:30:25.662000 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0582986
I0814 12:30:25.662014 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0582986 (* 1 = 0.0582986 loss)
I0814 12:30:25.662022 28599 sgd_solver.cpp:106] Iteration 19900, lr = 0.0001
I0814 12:30:58.235226 28599 solver.cpp:229] Iteration 19950, loss = 0.149625
I0814 12:30:58.383286 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0866691
I0814 12:30:58.383359 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.162506
I0814 12:30:58.383388 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0748124
I0814 12:30:58.383445 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0748124 (* 1 = 0.0748124 loss)
I0814 12:30:58.383460 28599 sgd_solver.cpp:106] Iteration 19950, lr = 0.0001
I0814 12:31:31.108784 28599 solver.cpp:229] Iteration 20000, loss = 0.208584
I0814 12:31:31.257894 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.103674
I0814 12:31:31.257948 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.192914
I0814 12:31:31.257968 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.104292
I0814 12:31:31.257979 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.104292 (* 1 = 0.104292 loss)
I0814 12:31:31.257987 28599 sgd_solver.cpp:106] Iteration 20000, lr = 0.0001
I0814 12:32:04.243645 28599 solver.cpp:229] Iteration 20050, loss = 0.191104
I0814 12:32:04.391866 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.118322
I0814 12:32:04.391932 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.226706
I0814 12:32:04.391953 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.095552
I0814 12:32:04.391968 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.095552 (* 1 = 0.095552 loss)
I0814 12:32:04.391976 28599 sgd_solver.cpp:106] Iteration 20050, lr = 0.0001
I0814 12:32:37.620689 28599 solver.cpp:229] Iteration 20100, loss = 0.106247
I0814 12:32:37.770277 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0617619
I0814 12:32:37.770332 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.11839
I0814 12:32:37.770351 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0531237
I0814 12:32:37.770365 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0531237 (* 1 = 0.0531237 loss)
I0814 12:32:37.770376 28599 sgd_solver.cpp:106] Iteration 20100, lr = 0.0001
I0814 12:33:10.316125 28599 solver.cpp:229] Iteration 20150, loss = 0.0885042
I0814 12:33:10.464197 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0521959
I0814 12:33:10.464273 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0997909
I0814 12:33:10.464298 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0442521
I0814 12:33:10.464313 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0442521 (* 1 = 0.0442521 loss)
I0814 12:33:10.464326 28599 sgd_solver.cpp:106] Iteration 20150, lr = 0.0001
I0814 12:33:43.521039 28599 solver.cpp:229] Iteration 20200, loss = 0.158794
I0814 12:33:43.670591 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0947337
I0814 12:33:43.670642 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.182994
I0814 12:33:43.670660 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0793971
I0814 12:33:43.670670 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0793971 (* 1 = 0.0793971 loss)
I0814 12:33:43.670680 28599 sgd_solver.cpp:106] Iteration 20200, lr = 0.0001
I0814 12:34:19.901839 28599 solver.cpp:229] Iteration 20250, loss = 0.114073
I0814 12:34:20.049000 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0655921
I0814 12:34:20.049079 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.124755
I0814 12:34:20.049106 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0570364
I0814 12:34:20.049126 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0570364 (* 1 = 0.0570364 loss)
I0814 12:34:20.049140 28599 sgd_solver.cpp:106] Iteration 20250, lr = 0.0001
I0814 12:34:53.233458 28599 solver.cpp:229] Iteration 20300, loss = 0.150114
I0814 12:34:53.382895 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0856588
I0814 12:34:53.382957 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.162887
I0814 12:34:53.382983 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0750568
I0814 12:34:53.382994 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0750568 (* 1 = 0.0750568 loss)
I0814 12:34:53.383002 28599 sgd_solver.cpp:106] Iteration 20300, lr = 0.0001
I0814 12:35:26.377630 28599 solver.cpp:229] Iteration 20350, loss = 0.118864
I0814 12:35:26.526914 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.061192
I0814 12:35:26.526970 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.112897
I0814 12:35:26.526990 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0594319
I0814 12:35:26.527004 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0594319 (* 1 = 0.0594319 loss)
I0814 12:35:26.527014 28599 sgd_solver.cpp:106] Iteration 20350, lr = 0.0001
I0814 12:35:59.284384 28599 solver.cpp:229] Iteration 20400, loss = 0.134453
I0814 12:35:59.434756 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.083867
I0814 12:35:59.434831 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.158567
I0814 12:35:59.434859 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0672266
I0814 12:35:59.434875 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0672266 (* 1 = 0.0672266 loss)
I0814 12:35:59.434886 28599 sgd_solver.cpp:106] Iteration 20400, lr = 0.0001
I0814 12:36:32.445520 28599 solver.cpp:229] Iteration 20450, loss = 0.182945
I0814 12:36:32.593791 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.087208
I0814 12:36:32.593853 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.161992
I0814 12:36:32.593876 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0914723
I0814 12:36:32.593889 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0914723 (* 1 = 0.0914723 loss)
I0814 12:36:32.593899 28599 sgd_solver.cpp:106] Iteration 20450, lr = 0.0001
I0814 12:37:06.233634 28599 solver.cpp:229] Iteration 20500, loss = 0.15877
I0814 12:37:06.382715 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.084967
I0814 12:37:06.382772 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.159601
I0814 12:37:06.382791 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.079385
I0814 12:37:06.382803 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.079385 (* 1 = 0.079385 loss)
I0814 12:37:06.382815 28599 sgd_solver.cpp:106] Iteration 20500, lr = 0.0001
I0814 12:37:40.161485 28599 solver.cpp:229] Iteration 20550, loss = 0.0929025
I0814 12:37:40.310847 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.042994
I0814 12:37:40.310904 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0813114
I0814 12:37:40.310926 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0464512
I0814 12:37:40.310938 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0464512 (* 1 = 0.0464512 loss)
I0814 12:37:40.310947 28599 sgd_solver.cpp:106] Iteration 20550, lr = 0.0001
I0814 12:38:13.756724 28599 solver.cpp:229] Iteration 20600, loss = 0.139243
I0814 12:38:13.905532 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.08328
I0814 12:38:13.905589 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.157692
I0814 12:38:13.905609 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0696215
I0814 12:38:13.905623 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0696215 (* 1 = 0.0696215 loss)
I0814 12:38:13.905633 28599 sgd_solver.cpp:106] Iteration 20600, lr = 0.0001
I0814 12:38:46.860863 28599 solver.cpp:229] Iteration 20650, loss = 0.258322
I0814 12:38:47.009635 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.157942
I0814 12:38:47.009690 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.307005
I0814 12:38:47.009704 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.129161
I0814 12:38:47.009716 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.129161 (* 1 = 0.129161 loss)
I0814 12:38:47.009724 28599 sgd_solver.cpp:106] Iteration 20650, lr = 0.0001
I0814 12:39:20.251613 28599 solver.cpp:229] Iteration 20700, loss = 0.156607
I0814 12:39:20.400964 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0948906
I0814 12:39:20.401016 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.181986
I0814 12:39:20.401033 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0783034
I0814 12:39:20.401044 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0783034 (* 1 = 0.0783034 loss)
I0814 12:39:20.401053 28599 sgd_solver.cpp:106] Iteration 20700, lr = 0.0001
I0814 12:39:53.223093 28599 solver.cpp:229] Iteration 20750, loss = 0.0838389
I0814 12:39:53.372005 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0549812
I0814 12:39:53.372062 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.106477
I0814 12:39:53.372081 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0419194
I0814 12:39:53.372097 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0419194 (* 1 = 0.0419194 loss)
I0814 12:39:53.372108 28599 sgd_solver.cpp:106] Iteration 20750, lr = 0.0001
I0814 12:40:26.701555 28599 solver.cpp:229] Iteration 20800, loss = 0.259904
I0814 12:40:26.850554 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.117226
I0814 12:40:26.850615 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.220597
I0814 12:40:26.850634 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.129952
I0814 12:40:26.850648 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.129952 (* 1 = 0.129952 loss)
I0814 12:40:26.850657 28599 sgd_solver.cpp:106] Iteration 20800, lr = 0.0001
I0814 12:41:00.232836 28599 solver.cpp:229] Iteration 20850, loss = 0.137917
I0814 12:41:00.383203 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0834132
I0814 12:41:00.383270 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163393
I0814 12:41:00.383291 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0689585
I0814 12:41:00.383306 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0689585 (* 1 = 0.0689585 loss)
I0814 12:41:00.383318 28599 sgd_solver.cpp:106] Iteration 20850, lr = 0.0001
I0814 12:41:33.745878 28599 solver.cpp:229] Iteration 20900, loss = 0.149923
I0814 12:41:33.896188 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0941115
I0814 12:41:33.896242 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.179726
I0814 12:41:33.896262 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0749613
I0814 12:41:33.896275 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0749613 (* 1 = 0.0749613 loss)
I0814 12:41:33.896283 28599 sgd_solver.cpp:106] Iteration 20900, lr = 0.0001
I0814 12:42:07.113090 28599 solver.cpp:229] Iteration 20950, loss = 0.117079
I0814 12:42:07.262207 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0683482
I0814 12:42:07.262269 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129866
I0814 12:42:07.262291 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0585396
I0814 12:42:07.262306 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0585396 (* 1 = 0.0585396 loss)
I0814 12:42:07.262317 28599 sgd_solver.cpp:106] Iteration 20950, lr = 0.0001
I0814 12:42:40.066895 28599 solver.cpp:229] Iteration 21000, loss = 0.151627
I0814 12:42:40.215368 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0832474
I0814 12:42:40.215436 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.158435
I0814 12:42:40.215458 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0758134
I0814 12:42:40.215474 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0758134 (* 1 = 0.0758134 loss)
I0814 12:42:40.215486 28599 sgd_solver.cpp:106] Iteration 21000, lr = 0.0001
I0814 12:43:13.008553 28599 solver.cpp:229] Iteration 21050, loss = 0.257977
I0814 12:43:13.157335 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.150962
I0814 12:43:13.157397 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.286103
I0814 12:43:13.157418 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.128988
I0814 12:43:13.157434 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.128988 (* 1 = 0.128988 loss)
I0814 12:43:13.157446 28599 sgd_solver.cpp:106] Iteration 21050, lr = 0.0001
I0814 12:43:47.264559 28599 solver.cpp:229] Iteration 21100, loss = 0.178363
I0814 12:43:47.413615 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.101338
I0814 12:43:47.413676 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.196212
I0814 12:43:47.413694 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0891814
I0814 12:43:47.413710 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0891814 (* 1 = 0.0891814 loss)
I0814 12:43:47.413718 28599 sgd_solver.cpp:106] Iteration 21100, lr = 0.0001
I0814 12:44:21.208148 28599 solver.cpp:229] Iteration 21150, loss = 0.105243
I0814 12:44:21.357444 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0707515
I0814 12:44:21.357616 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.134372
I0814 12:44:21.357651 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0526214
I0814 12:44:21.357666 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0526214 (* 1 = 0.0526214 loss)
I0814 12:44:21.357676 28599 sgd_solver.cpp:106] Iteration 21150, lr = 0.0001
I0814 12:44:55.008818 28599 solver.cpp:229] Iteration 21200, loss = 0.189909
I0814 12:44:55.157552 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.119814
I0814 12:44:55.157660 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.232165
I0814 12:44:55.157711 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0949542
I0814 12:44:55.157745 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0949542 (* 1 = 0.0949542 loss)
I0814 12:44:55.157771 28599 sgd_solver.cpp:106] Iteration 21200, lr = 0.0001
I0814 12:45:28.764170 28599 solver.cpp:229] Iteration 21250, loss = 0.0847569
I0814 12:45:28.912935 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0475626
I0814 12:45:28.912999 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.089551
I0814 12:45:28.913019 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0423784
I0814 12:45:28.913033 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0423784 (* 1 = 0.0423784 loss)
I0814 12:45:28.913043 28599 sgd_solver.cpp:106] Iteration 21250, lr = 0.0001
I0814 12:46:03.080660 28599 solver.cpp:229] Iteration 21300, loss = 0.22759
I0814 12:46:03.229300 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.135036
I0814 12:46:03.229358 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.262812
I0814 12:46:03.229378 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.113795
I0814 12:46:03.229393 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.113795 (* 1 = 0.113795 loss)
I0814 12:46:03.229403 28599 sgd_solver.cpp:106] Iteration 21300, lr = 0.0001
I0814 12:46:37.032414 28599 solver.cpp:229] Iteration 21350, loss = 0.192037
I0814 12:46:37.181226 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.119733
I0814 12:46:37.181283 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.214164
I0814 12:46:37.181303 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0960183
I0814 12:46:37.181319 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0960183 (* 1 = 0.0960183 loss)
I0814 12:46:37.181331 28599 sgd_solver.cpp:106] Iteration 21350, lr = 0.0001
I0814 12:47:10.832039 28599 solver.cpp:229] Iteration 21400, loss = 0.26019
I0814 12:47:10.979521 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.145295
I0814 12:47:10.979585 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.280698
I0814 12:47:10.979607 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.130095
I0814 12:47:10.979624 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.130095 (* 1 = 0.130095 loss)
I0814 12:47:10.979635 28599 sgd_solver.cpp:106] Iteration 21400, lr = 0.0001
I0814 12:47:43.591213 28599 solver.cpp:229] Iteration 21450, loss = 0.18592
I0814 12:47:43.740689 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.106333
I0814 12:47:43.740741 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.207877
I0814 12:47:43.740782 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0929598
I0814 12:47:43.740792 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0929598 (* 1 = 0.0929598 loss)
I0814 12:47:43.740799 28599 sgd_solver.cpp:106] Iteration 21450, lr = 0.0001
I0814 12:48:16.529136 28599 solver.cpp:229] Iteration 21500, loss = 0.0641265
I0814 12:48:16.678432 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0353945
I0814 12:48:16.678495 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0693017
I0814 12:48:16.678517 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0320633
I0814 12:48:16.678532 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0320633 (* 1 = 0.0320633 loss)
I0814 12:48:16.678544 28599 sgd_solver.cpp:106] Iteration 21500, lr = 0.0001
I0814 12:48:49.681833 28599 solver.cpp:229] Iteration 21550, loss = 0.117436
I0814 12:48:49.831071 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0751153
I0814 12:48:49.831125 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139766
I0814 12:48:49.831146 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0587182
I0814 12:48:49.831157 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0587182 (* 1 = 0.0587182 loss)
I0814 12:48:49.831166 28599 sgd_solver.cpp:106] Iteration 21550, lr = 0.0001
I0814 12:49:23.072644 28599 solver.cpp:229] Iteration 21600, loss = 0.164611
I0814 12:49:23.220795 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.091397
I0814 12:49:23.220863 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.171889
I0814 12:49:23.220890 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0823056
I0814 12:49:23.220903 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0823056 (* 1 = 0.0823056 loss)
I0814 12:49:23.220914 28599 sgd_solver.cpp:106] Iteration 21600, lr = 0.0001
I0814 12:49:57.155246 28599 solver.cpp:229] Iteration 21650, loss = 0.121713
I0814 12:49:57.304275 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0721997
I0814 12:49:57.304330 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139439
I0814 12:49:57.304347 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0608565
I0814 12:49:57.304356 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0608565 (* 1 = 0.0608565 loss)
I0814 12:49:57.304368 28599 sgd_solver.cpp:106] Iteration 21650, lr = 0.0001
I0814 12:50:30.700454 28599 solver.cpp:229] Iteration 21700, loss = 0.291493
I0814 12:50:30.849530 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.149101
I0814 12:50:30.849589 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.285514
I0814 12:50:30.849608 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.145747
I0814 12:50:30.849619 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.145747 (* 1 = 0.145747 loss)
I0814 12:50:30.849632 28599 sgd_solver.cpp:106] Iteration 21700, lr = 0.0001
I0814 12:51:04.012979 28599 solver.cpp:229] Iteration 21750, loss = 0.107386
I0814 12:51:04.161965 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0694388
I0814 12:51:04.162036 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132349
I0814 12:51:04.162061 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.053693
I0814 12:51:04.162078 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.053693 (* 1 = 0.053693 loss)
I0814 12:51:04.162092 28599 sgd_solver.cpp:106] Iteration 21750, lr = 0.0001
I0814 12:51:37.390985 28599 solver.cpp:229] Iteration 21800, loss = 0.290215
I0814 12:51:37.539844 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.170209
I0814 12:51:37.539896 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.304138
I0814 12:51:37.539911 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.145108
I0814 12:51:37.539921 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.145108 (* 1 = 0.145108 loss)
I0814 12:51:37.539929 28599 sgd_solver.cpp:106] Iteration 21800, lr = 0.0001
I0814 12:52:10.715651 28599 solver.cpp:229] Iteration 21850, loss = 0.224592
I0814 12:52:10.864375 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.114961
I0814 12:52:10.864428 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.208304
I0814 12:52:10.864444 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.112296
I0814 12:52:10.864454 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.112296 (* 1 = 0.112296 loss)
I0814 12:52:10.864460 28599 sgd_solver.cpp:106] Iteration 21850, lr = 0.0001
I0814 12:52:44.836822 28599 solver.cpp:229] Iteration 21900, loss = 0.0972679
I0814 12:52:44.985492 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0516328
I0814 12:52:44.985555 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0956497
I0814 12:52:44.985574 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0486339
I0814 12:52:44.985589 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0486339 (* 1 = 0.0486339 loss)
I0814 12:52:44.985600 28599 sgd_solver.cpp:106] Iteration 21900, lr = 0.0001
I0814 12:53:18.101099 28599 solver.cpp:229] Iteration 21950, loss = 0.113024
I0814 12:53:18.250499 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.072453
I0814 12:53:18.250567 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139912
I0814 12:53:18.250596 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0565118
I0814 12:53:18.250612 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0565118 (* 1 = 0.0565118 loss)
I0814 12:53:18.250623 28599 sgd_solver.cpp:106] Iteration 21950, lr = 0.0001
I0814 12:53:50.843185 28599 solver.cpp:456] Snapshotting to binary proto file norm_-0.01_iter_22000.caffemodel
I0814 12:53:50.871625 28599 sgd_solver.cpp:273] Snapshotting solver state to binary proto file norm_-0.01_iter_22000.solverstate
I0814 12:53:50.892859 28599 solver.cpp:338] Iteration 22000, Testing net (#0)
I0814 12:53:50.892904 28599 net.cpp:694] Ignoring source layer CustomData1
I0814 12:53:50.892909 28599 net.cpp:694] Ignoring source layer blob0_CustomData1_0_split
I0814 12:53:50.892912 28599 net.cpp:694] Ignoring source layer blob1_CustomData1_1_split
I0814 12:53:50.892927 28599 net.cpp:694] Ignoring source layer blob2_CustomData1_2_split
I0814 12:53:50.892938 28599 net.cpp:694] Ignoring source layer DummyData1
I0814 12:53:50.892956 28599 net.cpp:694] Ignoring source layer blob9_DummyData1_0_split
I0814 13:03:44.324370 28599 solver.cpp:406]     Test net output #0: down_up_half_loss_real = 0.0614319
I0814 13:03:44.324471 28599 solver.cpp:406]     Test net output #1: down_up_loss_real = 0.118198
I0814 13:03:44.324489 28599 solver.cpp:406]     Test net output #2: zoom_disp_loss0_real = 0.0497648
I0814 13:03:44.324496 28599 solver.cpp:406]     Test net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0497648 (* 1 = 0.0497648 loss)
I0814 13:03:44.485571 28599 solver.cpp:229] Iteration 22000, loss = 0.0886762
I0814 13:03:44.646234 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0535345
I0814 13:03:44.646289 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.102434
I0814 13:03:44.646317 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0443381
I0814 13:03:44.646337 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0443381 (* 1 = 0.0443381 loss)
I0814 13:03:44.646350 28599 sgd_solver.cpp:106] Iteration 22000, lr = 0.0001
I0814 13:04:17.384812 28599 solver.cpp:229] Iteration 22050, loss = 0.146197
I0814 13:04:17.533978 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0916813
I0814 13:04:17.534036 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.176423
I0814 13:04:17.534065 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0730984
I0814 13:04:17.534075 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0730984 (* 1 = 0.0730984 loss)
I0814 13:04:17.534082 28599 sgd_solver.cpp:106] Iteration 22050, lr = 0.0001
I0814 13:04:51.211338 28599 solver.cpp:229] Iteration 22100, loss = 0.173139
I0814 13:04:51.360838 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0790863
I0814 13:04:51.360903 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.151256
I0814 13:04:51.360926 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0865693
I0814 13:04:51.360941 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0865693 (* 1 = 0.0865693 loss)
I0814 13:04:51.360952 28599 sgd_solver.cpp:106] Iteration 22100, lr = 0.0001
I0814 13:05:24.542661 28599 solver.cpp:229] Iteration 22150, loss = 0.284684
I0814 13:05:24.697504 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.146968
I0814 13:05:24.697566 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.25135
I0814 13:05:24.697584 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.142342
I0814 13:05:24.697599 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.142342 (* 1 = 0.142342 loss)
I0814 13:05:24.697608 28599 sgd_solver.cpp:106] Iteration 22150, lr = 0.0001
I0814 13:05:57.714781 28599 solver.cpp:229] Iteration 22200, loss = 0.0670713
I0814 13:05:57.865490 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0378026
I0814 13:05:57.865543 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0726317
I0814 13:05:57.865558 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0335357
I0814 13:05:57.865567 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0335357 (* 1 = 0.0335357 loss)
I0814 13:05:57.865574 28599 sgd_solver.cpp:106] Iteration 22200, lr = 0.0001
I0814 13:06:30.763675 28599 solver.cpp:229] Iteration 22250, loss = 0.109498
I0814 13:06:30.917094 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0647746
I0814 13:06:30.917152 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.121919
I0814 13:06:30.917173 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0547492
I0814 13:06:30.917184 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0547492 (* 1 = 0.0547492 loss)
I0814 13:06:30.917192 28599 sgd_solver.cpp:106] Iteration 22250, lr = 0.0001
I0814 13:07:03.986693 28599 solver.cpp:229] Iteration 22300, loss = 0.17421
I0814 13:07:04.136855 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0718113
I0814 13:07:04.136919 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139646
I0814 13:07:04.136942 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0871049
I0814 13:07:04.136953 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0871049 (* 1 = 0.0871049 loss)
I0814 13:07:04.136962 28599 sgd_solver.cpp:106] Iteration 22300, lr = 0.0001
I0814 13:07:37.971726 28599 solver.cpp:229] Iteration 22350, loss = 0.15276
I0814 13:07:38.120251 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.088915
I0814 13:07:38.120319 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.172413
I0814 13:07:38.120349 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0763801
I0814 13:07:38.120405 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0763801 (* 1 = 0.0763801 loss)
I0814 13:07:38.120420 28599 sgd_solver.cpp:106] Iteration 22350, lr = 0.0001
I0814 13:08:10.866407 28599 solver.cpp:229] Iteration 22400, loss = 0.20166
I0814 13:08:11.015310 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.117893
I0814 13:08:11.015363 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.225825
I0814 13:08:11.015383 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.10083
I0814 13:08:11.015395 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.10083 (* 1 = 0.10083 loss)
I0814 13:08:11.015403 28599 sgd_solver.cpp:106] Iteration 22400, lr = 0.0001
I0814 13:08:44.232839 28599 solver.cpp:229] Iteration 22450, loss = 0.165353
I0814 13:08:44.381714 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0995352
I0814 13:08:44.381780 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.189353
I0814 13:08:44.381805 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0826764
I0814 13:08:44.381822 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0826764 (* 1 = 0.0826764 loss)
I0814 13:08:44.381834 28599 sgd_solver.cpp:106] Iteration 22450, lr = 0.0001
I0814 13:09:17.285949 28599 solver.cpp:229] Iteration 22500, loss = 0.107091
I0814 13:09:17.435348 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0640237
I0814 13:09:17.435402 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.124496
I0814 13:09:17.435420 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0535456
I0814 13:09:17.435431 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0535456 (* 1 = 0.0535456 loss)
I0814 13:09:17.435438 28599 sgd_solver.cpp:106] Iteration 22500, lr = 0.0001
I0814 13:09:50.251613 28599 solver.cpp:229] Iteration 22550, loss = 0.117863
I0814 13:09:50.400904 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0625078
I0814 13:09:50.400964 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.11975
I0814 13:09:50.400986 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0589314
I0814 13:09:50.400998 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0589314 (* 1 = 0.0589314 loss)
I0814 13:09:50.401007 28599 sgd_solver.cpp:106] Iteration 22550, lr = 0.0001
I0814 13:10:23.297051 28599 solver.cpp:229] Iteration 22600, loss = 0.135138
I0814 13:10:23.446090 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0737936
I0814 13:10:23.446156 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.140605
I0814 13:10:23.446178 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0675692
I0814 13:10:23.446192 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0675692 (* 1 = 0.0675692 loss)
I0814 13:10:23.446203 28599 sgd_solver.cpp:106] Iteration 22600, lr = 0.0001
I0814 13:10:56.781389 28599 solver.cpp:229] Iteration 22650, loss = 0.11814
I0814 13:10:56.930662 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0693766
I0814 13:10:56.930724 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.133197
I0814 13:10:56.930743 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0590699
I0814 13:10:56.930757 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0590699 (* 1 = 0.0590699 loss)
I0814 13:10:56.930768 28599 sgd_solver.cpp:106] Iteration 22650, lr = 0.0001
I0814 13:11:30.233091 28599 solver.cpp:229] Iteration 22700, loss = 0.148261
I0814 13:11:30.382052 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.101064
I0814 13:11:30.382105 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.193937
I0814 13:11:30.382120 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0741303
I0814 13:11:30.382130 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0741303 (* 1 = 0.0741303 loss)
I0814 13:11:30.382138 28599 sgd_solver.cpp:106] Iteration 22700, lr = 0.0001
I0814 13:12:03.333489 28599 solver.cpp:229] Iteration 22750, loss = 0.200073
I0814 13:12:03.482573 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.116439
I0814 13:12:03.482630 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.223482
I0814 13:12:03.482650 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.100037
I0814 13:12:03.482661 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.100037 (* 1 = 0.100037 loss)
I0814 13:12:03.482671 28599 sgd_solver.cpp:106] Iteration 22750, lr = 0.0001
I0814 13:12:36.217778 28599 solver.cpp:229] Iteration 22800, loss = 0.18699
I0814 13:12:36.366631 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.112627
I0814 13:12:36.366693 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.220525
I0814 13:12:36.366714 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0934952
I0814 13:12:36.366729 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0934952 (* 1 = 0.0934952 loss)
I0814 13:12:36.366740 28599 sgd_solver.cpp:106] Iteration 22800, lr = 0.0001
I0814 13:13:09.741482 28599 solver.cpp:229] Iteration 22850, loss = 0.198653
I0814 13:13:09.890161 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.107752
I0814 13:13:09.890234 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.19165
I0814 13:13:09.890260 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0993266
I0814 13:13:09.890275 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0993266 (* 1 = 0.0993266 loss)
I0814 13:13:09.890286 28599 sgd_solver.cpp:106] Iteration 22850, lr = 0.0001
I0814 13:13:42.825259 28599 solver.cpp:229] Iteration 22900, loss = 0.149802
I0814 13:13:42.974594 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0849479
I0814 13:13:42.974651 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.166533
I0814 13:13:42.974671 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0749011
I0814 13:13:42.974683 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0749011 (* 1 = 0.0749011 loss)
I0814 13:13:42.974691 28599 sgd_solver.cpp:106] Iteration 22900, lr = 0.0001
I0814 13:14:16.341825 28599 solver.cpp:229] Iteration 22950, loss = 0.146424
I0814 13:14:16.490885 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0854566
I0814 13:14:16.490942 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.159039
I0814 13:14:16.490960 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0732118
I0814 13:14:16.490972 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0732118 (* 1 = 0.0732118 loss)
I0814 13:14:16.490978 28599 sgd_solver.cpp:106] Iteration 22950, lr = 0.0001
I0814 13:14:49.298782 28599 solver.cpp:229] Iteration 23000, loss = 0.128566
I0814 13:14:49.447516 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0721196
I0814 13:14:49.447583 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139755
I0814 13:14:49.447634 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.064283
I0814 13:14:49.447649 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.064283 (* 1 = 0.064283 loss)
I0814 13:14:49.447659 28599 sgd_solver.cpp:106] Iteration 23000, lr = 0.0001
I0814 13:15:22.751878 28599 solver.cpp:229] Iteration 23050, loss = 0.0734346
I0814 13:15:22.900851 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0410857
I0814 13:15:22.900905 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0813196
I0814 13:15:22.900923 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0367173
I0814 13:15:22.900934 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0367173 (* 1 = 0.0367173 loss)
I0814 13:15:22.900943 28599 sgd_solver.cpp:106] Iteration 23050, lr = 0.0001
I0814 13:15:56.326196 28599 solver.cpp:229] Iteration 23100, loss = 0.126896
I0814 13:15:56.474879 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0670211
I0814 13:15:56.474947 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129217
I0814 13:15:56.474972 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0634479
I0814 13:15:56.474988 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0634479 (* 1 = 0.0634479 loss)
I0814 13:15:56.475003 28599 sgd_solver.cpp:106] Iteration 23100, lr = 0.0001
I0814 13:16:29.986035 28599 solver.cpp:229] Iteration 23150, loss = 0.156808
I0814 13:16:30.135190 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0719213
I0814 13:16:30.135243 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.14266
I0814 13:16:30.135265 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0784042
I0814 13:16:30.135277 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0784042 (* 1 = 0.0784042 loss)
I0814 13:16:30.135284 28599 sgd_solver.cpp:106] Iteration 23150, lr = 0.0001
I0814 13:17:03.658314 28599 solver.cpp:229] Iteration 23200, loss = 0.22964
I0814 13:17:03.807379 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0868099
I0814 13:17:03.807442 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.172451
I0814 13:17:03.807464 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11482
I0814 13:17:03.807478 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11482 (* 1 = 0.11482 loss)
I0814 13:17:03.807489 28599 sgd_solver.cpp:106] Iteration 23200, lr = 0.0001
I0814 13:17:37.968938 28599 solver.cpp:229] Iteration 23250, loss = 0.164667
I0814 13:17:38.117822 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0941591
I0814 13:17:38.117888 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.178828
I0814 13:17:38.117910 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0823333
I0814 13:17:38.117928 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0823333 (* 1 = 0.0823333 loss)
I0814 13:17:38.117940 28599 sgd_solver.cpp:106] Iteration 23250, lr = 0.0001
I0814 13:18:11.617180 28599 solver.cpp:229] Iteration 23300, loss = 0.154836
I0814 13:18:11.765728 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0958174
I0814 13:18:11.765789 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.177927
I0814 13:18:11.765811 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0774179
I0814 13:18:11.765827 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0774179 (* 1 = 0.0774179 loss)
I0814 13:18:11.765838 28599 sgd_solver.cpp:106] Iteration 23300, lr = 0.0001
I0814 13:18:45.934200 28599 solver.cpp:229] Iteration 23350, loss = 0.169905
I0814 13:18:46.084183 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.105762
I0814 13:18:46.084245 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.191425
I0814 13:18:46.084267 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0849523
I0814 13:18:46.084283 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0849523 (* 1 = 0.0849523 loss)
I0814 13:18:46.084292 28599 sgd_solver.cpp:106] Iteration 23350, lr = 0.0001
I0814 13:19:19.834055 28599 solver.cpp:229] Iteration 23400, loss = 0.094329
I0814 13:19:19.983621 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0613582
I0814 13:19:19.983683 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.116949
I0814 13:19:19.983705 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0471645
I0814 13:19:19.983721 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0471645 (* 1 = 0.0471645 loss)
I0814 13:19:19.983732 28599 sgd_solver.cpp:106] Iteration 23400, lr = 0.0001
I0814 13:19:53.445966 28599 solver.cpp:229] Iteration 23450, loss = 0.234571
I0814 13:19:53.595213 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.138958
I0814 13:19:53.595280 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.249541
I0814 13:19:53.595305 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.117286
I0814 13:19:53.595320 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.117286 (* 1 = 0.117286 loss)
I0814 13:19:53.595330 28599 sgd_solver.cpp:106] Iteration 23450, lr = 0.0001
I0814 13:20:27.581792 28599 solver.cpp:229] Iteration 23500, loss = 0.0746579
I0814 13:20:27.731050 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0410902
I0814 13:20:27.731118 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0796348
I0814 13:20:27.731142 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0373289
I0814 13:20:27.731154 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0373289 (* 1 = 0.0373289 loss)
I0814 13:20:27.731164 28599 sgd_solver.cpp:106] Iteration 23500, lr = 0.0001
I0814 13:21:01.823624 28599 solver.cpp:229] Iteration 23550, loss = 0.145543
I0814 13:21:01.972407 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0829917
I0814 13:21:01.972472 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.155947
I0814 13:21:01.972494 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0727716
I0814 13:21:01.972512 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0727716 (* 1 = 0.0727716 loss)
I0814 13:21:01.972523 28599 sgd_solver.cpp:106] Iteration 23550, lr = 0.0001
I0814 13:21:34.592368 28599 solver.cpp:229] Iteration 23600, loss = 0.263568
I0814 13:21:34.741407 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.174837
I0814 13:21:34.741467 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.338947
I0814 13:21:34.741482 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.131784
I0814 13:21:34.741498 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.131784 (* 1 = 0.131784 loss)
I0814 13:21:34.741509 28599 sgd_solver.cpp:106] Iteration 23600, lr = 0.0001
I0814 13:22:08.176075 28599 solver.cpp:229] Iteration 23650, loss = 0.162294
I0814 13:22:08.327927 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0853243
I0814 13:22:08.327993 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.16239
I0814 13:22:08.328016 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0811471
I0814 13:22:08.328032 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0811471 (* 1 = 0.0811471 loss)
I0814 13:22:08.328043 28599 sgd_solver.cpp:106] Iteration 23650, lr = 0.0001
I0814 13:22:41.124941 28599 solver.cpp:229] Iteration 23700, loss = 0.162391
I0814 13:22:41.276706 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.102147
I0814 13:22:41.276772 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.196979
I0814 13:22:41.276793 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0811956
I0814 13:22:41.276809 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0811956 (* 1 = 0.0811956 loss)
I0814 13:22:41.276819 28599 sgd_solver.cpp:106] Iteration 23700, lr = 0.0001
I0814 13:23:14.447348 28599 solver.cpp:229] Iteration 23750, loss = 0.108355
I0814 13:23:14.597031 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0656403
I0814 13:23:14.597082 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.125491
I0814 13:23:14.597098 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0541775
I0814 13:23:14.597107 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0541775 (* 1 = 0.0541775 loss)
I0814 13:23:14.597115 28599 sgd_solver.cpp:106] Iteration 23750, lr = 0.0001
I0814 13:23:47.883988 28599 solver.cpp:229] Iteration 23800, loss = 0.128969
I0814 13:23:48.032667 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0689292
I0814 13:23:48.032727 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.130121
I0814 13:23:48.032745 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0644845
I0814 13:23:48.032757 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0644845 (* 1 = 0.0644845 loss)
I0814 13:23:48.032768 28599 sgd_solver.cpp:106] Iteration 23800, lr = 0.0001
I0814 13:24:21.846256 28599 solver.cpp:229] Iteration 23850, loss = 0.126678
I0814 13:24:21.994700 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0703448
I0814 13:24:21.994762 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.134971
I0814 13:24:21.994782 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0633388
I0814 13:24:21.994794 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0633388 (* 1 = 0.0633388 loss)
I0814 13:24:21.994808 28599 sgd_solver.cpp:106] Iteration 23850, lr = 0.0001
I0814 13:24:54.790603 28599 solver.cpp:229] Iteration 23900, loss = 0.0880842
I0814 13:24:54.939968 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0518254
I0814 13:24:54.940028 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0991386
I0814 13:24:54.940048 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0440421
I0814 13:24:54.940062 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0440421 (* 1 = 0.0440421 loss)
I0814 13:24:54.940070 28599 sgd_solver.cpp:106] Iteration 23900, lr = 0.0001
I0814 13:25:27.970389 28599 solver.cpp:229] Iteration 23950, loss = 0.102709
I0814 13:25:28.119401 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0618467
I0814 13:25:28.119457 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.118314
I0814 13:25:28.119474 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0513544
I0814 13:25:28.119487 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0513544 (* 1 = 0.0513544 loss)
I0814 13:25:28.119495 28599 sgd_solver.cpp:106] Iteration 23950, lr = 0.0001
I0814 13:26:01.985491 28599 solver.cpp:229] Iteration 24000, loss = 0.123656
I0814 13:26:02.134711 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0758737
I0814 13:26:02.134778 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.144775
I0814 13:26:02.134794 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0618281
I0814 13:26:02.134804 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0618281 (* 1 = 0.0618281 loss)
I0814 13:26:02.134814 28599 sgd_solver.cpp:106] Iteration 24000, lr = 0.0001
I0814 13:26:34.993659 28599 solver.cpp:229] Iteration 24050, loss = 0.164632
I0814 13:26:35.142940 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.101014
I0814 13:26:35.143009 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.191862
I0814 13:26:35.143030 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0823161
I0814 13:26:35.143045 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0823161 (* 1 = 0.0823161 loss)
I0814 13:26:35.143056 28599 sgd_solver.cpp:106] Iteration 24050, lr = 0.0001
I0814 13:27:08.087462 28599 solver.cpp:229] Iteration 24100, loss = 0.23956
I0814 13:27:08.236259 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.14649
I0814 13:27:08.236322 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.280758
I0814 13:27:08.236343 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11978
I0814 13:27:08.236358 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11978 (* 1 = 0.11978 loss)
I0814 13:27:08.236368 28599 sgd_solver.cpp:106] Iteration 24100, lr = 0.0001
I0814 13:27:42.147559 28599 solver.cpp:229] Iteration 24150, loss = 0.214757
I0814 13:27:42.296528 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.134558
I0814 13:27:42.296591 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.257154
I0814 13:27:42.296613 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.107379
I0814 13:27:42.296630 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.107379 (* 1 = 0.107379 loss)
I0814 13:27:42.296639 28599 sgd_solver.cpp:106] Iteration 24150, lr = 0.0001
I0814 13:28:15.948884 28599 solver.cpp:229] Iteration 24200, loss = 0.130843
I0814 13:28:16.097729 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0682183
I0814 13:28:16.097792 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.134395
I0814 13:28:16.097817 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0654216
I0814 13:28:16.097832 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0654216 (* 1 = 0.0654216 loss)
I0814 13:28:16.097843 28599 sgd_solver.cpp:106] Iteration 24200, lr = 0.0001
I0814 13:28:50.015295 28599 solver.cpp:229] Iteration 24250, loss = 0.106776
I0814 13:28:50.164212 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0521154
I0814 13:28:50.164275 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0980389
I0814 13:28:50.164295 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.053388
I0814 13:28:50.164310 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.053388 (* 1 = 0.053388 loss)
I0814 13:28:50.164321 28599 sgd_solver.cpp:106] Iteration 24250, lr = 0.0001
I0814 13:29:24.225880 28599 solver.cpp:229] Iteration 24300, loss = 0.108101
I0814 13:29:24.372803 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0605889
I0814 13:29:24.372866 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.114964
I0814 13:29:24.372889 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0540506
I0814 13:29:24.372902 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0540506 (* 1 = 0.0540506 loss)
I0814 13:29:24.372912 28599 sgd_solver.cpp:106] Iteration 24300, lr = 0.0001
I0814 13:29:58.083766 28599 solver.cpp:229] Iteration 24350, loss = 0.162356
I0814 13:29:58.232918 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.100242
I0814 13:29:58.232980 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.191648
I0814 13:29:58.233001 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0811778
I0814 13:29:58.233016 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0811778 (* 1 = 0.0811778 loss)
I0814 13:29:58.233031 28599 sgd_solver.cpp:106] Iteration 24350, lr = 0.0001
I0814 13:30:31.781644 28599 solver.cpp:229] Iteration 24400, loss = 0.201641
I0814 13:30:31.930063 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.132786
I0814 13:30:31.930130 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.251954
I0814 13:30:31.930150 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.10082
I0814 13:30:31.930161 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.10082 (* 1 = 0.10082 loss)
I0814 13:30:31.930171 28599 sgd_solver.cpp:106] Iteration 24400, lr = 0.0001
I0814 13:31:05.596328 28599 solver.cpp:229] Iteration 24450, loss = 0.116399
I0814 13:31:05.744976 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0612395
I0814 13:31:05.745036 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.115289
I0814 13:31:05.745057 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0581995
I0814 13:31:05.745074 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0581995 (* 1 = 0.0581995 loss)
I0814 13:31:05.745084 28599 sgd_solver.cpp:106] Iteration 24450, lr = 0.0001
I0814 13:31:39.182000 28599 solver.cpp:229] Iteration 24500, loss = 0.0661007
I0814 13:31:39.331784 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0358434
I0814 13:31:39.331849 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0716418
I0814 13:31:39.331872 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0330503
I0814 13:31:39.331925 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0330503 (* 1 = 0.0330503 loss)
I0814 13:31:39.331940 28599 sgd_solver.cpp:106] Iteration 24500, lr = 0.0001
I0814 13:32:12.660028 28599 solver.cpp:229] Iteration 24550, loss = 0.152655
I0814 13:32:12.809039 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0837628
I0814 13:32:12.809103 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.15586
I0814 13:32:12.809124 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0763274
I0814 13:32:12.809139 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0763274 (* 1 = 0.0763274 loss)
I0814 13:32:12.809149 28599 sgd_solver.cpp:106] Iteration 24550, lr = 0.0001
I0814 13:32:45.854236 28599 solver.cpp:229] Iteration 24600, loss = 0.238586
I0814 13:32:46.003136 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.133955
I0814 13:32:46.003198 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.251259
I0814 13:32:46.003217 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.119293
I0814 13:32:46.003267 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.119293 (* 1 = 0.119293 loss)
I0814 13:32:46.003279 28599 sgd_solver.cpp:106] Iteration 24600, lr = 0.0001
I0814 13:33:19.666832 28599 solver.cpp:229] Iteration 24650, loss = 0.16549
I0814 13:33:19.815557 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0963304
I0814 13:33:19.815623 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.182213
I0814 13:33:19.815642 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0827449
I0814 13:33:19.815656 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0827449 (* 1 = 0.0827449 loss)
I0814 13:33:19.815665 28599 sgd_solver.cpp:106] Iteration 24650, lr = 0.0001
I0814 13:33:53.353513 28599 solver.cpp:229] Iteration 24700, loss = 0.0992185
I0814 13:33:53.503933 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0555759
I0814 13:33:53.503994 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.10592
I0814 13:33:53.504016 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0496092
I0814 13:33:53.504034 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0496092 (* 1 = 0.0496092 loss)
I0814 13:33:53.504045 28599 sgd_solver.cpp:106] Iteration 24700, lr = 0.0001
I0814 13:34:27.069376 28599 solver.cpp:229] Iteration 24750, loss = 0.106173
I0814 13:34:27.218631 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0607172
I0814 13:34:27.218699 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.103627
I0814 13:34:27.218727 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0530862
I0814 13:34:27.218744 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0530862 (* 1 = 0.0530862 loss)
I0814 13:34:27.218756 28599 sgd_solver.cpp:106] Iteration 24750, lr = 0.0001
I0814 13:35:00.560739 28599 solver.cpp:229] Iteration 24800, loss = 0.104204
I0814 13:35:00.709439 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0497666
I0814 13:35:00.709501 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0932364
I0814 13:35:00.709520 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0521021
I0814 13:35:00.709534 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0521021 (* 1 = 0.0521021 loss)
I0814 13:35:00.709544 28599 sgd_solver.cpp:106] Iteration 24800, lr = 0.0001
I0814 13:35:34.180167 28599 solver.cpp:229] Iteration 24850, loss = 0.206043
I0814 13:35:34.328835 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.112185
I0814 13:35:34.328896 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.213595
I0814 13:35:34.328915 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.103022
I0814 13:35:34.328927 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.103022 (* 1 = 0.103022 loss)
I0814 13:35:34.328938 28599 sgd_solver.cpp:106] Iteration 24850, lr = 0.0001
I0814 13:36:08.205432 28599 solver.cpp:229] Iteration 24900, loss = 0.156043
I0814 13:36:08.354346 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.106738
I0814 13:36:08.354410 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.204654
I0814 13:36:08.354431 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0780214
I0814 13:36:08.354449 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0780214 (* 1 = 0.0780214 loss)
I0814 13:36:08.354460 28599 sgd_solver.cpp:106] Iteration 24900, lr = 0.0001
I0814 13:36:40.044308 28599 solver.cpp:229] Iteration 24950, loss = 0.209782
I0814 13:36:40.193541 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.111607
I0814 13:36:40.193603 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.209032
I0814 13:36:40.193634 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.104891
I0814 13:36:40.193642 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.104891 (* 1 = 0.104891 loss)
I0814 13:36:40.193650 28599 sgd_solver.cpp:106] Iteration 24950, lr = 0.0001
I0814 13:37:15.475253 28599 solver.cpp:229] Iteration 25000, loss = 0.102672
I0814 13:37:15.623227 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0592529
I0814 13:37:15.623453 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.113807
I0814 13:37:15.623554 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.051336
I0814 13:37:15.623608 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.051336 (* 1 = 0.051336 loss)
I0814 13:37:15.623648 28599 sgd_solver.cpp:106] Iteration 25000, lr = 0.0001
I0814 13:37:48.759332 28599 solver.cpp:229] Iteration 25050, loss = 0.222026
I0814 13:37:48.907969 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.127006
I0814 13:37:48.908031 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.218215
I0814 13:37:48.908053 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.111013
I0814 13:37:48.908067 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.111013 (* 1 = 0.111013 loss)
I0814 13:37:48.908077 28599 sgd_solver.cpp:106] Iteration 25050, lr = 0.0001
I0814 13:38:22.052781 28599 solver.cpp:229] Iteration 25100, loss = 0.207105
I0814 13:38:22.200981 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.12029
I0814 13:38:22.201047 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.233225
I0814 13:38:22.201068 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.103552
I0814 13:38:22.201086 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.103552 (* 1 = 0.103552 loss)
I0814 13:38:22.201097 28599 sgd_solver.cpp:106] Iteration 25100, lr = 0.0001
I0814 13:38:55.439005 28599 solver.cpp:229] Iteration 25150, loss = 0.156057
I0814 13:38:55.587765 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0783748
I0814 13:38:55.587827 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.152419
I0814 13:38:55.587847 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0780284
I0814 13:38:55.587862 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0780284 (* 1 = 0.0780284 loss)
I0814 13:38:55.587872 28599 sgd_solver.cpp:106] Iteration 25150, lr = 0.0001
I0814 13:39:28.282578 28599 solver.cpp:229] Iteration 25200, loss = 0.193099
I0814 13:39:28.431661 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.110192
I0814 13:39:28.431717 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.19293
I0814 13:39:28.431733 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0965496
I0814 13:39:28.431743 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0965496 (* 1 = 0.0965496 loss)
I0814 13:39:28.431751 28599 sgd_solver.cpp:106] Iteration 25200, lr = 0.0001
I0814 13:40:00.830754 28599 solver.cpp:229] Iteration 25250, loss = 0.152725
I0814 13:40:00.979240 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0875209
I0814 13:40:00.979297 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163709
I0814 13:40:00.979316 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0763625
I0814 13:40:00.979326 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0763625 (* 1 = 0.0763625 loss)
I0814 13:40:00.979336 28599 sgd_solver.cpp:106] Iteration 25250, lr = 0.0001
I0814 13:40:33.809481 28599 solver.cpp:229] Iteration 25300, loss = 0.179517
I0814 13:40:33.958812 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.109106
I0814 13:40:33.958869 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.200359
I0814 13:40:33.958887 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0897586
I0814 13:40:33.958900 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0897586 (* 1 = 0.0897586 loss)
I0814 13:40:33.958909 28599 sgd_solver.cpp:106] Iteration 25300, lr = 0.0001
I0814 13:41:06.825908 28599 solver.cpp:229] Iteration 25350, loss = 0.0611852
I0814 13:41:06.974812 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0191544
I0814 13:41:06.974886 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0375803
I0814 13:41:06.974911 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0305926
I0814 13:41:06.974927 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0305926 (* 1 = 0.0305926 loss)
I0814 13:41:06.974939 28599 sgd_solver.cpp:106] Iteration 25350, lr = 0.0001
I0814 13:41:39.958744 28599 solver.cpp:229] Iteration 25400, loss = 0.0957544
I0814 13:41:40.107687 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0618376
I0814 13:41:40.107740 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.118405
I0814 13:41:40.107756 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0478771
I0814 13:41:40.107769 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0478771 (* 1 = 0.0478771 loss)
I0814 13:41:40.107775 28599 sgd_solver.cpp:106] Iteration 25400, lr = 0.0001
I0814 13:42:12.664916 28599 solver.cpp:229] Iteration 25450, loss = 0.159475
I0814 13:42:12.813617 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0940386
I0814 13:42:12.813678 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.174272
I0814 13:42:12.813697 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0797373
I0814 13:42:12.813711 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0797373 (* 1 = 0.0797373 loss)
I0814 13:42:12.813721 28599 sgd_solver.cpp:106] Iteration 25450, lr = 0.0001
I0814 13:42:45.566720 28599 solver.cpp:229] Iteration 25500, loss = 0.164541
I0814 13:42:45.715631 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.10121
I0814 13:42:45.715714 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.18786
I0814 13:42:45.715732 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0822703
I0814 13:42:45.715742 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0822703 (* 1 = 0.0822703 loss)
I0814 13:42:45.715750 28599 sgd_solver.cpp:106] Iteration 25500, lr = 0.0001
I0814 13:43:18.329910 28599 solver.cpp:229] Iteration 25550, loss = 0.226708
I0814 13:43:18.478992 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.129984
I0814 13:43:18.479046 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.244665
I0814 13:43:18.479063 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.113354
I0814 13:43:18.479075 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.113354 (* 1 = 0.113354 loss)
I0814 13:43:18.479084 28599 sgd_solver.cpp:106] Iteration 25550, lr = 0.0001
I0814 13:43:51.245312 28599 solver.cpp:229] Iteration 25600, loss = 0.152875
I0814 13:43:51.395747 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0759259
I0814 13:43:51.395802 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.146408
I0814 13:43:51.395817 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0764376
I0814 13:43:51.395826 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0764376 (* 1 = 0.0764376 loss)
I0814 13:43:51.395836 28599 sgd_solver.cpp:106] Iteration 25600, lr = 0.0001
I0814 13:44:24.096315 28599 solver.cpp:229] Iteration 25650, loss = 0.163087
I0814 13:44:24.245308 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0918689
I0814 13:44:24.245374 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.171464
I0814 13:44:24.245400 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0815437
I0814 13:44:24.245414 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0815437 (* 1 = 0.0815437 loss)
I0814 13:44:24.245424 28599 sgd_solver.cpp:106] Iteration 25650, lr = 0.0001
I0814 13:44:57.474182 28599 solver.cpp:229] Iteration 25700, loss = 0.0624702
I0814 13:44:57.623170 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0395034
I0814 13:44:57.623224 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.074079
I0814 13:44:57.623240 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0312351
I0814 13:44:57.623250 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0312351 (* 1 = 0.0312351 loss)
I0814 13:44:57.623258 28599 sgd_solver.cpp:106] Iteration 25700, lr = 0.0001
I0814 13:45:30.317044 28599 solver.cpp:229] Iteration 25750, loss = 0.0895093
I0814 13:45:30.465059 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0563893
I0814 13:45:30.465111 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.108198
I0814 13:45:30.465128 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0447547
I0814 13:45:30.465139 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0447547 (* 1 = 0.0447547 loss)
I0814 13:45:30.465147 28599 sgd_solver.cpp:106] Iteration 25750, lr = 0.0001
I0814 13:46:03.967221 28599 solver.cpp:229] Iteration 25800, loss = 0.119764
I0814 13:46:04.116020 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0747124
I0814 13:46:04.116084 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.142127
I0814 13:46:04.116117 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0598821
I0814 13:46:04.116132 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0598821 (* 1 = 0.0598821 loss)
I0814 13:46:04.116142 28599 sgd_solver.cpp:106] Iteration 25800, lr = 0.0001
I0814 13:46:37.003602 28599 solver.cpp:229] Iteration 25850, loss = 0.129431
I0814 13:46:37.152387 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0735283
I0814 13:46:37.152453 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.134807
I0814 13:46:37.152477 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0647154
I0814 13:46:37.152489 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0647154 (* 1 = 0.0647154 loss)
I0814 13:46:37.152499 28599 sgd_solver.cpp:106] Iteration 25850, lr = 0.0001
I0814 13:47:09.929637 28599 solver.cpp:229] Iteration 25900, loss = 0.132558
I0814 13:47:10.078774 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0771787
I0814 13:47:10.078824 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.147403
I0814 13:47:10.078841 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0662791
I0814 13:47:10.078851 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0662791 (* 1 = 0.0662791 loss)
I0814 13:47:10.078860 28599 sgd_solver.cpp:106] Iteration 25900, lr = 0.0001
I0814 13:47:43.070462 28599 solver.cpp:229] Iteration 25950, loss = 0.113758
I0814 13:47:43.219189 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0701307
I0814 13:47:43.219251 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.133906
I0814 13:47:43.219272 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.056879
I0814 13:47:43.219286 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.056879 (* 1 = 0.056879 loss)
I0814 13:47:43.219295 28599 sgd_solver.cpp:106] Iteration 25950, lr = 0.0001
I0814 13:48:16.192874 28599 solver.cpp:229] Iteration 26000, loss = 0.164384
I0814 13:48:16.341713 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.101892
I0814 13:48:16.341763 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.19384
I0814 13:48:16.341778 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.082192
I0814 13:48:16.341789 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.082192 (* 1 = 0.082192 loss)
I0814 13:48:16.341795 28599 sgd_solver.cpp:106] Iteration 26000, lr = 0.0001
I0814 13:48:49.023625 28599 solver.cpp:229] Iteration 26050, loss = 0.0880215
I0814 13:48:49.170332 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0520956
I0814 13:48:49.170392 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0992386
I0814 13:48:49.170408 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0440107
I0814 13:48:49.170420 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0440107 (* 1 = 0.0440107 loss)
I0814 13:48:49.170429 28599 sgd_solver.cpp:106] Iteration 26050, lr = 0.0001
I0814 13:49:22.447315 28599 solver.cpp:229] Iteration 26100, loss = 0.13655
I0814 13:49:22.596293 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0766205
I0814 13:49:22.596377 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.146506
I0814 13:49:22.596411 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0682751
I0814 13:49:22.596429 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0682751 (* 1 = 0.0682751 loss)
I0814 13:49:22.596443 28599 sgd_solver.cpp:106] Iteration 26100, lr = 0.0001
I0814 13:49:55.808023 28599 solver.cpp:229] Iteration 26150, loss = 0.355732
I0814 13:49:55.956838 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.21383
I0814 13:49:55.956897 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.4019
I0814 13:49:55.956915 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.177866
I0814 13:49:55.956928 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.177866 (* 1 = 0.177866 loss)
I0814 13:49:55.956936 28599 sgd_solver.cpp:106] Iteration 26150, lr = 0.0001
I0814 13:50:28.475788 28599 solver.cpp:229] Iteration 26200, loss = 0.229694
I0814 13:50:28.624666 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.148825
I0814 13:50:28.624717 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.284235
I0814 13:50:28.624735 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.114847
I0814 13:50:28.624745 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.114847 (* 1 = 0.114847 loss)
I0814 13:50:28.624755 28599 sgd_solver.cpp:106] Iteration 26200, lr = 0.0001
I0814 13:51:01.835678 28599 solver.cpp:229] Iteration 26250, loss = 0.061853
I0814 13:51:01.984359 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0297059
I0814 13:51:01.984426 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0589277
I0814 13:51:01.984452 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0309265
I0814 13:51:01.984518 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0309265 (* 1 = 0.0309265 loss)
I0814 13:51:01.984555 28599 sgd_solver.cpp:106] Iteration 26250, lr = 0.0001
I0814 13:51:34.466365 28599 solver.cpp:229] Iteration 26300, loss = 0.169336
I0814 13:51:34.615618 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.108548
I0814 13:51:34.615686 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.204316
I0814 13:51:34.615711 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0846681
I0814 13:51:34.615731 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0846681 (* 1 = 0.0846681 loss)
I0814 13:51:34.615743 28599 sgd_solver.cpp:106] Iteration 26300, lr = 0.0001
I0814 13:52:07.709383 28599 solver.cpp:229] Iteration 26350, loss = 0.111785
I0814 13:52:07.858369 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.064864
I0814 13:52:07.858445 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.128368
I0814 13:52:07.858474 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0558925
I0814 13:52:07.858494 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0558925 (* 1 = 0.0558925 loss)
I0814 13:52:07.858508 28599 sgd_solver.cpp:106] Iteration 26350, lr = 0.0001
I0814 13:52:41.248420 28599 solver.cpp:229] Iteration 26400, loss = 0.156966
I0814 13:52:41.397330 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0980902
I0814 13:52:41.397383 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.187342
I0814 13:52:41.397399 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.078483
I0814 13:52:41.397410 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.078483 (* 1 = 0.078483 loss)
I0814 13:52:41.397418 28599 sgd_solver.cpp:106] Iteration 26400, lr = 0.0001
I0814 13:53:14.532101 28599 solver.cpp:229] Iteration 26450, loss = 0.150514
I0814 13:53:14.681150 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0864982
I0814 13:53:14.681206 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163899
I0814 13:53:14.681224 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0752571
I0814 13:53:14.681236 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0752571 (* 1 = 0.0752571 loss)
I0814 13:53:14.681244 28599 sgd_solver.cpp:106] Iteration 26450, lr = 0.0001
I0814 13:53:47.717803 28599 solver.cpp:229] Iteration 26500, loss = 0.111299
I0814 13:53:47.866770 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0644434
I0814 13:53:47.866821 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.119825
I0814 13:53:47.866837 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0556496
I0814 13:53:47.866847 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0556496 (* 1 = 0.0556496 loss)
I0814 13:53:47.866856 28599 sgd_solver.cpp:106] Iteration 26500, lr = 0.0001
I0814 13:54:20.693927 28599 solver.cpp:229] Iteration 26550, loss = 0.250768
I0814 13:54:20.842640 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.158247
I0814 13:54:20.842692 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.298921
I0814 13:54:20.842710 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.125384
I0814 13:54:20.842720 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.125384 (* 1 = 0.125384 loss)
I0814 13:54:20.842730 28599 sgd_solver.cpp:106] Iteration 26550, lr = 0.0001
I0814 13:54:53.941673 28599 solver.cpp:229] Iteration 26600, loss = 0.124026
I0814 13:54:54.090522 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0776734
I0814 13:54:54.090576 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.149086
I0814 13:54:54.090593 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0620131
I0814 13:54:54.090603 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0620131 (* 1 = 0.0620131 loss)
I0814 13:54:54.090611 28599 sgd_solver.cpp:106] Iteration 26600, lr = 0.0001
I0814 13:55:27.337836 28599 solver.cpp:229] Iteration 26650, loss = 0.135498
I0814 13:55:27.486711 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0672223
I0814 13:55:27.486764 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.124329
I0814 13:55:27.486783 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0677488
I0814 13:55:27.486793 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0677488 (* 1 = 0.0677488 loss)
I0814 13:55:27.486802 28599 sgd_solver.cpp:106] Iteration 26650, lr = 0.0001
I0814 13:56:00.524806 28599 solver.cpp:229] Iteration 26700, loss = 0.24702
I0814 13:56:00.673923 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.152838
I0814 13:56:00.673981 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.298253
I0814 13:56:00.674005 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.12351
I0814 13:56:00.674017 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.12351 (* 1 = 0.12351 loss)
I0814 13:56:00.674026 28599 sgd_solver.cpp:106] Iteration 26700, lr = 0.0001
I0814 13:56:33.606056 28599 solver.cpp:229] Iteration 26750, loss = 0.114539
I0814 13:56:33.754921 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.067972
I0814 13:56:33.754981 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.130524
I0814 13:56:33.755000 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0572694
I0814 13:56:33.755013 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0572694 (* 1 = 0.0572694 loss)
I0814 13:56:33.755023 28599 sgd_solver.cpp:106] Iteration 26750, lr = 0.0001
I0814 13:57:06.631594 28599 solver.cpp:229] Iteration 26800, loss = 0.218282
I0814 13:57:06.780616 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.121492
I0814 13:57:06.780680 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.233761
I0814 13:57:06.780747 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.109141
I0814 13:57:06.780768 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.109141 (* 1 = 0.109141 loss)
I0814 13:57:06.780781 28599 sgd_solver.cpp:106] Iteration 26800, lr = 0.0001
I0814 13:57:39.775501 28599 solver.cpp:229] Iteration 26850, loss = 0.102718
I0814 13:57:39.924393 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0616956
I0814 13:57:39.924458 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.115068
I0814 13:57:39.924480 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0513592
I0814 13:57:39.924496 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0513592 (* 1 = 0.0513592 loss)
I0814 13:57:39.924506 28599 sgd_solver.cpp:106] Iteration 26850, lr = 0.0001
I0814 13:58:12.682020 28599 solver.cpp:229] Iteration 26900, loss = 0.183286
I0814 13:58:12.830298 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.117938
I0814 13:58:12.830379 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.223223
I0814 13:58:12.830407 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0916429
I0814 13:58:12.830430 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0916429 (* 1 = 0.0916429 loss)
I0814 13:58:12.830446 28599 sgd_solver.cpp:106] Iteration 26900, lr = 0.0001
I0814 13:58:45.729821 28599 solver.cpp:229] Iteration 26950, loss = 0.159685
I0814 13:58:45.878840 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0954003
I0814 13:58:45.878906 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.186001
I0814 13:58:45.878922 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0798426
I0814 13:58:45.878931 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0798426 (* 1 = 0.0798426 loss)
I0814 13:58:45.878938 28599 sgd_solver.cpp:106] Iteration 26950, lr = 0.0001
I0814 13:59:18.924221 28599 solver.cpp:229] Iteration 27000, loss = 0.0812278
I0814 13:59:19.073019 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0549593
I0814 13:59:19.073081 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.105625
I0814 13:59:19.073102 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0406139
I0814 13:59:19.073114 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0406139 (* 1 = 0.0406139 loss)
I0814 13:59:19.073124 28599 sgd_solver.cpp:106] Iteration 27000, lr = 0.0001
I0814 13:59:51.862135 28599 solver.cpp:229] Iteration 27050, loss = 0.134542
I0814 13:59:52.011301 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0795061
I0814 13:59:52.011349 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.148776
I0814 13:59:52.011366 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0672707
I0814 13:59:52.011378 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0672707 (* 1 = 0.0672707 loss)
I0814 13:59:52.011385 28599 sgd_solver.cpp:106] Iteration 27050, lr = 0.0001
I0814 14:00:25.317526 28599 solver.cpp:229] Iteration 27100, loss = 0.151906
I0814 14:00:25.466009 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0859887
I0814 14:00:25.466070 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.15988
I0814 14:00:25.466094 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0759528
I0814 14:00:25.466107 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0759528 (* 1 = 0.0759528 loss)
I0814 14:00:25.466117 28599 sgd_solver.cpp:106] Iteration 27100, lr = 0.0001
I0814 14:00:58.478410 28599 solver.cpp:229] Iteration 27150, loss = 0.171812
I0814 14:00:58.626735 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.115942
I0814 14:00:58.626792 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.221853
I0814 14:00:58.626811 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0859061
I0814 14:00:58.626824 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0859061 (* 1 = 0.0859061 loss)
I0814 14:00:58.626833 28599 sgd_solver.cpp:106] Iteration 27150, lr = 0.0001
I0814 14:01:31.763171 28599 solver.cpp:229] Iteration 27200, loss = 0.267793
I0814 14:01:31.913597 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.138972
I0814 14:01:31.913656 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.264334
I0814 14:01:31.913676 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.133897
I0814 14:01:31.913686 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.133897 (* 1 = 0.133897 loss)
I0814 14:01:31.913698 28599 sgd_solver.cpp:106] Iteration 27200, lr = 0.0001
I0814 14:02:05.090437 28599 solver.cpp:229] Iteration 27250, loss = 0.130977
I0814 14:02:05.240016 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0802108
I0814 14:02:05.240067 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.152504
I0814 14:02:05.240097 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0654884
I0814 14:02:05.240116 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0654884 (* 1 = 0.0654884 loss)
I0814 14:02:05.240124 28599 sgd_solver.cpp:106] Iteration 27250, lr = 0.0001
I0814 14:02:38.244995 28599 solver.cpp:229] Iteration 27300, loss = 0.305645
I0814 14:02:38.393764 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.171637
I0814 14:02:38.393822 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.311447
I0814 14:02:38.393839 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.152822
I0814 14:02:38.393851 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.152822 (* 1 = 0.152822 loss)
I0814 14:02:38.393859 28599 sgd_solver.cpp:106] Iteration 27300, lr = 0.0001
I0814 14:03:11.344413 28599 solver.cpp:229] Iteration 27350, loss = 0.166291
I0814 14:03:11.493033 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0885406
I0814 14:03:11.493100 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.161839
I0814 14:03:11.493121 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0831454
I0814 14:03:11.493166 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0831454 (* 1 = 0.0831454 loss)
I0814 14:03:11.493192 28599 sgd_solver.cpp:106] Iteration 27350, lr = 0.0001
I0814 14:03:44.849853 28599 solver.cpp:229] Iteration 27400, loss = 0.0741878
I0814 14:03:44.998278 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0426728
I0814 14:03:44.998342 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0795325
I0814 14:03:44.998363 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0370939
I0814 14:03:44.998379 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0370939 (* 1 = 0.0370939 loss)
I0814 14:03:44.998390 28599 sgd_solver.cpp:106] Iteration 27400, lr = 0.0001
I0814 14:04:18.716012 28599 solver.cpp:229] Iteration 27450, loss = 0.114113
I0814 14:04:18.865221 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0700632
I0814 14:04:18.865272 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132488
I0814 14:04:18.865296 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0570564
I0814 14:04:18.865316 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0570564 (* 1 = 0.0570564 loss)
I0814 14:04:18.865324 28599 sgd_solver.cpp:106] Iteration 27450, lr = 0.0001
I0814 14:04:51.398092 28599 solver.cpp:456] Snapshotting to binary proto file norm_-0.01_iter_27500.caffemodel
I0814 14:04:51.446447 28599 sgd_solver.cpp:273] Snapshotting solver state to binary proto file norm_-0.01_iter_27500.solverstate
I0814 14:04:51.484513 28599 solver.cpp:338] Iteration 27500, Testing net (#0)
I0814 14:04:51.484578 28599 net.cpp:694] Ignoring source layer CustomData1
I0814 14:04:51.484597 28599 net.cpp:694] Ignoring source layer blob0_CustomData1_0_split
I0814 14:04:51.484602 28599 net.cpp:694] Ignoring source layer blob1_CustomData1_1_split
I0814 14:04:51.484611 28599 net.cpp:694] Ignoring source layer blob2_CustomData1_2_split
I0814 14:04:51.484632 28599 net.cpp:694] Ignoring source layer DummyData1
I0814 14:04:51.484644 28599 net.cpp:694] Ignoring source layer blob9_DummyData1_0_split
I0814 14:14:45.311066 28599 solver.cpp:406]     Test net output #0: down_up_half_loss_real = 0.0614319
I0814 14:14:45.311209 28599 solver.cpp:406]     Test net output #1: down_up_loss_real = 0.118198
I0814 14:14:45.311218 28599 solver.cpp:406]     Test net output #2: zoom_disp_loss0_real = 0.0637525
I0814 14:14:45.311235 28599 solver.cpp:406]     Test net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0637525 (* 1 = 0.0637525 loss)
I0814 14:14:45.472458 28599 solver.cpp:229] Iteration 27500, loss = 0.124419
I0814 14:14:45.629719 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0512158
I0814 14:14:45.629794 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.101664
I0814 14:14:45.629822 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0622097
I0814 14:14:45.629832 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0622097 (* 1 = 0.0622097 loss)
I0814 14:14:45.629842 28599 sgd_solver.cpp:106] Iteration 27500, lr = 0.0001
I0814 14:15:18.487062 28599 solver.cpp:229] Iteration 27550, loss = 0.121667
I0814 14:15:18.636942 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0626168
I0814 14:15:18.637003 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.119998
I0814 14:15:18.637030 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0608336
I0814 14:15:18.637042 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0608336 (* 1 = 0.0608336 loss)
I0814 14:15:18.637048 28599 sgd_solver.cpp:106] Iteration 27550, lr = 0.0001
I0814 14:15:52.047853 28599 solver.cpp:229] Iteration 27600, loss = 0.163029
I0814 14:15:52.204591 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.095185
I0814 14:15:52.204646 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.178925
I0814 14:15:52.204665 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0815144
I0814 14:15:52.204675 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0815144 (* 1 = 0.0815144 loss)
I0814 14:15:52.204684 28599 sgd_solver.cpp:106] Iteration 27600, lr = 0.0001
I0814 14:16:25.382107 28599 solver.cpp:229] Iteration 27650, loss = 0.188284
I0814 14:16:25.535882 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.104119
I0814 14:16:25.535934 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.181482
I0814 14:16:25.535950 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.094142
I0814 14:16:25.535961 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.094142 (* 1 = 0.094142 loss)
I0814 14:16:25.535972 28599 sgd_solver.cpp:106] Iteration 27650, lr = 0.0001
I0814 14:16:59.396879 28599 solver.cpp:229] Iteration 27700, loss = 0.055298
I0814 14:16:59.550606 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0330386
I0814 14:16:59.550659 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0628492
I0814 14:16:59.550680 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0276489
I0814 14:16:59.550691 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0276489 (* 1 = 0.0276489 loss)
I0814 14:16:59.550699 28599 sgd_solver.cpp:106] Iteration 27700, lr = 0.0001
I0814 14:17:32.568678 28599 solver.cpp:229] Iteration 27750, loss = 0.122254
I0814 14:17:32.719382 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0696165
I0814 14:17:32.719439 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.134397
I0814 14:17:32.719457 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.061127
I0814 14:17:32.719468 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.061127 (* 1 = 0.061127 loss)
I0814 14:17:32.719478 28599 sgd_solver.cpp:106] Iteration 27750, lr = 0.0001
I0814 14:18:06.077724 28599 solver.cpp:229] Iteration 27800, loss = 0.162093
I0814 14:18:06.226454 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0809172
I0814 14:18:06.226510 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.154975
I0814 14:18:06.226526 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0810467
I0814 14:18:06.226537 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0810467 (* 1 = 0.0810467 loss)
I0814 14:18:06.226546 28599 sgd_solver.cpp:106] Iteration 27800, lr = 0.0001
I0814 14:18:39.663486 28599 solver.cpp:229] Iteration 27850, loss = 0.151948
I0814 14:18:39.814113 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0823067
I0814 14:18:39.814184 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163485
I0814 14:18:39.814210 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0759739
I0814 14:18:39.814225 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0759739 (* 1 = 0.0759739 loss)
I0814 14:18:39.814237 28599 sgd_solver.cpp:106] Iteration 27850, lr = 0.0001
I0814 14:19:13.054208 28599 solver.cpp:229] Iteration 27900, loss = 0.216809
I0814 14:19:13.203233 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.128269
I0814 14:19:13.203295 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.236326
I0814 14:19:13.203315 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.108404
I0814 14:19:13.203330 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.108404 (* 1 = 0.108404 loss)
I0814 14:19:13.203341 28599 sgd_solver.cpp:106] Iteration 27900, lr = 0.0001
I0814 14:19:46.704186 28599 solver.cpp:229] Iteration 27950, loss = 0.138563
I0814 14:19:46.854032 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0693039
I0814 14:19:46.854086 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.131351
I0814 14:19:46.854101 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0692814
I0814 14:19:46.854115 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0692814 (* 1 = 0.0692814 loss)
I0814 14:19:46.854122 28599 sgd_solver.cpp:106] Iteration 27950, lr = 0.0001
I0814 14:20:20.497292 28599 solver.cpp:229] Iteration 28000, loss = 0.107856
I0814 14:20:20.646751 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0659199
I0814 14:20:20.646809 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.127243
I0814 14:20:20.646827 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0539278
I0814 14:20:20.646839 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0539278 (* 1 = 0.0539278 loss)
I0814 14:20:20.646847 28599 sgd_solver.cpp:106] Iteration 28000, lr = 0.0001
I0814 14:20:53.650234 28599 solver.cpp:229] Iteration 28050, loss = 0.119997
I0814 14:20:53.798535 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0698454
I0814 14:20:53.798596 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.134454
I0814 14:20:53.798616 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0599986
I0814 14:20:53.798629 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0599986 (* 1 = 0.0599986 loss)
I0814 14:20:53.798637 28599 sgd_solver.cpp:106] Iteration 28050, lr = 0.0001
I0814 14:21:27.011220 28599 solver.cpp:229] Iteration 28100, loss = 0.170504
I0814 14:21:27.160082 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.100282
I0814 14:21:27.160150 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.155144
I0814 14:21:27.160169 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0852519
I0814 14:21:27.160183 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0852519 (* 1 = 0.0852519 loss)
I0814 14:21:27.160193 28599 sgd_solver.cpp:106] Iteration 28100, lr = 0.0001
I0814 14:21:59.866796 28599 solver.cpp:229] Iteration 28150, loss = 0.158987
I0814 14:22:00.016041 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0803164
I0814 14:22:00.016104 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.151519
I0814 14:22:00.016124 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0794934
I0814 14:22:00.016135 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0794934 (* 1 = 0.0794934 loss)
I0814 14:22:00.016145 28599 sgd_solver.cpp:106] Iteration 28150, lr = 0.0001
I0814 14:22:32.706441 28599 solver.cpp:229] Iteration 28200, loss = 0.219749
I0814 14:22:32.855810 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0761542
I0814 14:22:32.855865 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.147391
I0814 14:22:32.855882 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.109875
I0814 14:22:32.855893 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.109875 (* 1 = 0.109875 loss)
I0814 14:22:32.855901 28599 sgd_solver.cpp:106] Iteration 28200, lr = 0.0001
I0814 14:23:06.244895 28599 solver.cpp:229] Iteration 28250, loss = 0.143145
I0814 14:23:06.394105 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0926711
I0814 14:23:06.394170 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.173988
I0814 14:23:06.394189 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0715728
I0814 14:23:06.394201 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0715728 (* 1 = 0.0715728 loss)
I0814 14:23:06.394212 28599 sgd_solver.cpp:106] Iteration 28250, lr = 0.0001
I0814 14:23:39.453794 28599 solver.cpp:229] Iteration 28300, loss = 0.178989
I0814 14:23:39.603102 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.109308
I0814 14:23:39.603168 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.209103
I0814 14:23:39.603183 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0894947
I0814 14:23:39.603194 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0894947 (* 1 = 0.0894947 loss)
I0814 14:23:39.603204 28599 sgd_solver.cpp:106] Iteration 28300, lr = 0.0001
I0814 14:24:12.352695 28599 solver.cpp:229] Iteration 28350, loss = 0.154195
I0814 14:24:12.502041 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0808454
I0814 14:24:12.502092 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.143128
I0814 14:24:12.502111 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0770976
I0814 14:24:12.502121 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0770976 (* 1 = 0.0770976 loss)
I0814 14:24:12.502135 28599 sgd_solver.cpp:106] Iteration 28350, lr = 0.0001
I0814 14:24:45.396014 28599 solver.cpp:229] Iteration 28400, loss = 0.273165
I0814 14:24:45.545306 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.171027
I0814 14:24:45.545367 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.318805
I0814 14:24:45.545387 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.136583
I0814 14:24:45.545398 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.136583 (* 1 = 0.136583 loss)
I0814 14:24:45.545410 28599 sgd_solver.cpp:106] Iteration 28400, lr = 0.0001
I0814 14:25:18.878387 28599 solver.cpp:229] Iteration 28450, loss = 0.185954
I0814 14:25:19.027616 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.108385
I0814 14:25:19.027675 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.199884
I0814 14:25:19.027694 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0929772
I0814 14:25:19.027706 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0929772 (* 1 = 0.0929772 loss)
I0814 14:25:19.027719 28599 sgd_solver.cpp:106] Iteration 28450, lr = 0.0001
I0814 14:25:52.441680 28599 solver.cpp:229] Iteration 28500, loss = 0.12299
I0814 14:25:52.590869 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0688191
I0814 14:25:52.590929 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132559
I0814 14:25:52.590947 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0614953
I0814 14:25:52.590960 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0614953 (* 1 = 0.0614953 loss)
I0814 14:25:52.590970 28599 sgd_solver.cpp:106] Iteration 28500, lr = 0.0001
I0814 14:26:25.985293 28599 solver.cpp:229] Iteration 28550, loss = 0.152285
I0814 14:26:26.134240 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0804716
I0814 14:26:26.134301 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.155624
I0814 14:26:26.134320 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0761425
I0814 14:26:26.134335 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0761425 (* 1 = 0.0761425 loss)
I0814 14:26:26.134346 28599 sgd_solver.cpp:106] Iteration 28550, lr = 0.0001
I0814 14:26:59.773468 28599 solver.cpp:229] Iteration 28600, loss = 0.116806
I0814 14:26:59.922219 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0815906
I0814 14:26:59.922277 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.152195
I0814 14:26:59.922294 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0584032
I0814 14:26:59.922307 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0584032 (* 1 = 0.0584032 loss)
I0814 14:26:59.922315 28599 sgd_solver.cpp:106] Iteration 28600, lr = 0.0001
I0814 14:27:32.988379 28599 solver.cpp:229] Iteration 28650, loss = 0.139723
I0814 14:27:33.137159 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0828649
I0814 14:27:33.137223 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.160734
I0814 14:27:33.137243 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0698617
I0814 14:27:33.137257 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0698617 (* 1 = 0.0698617 loss)
I0814 14:27:33.137271 28599 sgd_solver.cpp:106] Iteration 28650, lr = 0.0001
I0814 14:28:06.323279 28599 solver.cpp:229] Iteration 28700, loss = 0.104797
I0814 14:28:06.472192 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0704245
I0814 14:28:06.472249 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.135112
I0814 14:28:06.472268 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0523987
I0814 14:28:06.472280 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0523987 (* 1 = 0.0523987 loss)
I0814 14:28:06.472290 28599 sgd_solver.cpp:106] Iteration 28700, lr = 0.0001
I0814 14:28:39.354796 28599 solver.cpp:229] Iteration 28750, loss = 0.183205
I0814 14:28:39.503751 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.104232
I0814 14:28:39.503813 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.201806
I0814 14:28:39.503832 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0916028
I0814 14:28:39.503844 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0916028 (* 1 = 0.0916028 loss)
I0814 14:28:39.503854 28599 sgd_solver.cpp:106] Iteration 28750, lr = 0.0001
I0814 14:29:11.892421 28599 solver.cpp:229] Iteration 28800, loss = 0.175436
I0814 14:29:12.041419 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.117
I0814 14:29:12.041471 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.221331
I0814 14:29:12.041486 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.087718
I0814 14:29:12.041496 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.087718 (* 1 = 0.087718 loss)
I0814 14:29:12.041503 28599 sgd_solver.cpp:106] Iteration 28800, lr = 0.0001
I0814 14:29:45.118470 28599 solver.cpp:229] Iteration 28850, loss = 0.124538
I0814 14:29:45.266922 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0753172
I0814 14:29:45.266984 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.143963
I0814 14:29:45.267004 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0622694
I0814 14:29:45.267020 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0622694 (* 1 = 0.0622694 loss)
I0814 14:29:45.267032 28599 sgd_solver.cpp:106] Iteration 28850, lr = 0.0001
I0814 14:30:18.635880 28599 solver.cpp:229] Iteration 28900, loss = 0.0868344
I0814 14:30:18.783563 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0576265
I0814 14:30:18.783641 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.111687
I0814 14:30:18.783668 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0434173
I0814 14:30:18.783686 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0434173 (* 1 = 0.0434173 loss)
I0814 14:30:18.783699 28599 sgd_solver.cpp:106] Iteration 28900, lr = 0.0001
I0814 14:30:51.632634 28599 solver.cpp:229] Iteration 28950, loss = 0.255507
I0814 14:30:51.781388 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.15276
I0814 14:30:51.781447 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.280961
I0814 14:30:51.781468 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.127753
I0814 14:30:51.781479 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.127753 (* 1 = 0.127753 loss)
I0814 14:30:51.781491 28599 sgd_solver.cpp:106] Iteration 28950, lr = 0.0001
I0814 14:31:25.015712 28599 solver.cpp:229] Iteration 29000, loss = 0.0884134
I0814 14:31:25.164716 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0516334
I0814 14:31:25.164779 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0987824
I0814 14:31:25.164806 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0442068
I0814 14:31:25.164821 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0442068 (* 1 = 0.0442068 loss)
I0814 14:31:25.164832 28599 sgd_solver.cpp:106] Iteration 29000, lr = 0.0001
I0814 14:31:57.771158 28599 solver.cpp:229] Iteration 29050, loss = 0.159943
I0814 14:31:57.920312 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0881056
I0814 14:31:57.920426 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.164753
I0814 14:31:57.920454 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0799717
I0814 14:31:57.920469 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0799717 (* 1 = 0.0799717 loss)
I0814 14:31:57.920478 28599 sgd_solver.cpp:106] Iteration 29050, lr = 0.0001
I0814 14:32:30.953507 28599 solver.cpp:229] Iteration 29100, loss = 0.249781
I0814 14:32:31.102213 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.10725
I0814 14:32:31.102277 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.207397
I0814 14:32:31.102299 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.124891
I0814 14:32:31.102313 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.124891 (* 1 = 0.124891 loss)
I0814 14:32:31.102324 28599 sgd_solver.cpp:106] Iteration 29100, lr = 0.0001
I0814 14:33:03.893620 28599 solver.cpp:229] Iteration 29150, loss = 0.159297
I0814 14:33:04.042562 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0765583
I0814 14:33:04.042618 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.145663
I0814 14:33:04.042639 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0796486
I0814 14:33:04.042649 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0796486 (* 1 = 0.0796486 loss)
I0814 14:33:04.042687 28599 sgd_solver.cpp:106] Iteration 29150, lr = 0.0001
I0814 14:33:37.137007 28599 solver.cpp:229] Iteration 29200, loss = 0.160737
I0814 14:33:37.286437 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.104886
I0814 14:33:37.286489 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.201459
I0814 14:33:37.286504 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0803686
I0814 14:33:37.286514 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0803686 (* 1 = 0.0803686 loss)
I0814 14:33:37.286520 28599 sgd_solver.cpp:106] Iteration 29200, lr = 0.0001
I0814 14:34:10.146204 28599 solver.cpp:229] Iteration 29250, loss = 0.101755
I0814 14:34:10.294921 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0683843
I0814 14:34:10.294988 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.131584
I0814 14:34:10.295011 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0508774
I0814 14:34:10.295027 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0508774 (* 1 = 0.0508774 loss)
I0814 14:34:10.295037 28599 sgd_solver.cpp:106] Iteration 29250, lr = 0.0001
I0814 14:34:43.702952 28599 solver.cpp:229] Iteration 29300, loss = 0.153466
I0814 14:34:43.851341 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0835765
I0814 14:34:43.851405 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.15911
I0814 14:34:43.851424 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0767329
I0814 14:34:43.851440 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0767329 (* 1 = 0.0767329 loss)
I0814 14:34:43.851451 28599 sgd_solver.cpp:106] Iteration 29300, lr = 0.0001
I0814 14:35:16.477092 28599 solver.cpp:229] Iteration 29350, loss = 0.0969054
I0814 14:35:16.626246 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0632541
I0814 14:35:16.626338 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.120545
I0814 14:35:16.626370 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0484528
I0814 14:35:16.626391 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0484528 (* 1 = 0.0484528 loss)
I0814 14:35:16.626405 28599 sgd_solver.cpp:106] Iteration 29350, lr = 0.0001
I0814 14:35:49.234598 28599 solver.cpp:229] Iteration 29400, loss = 0.124396
I0814 14:35:49.383815 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0778894
I0814 14:35:49.383882 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.14695
I0814 14:35:49.383908 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0621981
I0814 14:35:49.383922 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0621981 (* 1 = 0.0621981 loss)
I0814 14:35:49.383934 28599 sgd_solver.cpp:106] Iteration 29400, lr = 0.0001
I0814 14:36:22.360296 28599 solver.cpp:229] Iteration 29450, loss = 0.0943519
I0814 14:36:22.513469 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.049229
I0814 14:36:22.513527 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0940864
I0814 14:36:22.513551 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0471761
I0814 14:36:22.513559 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0471761 (* 1 = 0.0471761 loss)
I0814 14:36:22.513567 28599 sgd_solver.cpp:106] Iteration 29450, lr = 0.0001
I0814 14:36:56.043268 28599 solver.cpp:229] Iteration 29500, loss = 0.120804
I0814 14:36:56.192788 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0723621
I0814 14:36:56.192849 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.140312
I0814 14:36:56.192876 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0604022
I0814 14:36:56.192886 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0604022 (* 1 = 0.0604022 loss)
I0814 14:36:56.192895 28599 sgd_solver.cpp:106] Iteration 29500, lr = 0.0001
I0814 14:37:29.581938 28599 solver.cpp:229] Iteration 29550, loss = 0.170374
I0814 14:37:29.730160 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.101319
I0814 14:37:29.730221 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.186661
I0814 14:37:29.730237 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0851872
I0814 14:37:29.730248 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0851872 (* 1 = 0.0851872 loss)
I0814 14:37:29.730257 28599 sgd_solver.cpp:106] Iteration 29550, lr = 0.0001
I0814 14:38:02.263181 28599 solver.cpp:229] Iteration 29600, loss = 0.202284
I0814 14:38:02.411942 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.128176
I0814 14:38:02.412001 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.241038
I0814 14:38:02.412024 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.101142
I0814 14:38:02.412070 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.101142 (* 1 = 0.101142 loss)
I0814 14:38:02.412083 28599 sgd_solver.cpp:106] Iteration 29600, lr = 0.0001
I0814 14:38:35.693413 28599 solver.cpp:229] Iteration 29650, loss = 0.120661
I0814 14:38:35.842856 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.079161
I0814 14:38:35.842908 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.153811
I0814 14:38:35.842924 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0603307
I0814 14:38:35.842934 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0603307 (* 1 = 0.0603307 loss)
I0814 14:38:35.842941 28599 sgd_solver.cpp:106] Iteration 29650, lr = 0.0001
I0814 14:39:08.872311 28599 solver.cpp:229] Iteration 29700, loss = 0.0858469
I0814 14:39:09.020300 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0553809
I0814 14:39:09.020352 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.107548
I0814 14:39:09.020370 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0429236
I0814 14:39:09.020380 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0429236 (* 1 = 0.0429236 loss)
I0814 14:39:09.020388 28599 sgd_solver.cpp:106] Iteration 29700, lr = 0.0001
I0814 14:39:42.515125 28599 solver.cpp:229] Iteration 29750, loss = 0.116444
I0814 14:39:42.664491 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0722754
I0814 14:39:42.664553 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.136783
I0814 14:39:42.664573 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0582224
I0814 14:39:42.664587 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0582224 (* 1 = 0.0582224 loss)
I0814 14:39:42.664597 28599 sgd_solver.cpp:106] Iteration 29750, lr = 0.0001
I0814 14:40:15.767899 28599 solver.cpp:229] Iteration 29800, loss = 0.118902
I0814 14:40:15.916977 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0680304
I0814 14:40:15.917050 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129509
I0814 14:40:15.917074 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.059451
I0814 14:40:15.917093 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.059451 (* 1 = 0.059451 loss)
I0814 14:40:15.917104 28599 sgd_solver.cpp:106] Iteration 29800, lr = 0.0001
I0814 14:40:48.790279 28599 solver.cpp:229] Iteration 29850, loss = 0.287995
I0814 14:40:48.939652 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.133312
I0814 14:40:48.939715 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.260353
I0814 14:40:48.939733 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.143998
I0814 14:40:48.939752 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.143998 (* 1 = 0.143998 loss)
I0814 14:40:48.939760 28599 sgd_solver.cpp:106] Iteration 29850, lr = 0.0001
I0814 14:41:22.070299 28599 solver.cpp:229] Iteration 29900, loss = 0.0951566
I0814 14:41:22.219482 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0563834
I0814 14:41:22.219538 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.105831
I0814 14:41:22.219558 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0475785
I0814 14:41:22.219570 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0475785 (* 1 = 0.0475785 loss)
I0814 14:41:22.219579 28599 sgd_solver.cpp:106] Iteration 29900, lr = 0.0001
I0814 14:41:55.191579 28599 solver.cpp:229] Iteration 29950, loss = 0.1696
I0814 14:41:55.341054 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0875786
I0814 14:41:55.341107 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.161106
I0814 14:41:55.341135 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0847999
I0814 14:41:55.341153 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0847999 (* 1 = 0.0847999 loss)
I0814 14:41:55.341162 28599 sgd_solver.cpp:106] Iteration 29950, lr = 0.0001
I0814 14:42:27.750787 28599 solver.cpp:229] Iteration 30000, loss = 0.0784573
I0814 14:42:27.899622 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.045096
I0814 14:42:27.899675 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0908439
I0814 14:42:27.899694 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0392288
I0814 14:42:27.899705 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0392288 (* 1 = 0.0392288 loss)
I0814 14:42:27.899714 28599 sgd_solver.cpp:106] Iteration 30000, lr = 0.0001
I0814 14:43:01.138274 28599 solver.cpp:229] Iteration 30050, loss = 0.130952
I0814 14:43:01.287150 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0735718
I0814 14:43:01.287201 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.138791
I0814 14:43:01.287217 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0654764
I0814 14:43:01.287227 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0654764 (* 1 = 0.0654764 loss)
I0814 14:43:01.287235 28599 sgd_solver.cpp:106] Iteration 30050, lr = 0.0001
I0814 14:43:34.002718 28599 solver.cpp:229] Iteration 30100, loss = 0.196574
I0814 14:43:34.152314 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.128866
I0814 14:43:34.152369 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.239945
I0814 14:43:34.152387 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0982872
I0814 14:43:34.152398 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0982872 (* 1 = 0.0982872 loss)
I0814 14:43:34.152406 28599 sgd_solver.cpp:106] Iteration 30100, lr = 0.0001
I0814 14:44:07.131618 28599 solver.cpp:229] Iteration 30150, loss = 0.209066
I0814 14:44:07.280516 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.132227
I0814 14:44:07.280576 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.250837
I0814 14:44:07.280596 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.104533
I0814 14:44:07.280608 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.104533 (* 1 = 0.104533 loss)
I0814 14:44:07.280617 28599 sgd_solver.cpp:106] Iteration 30150, lr = 0.0001
I0814 14:44:40.007386 28599 solver.cpp:229] Iteration 30200, loss = 0.103921
I0814 14:44:40.156424 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0631713
I0814 14:44:40.156476 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122148
I0814 14:44:40.156497 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0519605
I0814 14:44:40.156512 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0519605 (* 1 = 0.0519605 loss)
I0814 14:44:40.156520 28599 sgd_solver.cpp:106] Iteration 30200, lr = 0.0001
I0814 14:45:13.000217 28599 solver.cpp:229] Iteration 30250, loss = 0.113543
I0814 14:45:13.149329 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0665142
I0814 14:45:13.149441 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.118777
I0814 14:45:13.149487 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0567714
I0814 14:45:13.149508 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0567714 (* 1 = 0.0567714 loss)
I0814 14:45:13.149523 28599 sgd_solver.cpp:106] Iteration 30250, lr = 0.0001
I0814 14:45:45.476671 28599 solver.cpp:229] Iteration 30300, loss = 0.10256
I0814 14:45:45.626157 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0596969
I0814 14:45:45.626211 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.113133
I0814 14:45:45.626229 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0512803
I0814 14:45:45.626240 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0512803 (* 1 = 0.0512803 loss)
I0814 14:45:45.626250 28599 sgd_solver.cpp:106] Iteration 30300, lr = 0.0001
I0814 14:46:19.047650 28599 solver.cpp:229] Iteration 30350, loss = 0.229331
I0814 14:46:19.197147 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.133271
I0814 14:46:19.197203 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.249979
I0814 14:46:19.197221 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.114666
I0814 14:46:19.197237 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.114666 (* 1 = 0.114666 loss)
I0814 14:46:19.197247 28599 sgd_solver.cpp:106] Iteration 30350, lr = 0.0001
I0814 14:46:52.336081 28599 solver.cpp:229] Iteration 30400, loss = 0.216548
I0814 14:46:52.492859 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.14125
I0814 14:46:52.492916 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.269393
I0814 14:46:52.492933 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.108274
I0814 14:46:52.492944 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.108274 (* 1 = 0.108274 loss)
I0814 14:46:52.492951 28599 sgd_solver.cpp:106] Iteration 30400, lr = 0.0001
I0814 14:47:25.458544 28599 solver.cpp:229] Iteration 30450, loss = 0.19203
I0814 14:47:25.607825 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.112045
I0814 14:47:25.607877 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.211389
I0814 14:47:25.607894 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0960151
I0814 14:47:25.607904 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0960151 (* 1 = 0.0960151 loss)
I0814 14:47:25.607914 28599 sgd_solver.cpp:106] Iteration 30450, lr = 0.0001
I0814 14:47:58.726904 28599 solver.cpp:229] Iteration 30500, loss = 0.106727
I0814 14:47:58.875629 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0595873
I0814 14:47:58.875685 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.113572
I0814 14:47:58.875705 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0533634
I0814 14:47:58.875715 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0533634 (* 1 = 0.0533634 loss)
I0814 14:47:58.875723 28599 sgd_solver.cpp:106] Iteration 30500, lr = 0.0001
I0814 14:48:31.751417 28599 solver.cpp:229] Iteration 30550, loss = 0.179449
I0814 14:48:31.900173 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0983839
I0814 14:48:31.900223 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.176249
I0814 14:48:31.900249 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0897245
I0814 14:48:31.900269 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0897245 (* 1 = 0.0897245 loss)
I0814 14:48:31.900279 28599 sgd_solver.cpp:106] Iteration 30550, lr = 0.0001
I0814 14:49:05.197857 28599 solver.cpp:229] Iteration 30600, loss = 0.126531
I0814 14:49:05.347244 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0773005
I0814 14:49:05.347304 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.150928
I0814 14:49:05.347327 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0632654
I0814 14:49:05.347340 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0632654 (* 1 = 0.0632654 loss)
I0814 14:49:05.347349 28599 sgd_solver.cpp:106] Iteration 30600, lr = 0.0001
I0814 14:49:38.065963 28599 solver.cpp:229] Iteration 30650, loss = 0.0979974
I0814 14:49:38.214993 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0605873
I0814 14:49:38.215059 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.118066
I0814 14:49:38.215081 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0489988
I0814 14:49:38.215096 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0489988 (* 1 = 0.0489988 loss)
I0814 14:49:38.215108 28599 sgd_solver.cpp:106] Iteration 30650, lr = 0.0001
I0814 14:50:11.590322 28599 solver.cpp:229] Iteration 30700, loss = 0.122505
I0814 14:50:11.739451 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0718901
I0814 14:50:11.739511 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.126249
I0814 14:50:11.739531 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0612528
I0814 14:50:11.739543 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0612528 (* 1 = 0.0612528 loss)
I0814 14:50:11.739552 28599 sgd_solver.cpp:106] Iteration 30700, lr = 0.0001
I0814 14:50:44.841699 28599 solver.cpp:229] Iteration 30750, loss = 0.140109
I0814 14:50:44.990655 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0766835
I0814 14:50:44.990720 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.145711
I0814 14:50:44.990738 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0700547
I0814 14:50:44.990751 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0700547 (* 1 = 0.0700547 loss)
I0814 14:50:44.990758 28599 sgd_solver.cpp:106] Iteration 30750, lr = 0.0001
I0814 14:51:17.127384 28599 solver.cpp:229] Iteration 30800, loss = 0.162263
I0814 14:51:17.276927 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0880107
I0814 14:51:17.276991 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.16832
I0814 14:51:17.277019 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0811318
I0814 14:51:17.277029 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0811318 (* 1 = 0.0811318 loss)
I0814 14:51:17.277039 28599 sgd_solver.cpp:106] Iteration 30800, lr = 0.0001
I0814 14:51:50.532662 28599 solver.cpp:229] Iteration 30850, loss = 0.0522375
I0814 14:51:50.685382 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0226408
I0814 14:51:50.685506 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0438402
I0814 14:51:50.685551 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0261189
I0814 14:51:50.685570 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0261189 (* 1 = 0.0261189 loss)
I0814 14:51:50.685585 28599 sgd_solver.cpp:106] Iteration 30850, lr = 0.0001
I0814 14:52:24.063395 28599 solver.cpp:229] Iteration 30900, loss = 0.165068
I0814 14:52:24.212416 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0848856
I0814 14:52:24.212471 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.164434
I0814 14:52:24.212491 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0825342
I0814 14:52:24.212503 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0825342 (* 1 = 0.0825342 loss)
I0814 14:52:24.212512 28599 sgd_solver.cpp:106] Iteration 30900, lr = 0.0001
I0814 14:52:57.100719 28599 solver.cpp:229] Iteration 30950, loss = 0.119958
I0814 14:52:57.249449 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0689703
I0814 14:52:57.249518 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.131497
I0814 14:52:57.249543 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0599791
I0814 14:52:57.249564 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0599791 (* 1 = 0.0599791 loss)
I0814 14:52:57.249578 28599 sgd_solver.cpp:106] Iteration 30950, lr = 0.0001
I0814 14:53:30.136584 28599 solver.cpp:229] Iteration 31000, loss = 0.114818
I0814 14:53:30.285614 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0690754
I0814 14:53:30.285662 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.13221
I0814 14:53:30.285676 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.057409
I0814 14:53:30.285686 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.057409 (* 1 = 0.057409 loss)
I0814 14:53:30.285694 28599 sgd_solver.cpp:106] Iteration 31000, lr = 0.0001
I0814 14:54:03.619345 28599 solver.cpp:229] Iteration 31050, loss = 0.175041
I0814 14:54:03.768095 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.113771
I0814 14:54:03.768152 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.213784
I0814 14:54:03.768198 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0875207
I0814 14:54:03.768213 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0875207 (* 1 = 0.0875207 loss)
I0814 14:54:03.768221 28599 sgd_solver.cpp:106] Iteration 31050, lr = 0.0001
I0814 14:54:36.621769 28599 solver.cpp:229] Iteration 31100, loss = 0.12428
I0814 14:54:36.770849 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0722502
I0814 14:54:36.770901 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.139013
I0814 14:54:36.770916 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0621403
I0814 14:54:36.770925 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0621403 (* 1 = 0.0621403 loss)
I0814 14:54:36.770932 28599 sgd_solver.cpp:106] Iteration 31100, lr = 0.0001
I0814 14:55:09.796212 28599 solver.cpp:229] Iteration 31150, loss = 0.135637
I0814 14:55:09.945011 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0800842
I0814 14:55:09.945088 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.158072
I0814 14:55:09.945116 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0678184
I0814 14:55:09.945132 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0678184 (* 1 = 0.0678184 loss)
I0814 14:55:09.945142 28599 sgd_solver.cpp:106] Iteration 31150, lr = 0.0001
I0814 14:55:42.870355 28599 solver.cpp:229] Iteration 31200, loss = 0.12654
I0814 14:55:43.019920 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0730044
I0814 14:55:43.019979 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.145242
I0814 14:55:43.020001 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0632699
I0814 14:55:43.020012 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0632699 (* 1 = 0.0632699 loss)
I0814 14:55:43.020021 28599 sgd_solver.cpp:106] Iteration 31200, lr = 0.0001
I0814 14:56:16.164103 28599 solver.cpp:229] Iteration 31250, loss = 0.0910773
I0814 14:56:16.313467 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0490521
I0814 14:56:16.313529 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0937979
I0814 14:56:16.313555 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0455387
I0814 14:56:16.313565 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0455387 (* 1 = 0.0455387 loss)
I0814 14:56:16.313572 28599 sgd_solver.cpp:106] Iteration 31250, lr = 0.0001
I0814 14:56:49.594452 28599 solver.cpp:229] Iteration 31300, loss = 0.128564
I0814 14:56:49.750263 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0784032
I0814 14:56:49.750329 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.148361
I0814 14:56:49.750353 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0642821
I0814 14:56:49.750371 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0642821 (* 1 = 0.0642821 loss)
I0814 14:56:49.750422 28599 sgd_solver.cpp:106] Iteration 31300, lr = 0.0001
I0814 14:57:22.764947 28599 solver.cpp:229] Iteration 31350, loss = 0.171028
I0814 14:57:22.913892 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0784963
I0814 14:57:22.913954 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.150564
I0814 14:57:22.913971 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0855143
I0814 14:57:22.913982 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0855143 (* 1 = 0.0855143 loss)
I0814 14:57:22.913990 28599 sgd_solver.cpp:106] Iteration 31350, lr = 0.0001
I0814 14:57:57.507221 28599 solver.cpp:229] Iteration 31400, loss = 0.103302
I0814 14:57:57.656257 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0624343
I0814 14:57:57.656311 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.120577
I0814 14:57:57.656325 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0516513
I0814 14:57:57.656335 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0516513 (* 1 = 0.0516513 loss)
I0814 14:57:57.656342 28599 sgd_solver.cpp:106] Iteration 31400, lr = 0.0001
I0814 14:58:31.297873 28599 solver.cpp:229] Iteration 31450, loss = 0.132395
I0814 14:58:31.446889 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0750358
I0814 14:58:31.446947 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.143808
I0814 14:58:31.447006 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0661976
I0814 14:58:31.447023 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0661976 (* 1 = 0.0661976 loss)
I0814 14:58:31.447032 28599 sgd_solver.cpp:106] Iteration 31450, lr = 0.0001
I0814 14:59:04.362293 28599 solver.cpp:229] Iteration 31500, loss = 0.11052
I0814 14:59:04.511795 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0648097
I0814 14:59:04.511844 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122795
I0814 14:59:04.511865 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.05526
I0814 14:59:04.511876 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.05526 (* 1 = 0.05526 loss)
I0814 14:59:04.511884 28599 sgd_solver.cpp:106] Iteration 31500, lr = 0.0001
I0814 14:59:37.455396 28599 solver.cpp:229] Iteration 31550, loss = 0.0966716
I0814 14:59:37.603266 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0599544
I0814 14:59:37.603387 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.114843
I0814 14:59:37.603435 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0483359
I0814 14:59:37.603468 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0483359 (* 1 = 0.0483359 loss)
I0814 14:59:37.603483 28599 sgd_solver.cpp:106] Iteration 31550, lr = 0.0001
I0814 15:00:11.055670 28599 solver.cpp:229] Iteration 31600, loss = 0.130714
I0814 15:00:11.204993 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0740504
I0814 15:00:11.205047 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.145983
I0814 15:00:11.205071 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0653569
I0814 15:00:11.205080 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0653569 (* 1 = 0.0653569 loss)
I0814 15:00:11.205096 28599 sgd_solver.cpp:106] Iteration 31600, lr = 0.0001
I0814 15:00:44.261574 28599 solver.cpp:229] Iteration 31650, loss = 0.253607
I0814 15:00:44.410446 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.16274
I0814 15:00:44.410501 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.315956
I0814 15:00:44.410519 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.126804
I0814 15:00:44.410562 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.126804 (* 1 = 0.126804 loss)
I0814 15:00:44.410574 28599 sgd_solver.cpp:106] Iteration 31650, lr = 0.0001
I0814 15:01:17.315026 28599 solver.cpp:229] Iteration 31700, loss = 0.170286
I0814 15:01:17.464155 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.097587
I0814 15:01:17.464205 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.185317
I0814 15:01:17.464220 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.085143
I0814 15:01:17.464229 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.085143 (* 1 = 0.085143 loss)
I0814 15:01:17.464237 28599 sgd_solver.cpp:106] Iteration 31700, lr = 0.0001
I0814 15:01:50.779103 28599 solver.cpp:229] Iteration 31750, loss = 0.0729876
I0814 15:01:50.931818 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0313045
I0814 15:01:50.931876 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0615178
I0814 15:01:50.931895 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0364939
I0814 15:01:50.931907 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0364939 (* 1 = 0.0364939 loss)
I0814 15:01:50.931916 28599 sgd_solver.cpp:106] Iteration 31750, lr = 0.0001
I0814 15:02:23.861434 28599 solver.cpp:229] Iteration 31800, loss = 0.196172
I0814 15:02:24.010567 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.107013
I0814 15:02:24.010627 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.198446
I0814 15:02:24.010648 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0980862
I0814 15:02:24.010661 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0980862 (* 1 = 0.0980862 loss)
I0814 15:02:24.010671 28599 sgd_solver.cpp:106] Iteration 31800, lr = 0.0001
I0814 15:02:57.987774 28599 solver.cpp:229] Iteration 31850, loss = 0.144336
I0814 15:02:58.137079 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0772134
I0814 15:02:58.137141 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.153108
I0814 15:02:58.137168 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0721681
I0814 15:02:58.137181 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0721681 (* 1 = 0.0721681 loss)
I0814 15:02:58.137190 28599 sgd_solver.cpp:106] Iteration 31850, lr = 0.0001
I0814 15:03:31.133160 28599 solver.cpp:229] Iteration 31900, loss = 0.144075
I0814 15:03:31.282058 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0886822
I0814 15:03:31.282127 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.168564
I0814 15:03:31.282152 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0720376
I0814 15:03:31.282171 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0720376 (* 1 = 0.0720376 loss)
I0814 15:03:31.282181 28599 sgd_solver.cpp:106] Iteration 31900, lr = 0.0001
I0814 15:04:04.337080 28599 solver.cpp:229] Iteration 31950, loss = 0.129799
I0814 15:04:04.486333 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0767926
I0814 15:04:04.486387 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.146986
I0814 15:04:04.486404 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0648995
I0814 15:04:04.486414 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0648995 (* 1 = 0.0648995 loss)
I0814 15:04:04.486423 28599 sgd_solver.cpp:106] Iteration 31950, lr = 0.0001
I0814 15:04:37.739204 28599 solver.cpp:229] Iteration 32000, loss = 0.12022
I0814 15:04:37.888291 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0771491
I0814 15:04:37.888345 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.141653
I0814 15:04:37.888360 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0601101
I0814 15:04:37.888371 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0601101 (* 1 = 0.0601101 loss)
I0814 15:04:37.888381 28599 sgd_solver.cpp:106] Iteration 32000, lr = 0.0001
I0814 15:05:11.575407 28599 solver.cpp:229] Iteration 32050, loss = 0.26313
I0814 15:05:11.724975 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.163489
I0814 15:05:11.725025 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.31385
I0814 15:05:11.725044 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.131565
I0814 15:05:11.725055 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.131565 (* 1 = 0.131565 loss)
I0814 15:05:11.725064 28599 sgd_solver.cpp:106] Iteration 32050, lr = 0.0001
I0814 15:05:45.224702 28599 solver.cpp:229] Iteration 32100, loss = 0.173463
I0814 15:05:45.373659 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0949564
I0814 15:05:45.373721 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.180512
I0814 15:05:45.373741 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0867317
I0814 15:05:45.373756 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0867317 (* 1 = 0.0867317 loss)
I0814 15:05:45.373766 28599 sgd_solver.cpp:106] Iteration 32100, lr = 0.0001
I0814 15:06:19.747835 28599 solver.cpp:229] Iteration 32150, loss = 0.1363
I0814 15:06:19.897241 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0907271
I0814 15:06:19.897306 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.176675
I0814 15:06:19.897328 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0681503
I0814 15:06:19.897343 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0681503 (* 1 = 0.0681503 loss)
I0814 15:06:19.897354 28599 sgd_solver.cpp:106] Iteration 32150, lr = 0.0001
I0814 15:06:53.615546 28599 solver.cpp:229] Iteration 32200, loss = 0.150336
I0814 15:06:53.769161 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0953895
I0814 15:06:53.769225 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.18597
I0814 15:06:53.769245 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0751679
I0814 15:06:53.769260 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0751679 (* 1 = 0.0751679 loss)
I0814 15:06:53.769271 28599 sgd_solver.cpp:106] Iteration 32200, lr = 0.0001
I0814 15:07:27.709380 28599 solver.cpp:229] Iteration 32250, loss = 0.141682
I0814 15:07:27.858157 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0925522
I0814 15:07:27.858225 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.175168
I0814 15:07:27.858248 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0708411
I0814 15:07:27.858263 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0708411 (* 1 = 0.0708411 loss)
I0814 15:07:27.858273 28599 sgd_solver.cpp:106] Iteration 32250, lr = 0.0001
I0814 15:08:02.313485 28599 solver.cpp:229] Iteration 32300, loss = 0.194886
I0814 15:08:02.462916 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.126383
I0814 15:08:02.462975 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.245744
I0814 15:08:02.463003 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0974432
I0814 15:08:02.463013 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0974432 (* 1 = 0.0974432 loss)
I0814 15:08:02.463021 28599 sgd_solver.cpp:106] Iteration 32300, lr = 0.0001
I0814 15:08:34.745956 28599 solver.cpp:229] Iteration 32350, loss = 0.146235
I0814 15:08:34.895347 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0879985
I0814 15:08:34.895398 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.169737
I0814 15:08:34.895432 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0731177
I0814 15:08:34.895442 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0731177 (* 1 = 0.0731177 loss)
I0814 15:08:34.895449 28599 sgd_solver.cpp:106] Iteration 32350, lr = 0.0001
I0814 15:09:07.170426 28599 solver.cpp:229] Iteration 32400, loss = 0.179823
I0814 15:09:07.317178 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.114769
I0814 15:09:07.317260 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.220846
I0814 15:09:07.317291 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0899116
I0814 15:09:07.317309 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0899116 (* 1 = 0.0899116 loss)
I0814 15:09:07.317323 28599 sgd_solver.cpp:106] Iteration 32400, lr = 0.0001
I0814 15:09:40.818205 28599 solver.cpp:229] Iteration 32450, loss = 0.150311
I0814 15:09:40.967061 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0897256
I0814 15:09:40.967115 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.176447
I0814 15:09:40.967131 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0751556
I0814 15:09:40.967144 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0751556 (* 1 = 0.0751556 loss)
I0814 15:09:40.967151 28599 sgd_solver.cpp:106] Iteration 32450, lr = 0.0001
I0814 15:10:14.070981 28599 solver.cpp:229] Iteration 32500, loss = 0.101014
I0814 15:10:14.220391 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0619583
I0814 15:10:14.220445 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.121181
I0814 15:10:14.220461 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0505072
I0814 15:10:14.220473 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0505072 (* 1 = 0.0505072 loss)
I0814 15:10:14.220480 28599 sgd_solver.cpp:106] Iteration 32500, lr = 0.0001
I0814 15:10:46.918929 28599 solver.cpp:229] Iteration 32550, loss = 0.119194
I0814 15:10:47.068482 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0792752
I0814 15:10:47.068542 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.148527
I0814 15:10:47.068560 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0595969
I0814 15:10:47.068577 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0595969 (* 1 = 0.0595969 loss)
I0814 15:10:47.068586 28599 sgd_solver.cpp:106] Iteration 32550, lr = 0.0001
I0814 15:11:20.535941 28599 solver.cpp:229] Iteration 32600, loss = 0.160797
I0814 15:11:20.684691 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0965181
I0814 15:11:20.684767 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.1816
I0814 15:11:20.684799 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0803987
I0814 15:11:20.684816 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0803987 (* 1 = 0.0803987 loss)
I0814 15:11:20.684834 28599 sgd_solver.cpp:106] Iteration 32600, lr = 0.0001
I0814 15:11:54.117463 28599 solver.cpp:229] Iteration 32650, loss = 0.171483
I0814 15:11:54.265895 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.114837
I0814 15:11:54.265956 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.223424
I0814 15:11:54.265982 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0857416
I0814 15:11:54.265995 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0857416 (* 1 = 0.0857416 loss)
I0814 15:11:54.266002 28599 sgd_solver.cpp:106] Iteration 32650, lr = 0.0001
I0814 15:12:28.838002 28599 solver.cpp:229] Iteration 32700, loss = 0.233056
I0814 15:12:28.986196 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.132387
I0814 15:12:28.986307 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.255168
I0814 15:12:28.986344 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.116528
I0814 15:12:28.986361 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.116528 (* 1 = 0.116528 loss)
I0814 15:12:28.986377 28599 sgd_solver.cpp:106] Iteration 32700, lr = 0.0001
I0814 15:13:02.782860 28599 solver.cpp:229] Iteration 32750, loss = 0.137537
I0814 15:13:02.931638 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.082786
I0814 15:13:02.931704 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.157916
I0814 15:13:02.931730 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0687689
I0814 15:13:02.931744 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0687689 (* 1 = 0.0687689 loss)
I0814 15:13:02.931754 28599 sgd_solver.cpp:106] Iteration 32750, lr = 0.0001
I0814 15:13:37.074955 28599 solver.cpp:229] Iteration 32800, loss = 0.23084
I0814 15:13:37.223850 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.134962
I0814 15:13:37.223912 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.246315
I0814 15:13:37.223934 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11542
I0814 15:13:37.223987 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11542 (* 1 = 0.11542 loss)
I0814 15:13:37.224017 28599 sgd_solver.cpp:106] Iteration 32800, lr = 0.0001
I0814 15:14:11.516279 28599 solver.cpp:229] Iteration 32850, loss = 0.231544
I0814 15:14:11.665210 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.109749
I0814 15:14:11.665277 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.19648
I0814 15:14:11.665299 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.115772
I0814 15:14:11.665321 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.115772 (* 1 = 0.115772 loss)
I0814 15:14:11.665333 28599 sgd_solver.cpp:106] Iteration 32850, lr = 0.0001
I0814 15:14:44.600668 28599 solver.cpp:229] Iteration 32900, loss = 0.110528
I0814 15:14:44.750116 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.069568
I0814 15:14:44.750171 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.132286
I0814 15:14:44.750196 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0552644
I0814 15:14:44.750214 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0552644 (* 1 = 0.0552644 loss)
I0814 15:14:44.750222 28599 sgd_solver.cpp:106] Iteration 32900, lr = 0.0001
I0814 15:15:18.113482 28599 solver.cpp:229] Iteration 32950, loss = 0.106996
I0814 15:15:18.262020 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0648069
I0814 15:15:18.262075 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.126111
I0814 15:15:18.262094 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0534982
I0814 15:15:18.262107 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0534982 (* 1 = 0.0534982 loss)
I0814 15:15:18.262117 28599 sgd_solver.cpp:106] Iteration 32950, lr = 0.0001
I0814 15:15:51.040128 28599 solver.cpp:456] Snapshotting to binary proto file norm_-0.01_iter_33000.caffemodel
I0814 15:15:51.076406 28599 sgd_solver.cpp:273] Snapshotting solver state to binary proto file norm_-0.01_iter_33000.solverstate
I0814 15:15:51.110131 28599 solver.cpp:338] Iteration 33000, Testing net (#0)
I0814 15:15:51.110164 28599 net.cpp:694] Ignoring source layer CustomData1
I0814 15:15:51.110174 28599 net.cpp:694] Ignoring source layer blob0_CustomData1_0_split
I0814 15:15:51.110183 28599 net.cpp:694] Ignoring source layer blob1_CustomData1_1_split
I0814 15:15:51.110193 28599 net.cpp:694] Ignoring source layer blob2_CustomData1_2_split
I0814 15:15:51.110210 28599 net.cpp:694] Ignoring source layer DummyData1
I0814 15:15:51.110222 28599 net.cpp:694] Ignoring source layer blob9_DummyData1_0_split
I0814 15:25:49.869525 28599 solver.cpp:406]     Test net output #0: down_up_half_loss_real = 0.0614319
I0814 15:25:49.869637 28599 solver.cpp:406]     Test net output #1: down_up_loss_real = 0.118198
I0814 15:25:49.869652 28599 solver.cpp:406]     Test net output #2: zoom_disp_loss0_real = 0.0472447
I0814 15:25:49.869659 28599 solver.cpp:406]     Test net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0472447 (* 1 = 0.0472447 loss)
I0814 15:25:50.023047 28599 solver.cpp:229] Iteration 33000, loss = 0.0953118
I0814 15:25:50.175889 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0606467
I0814 15:25:50.175931 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.117263
I0814 15:25:50.175947 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.047656
I0814 15:25:50.175957 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.047656 (* 1 = 0.047656 loss)
I0814 15:25:50.175977 28599 sgd_solver.cpp:106] Iteration 33000, lr = 0.0001
I0814 15:26:23.456915 28599 solver.cpp:229] Iteration 33050, loss = 0.155964
I0814 15:26:23.606232 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0956416
I0814 15:26:23.606314 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.185002
I0814 15:26:23.606338 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.077982
I0814 15:26:23.606348 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.077982 (* 1 = 0.077982 loss)
I0814 15:26:23.606355 28599 sgd_solver.cpp:106] Iteration 33050, lr = 0.0001
I0814 15:26:57.009981 28599 solver.cpp:229] Iteration 33100, loss = 0.105721
I0814 15:26:57.163805 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0663554
I0814 15:26:57.163861 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.127387
I0814 15:26:57.163877 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0528607
I0814 15:26:57.163885 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0528607 (* 1 = 0.0528607 loss)
I0814 15:26:57.163895 28599 sgd_solver.cpp:106] Iteration 33100, lr = 0.0001
I0814 15:27:30.738509 28599 solver.cpp:229] Iteration 33150, loss = 0.245264
I0814 15:27:30.894183 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.126537
I0814 15:27:30.894232 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.221521
I0814 15:27:30.894246 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.122632
I0814 15:27:30.894263 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.122632 (* 1 = 0.122632 loss)
I0814 15:27:30.894282 28599 sgd_solver.cpp:106] Iteration 33150, lr = 0.0001
I0814 15:28:03.642668 28599 solver.cpp:229] Iteration 33200, loss = 0.118902
I0814 15:28:03.796519 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0394437
I0814 15:28:03.796568 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0755846
I0814 15:28:03.796584 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0594513
I0814 15:28:03.796593 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0594513 (* 1 = 0.0594513 loss)
I0814 15:28:03.796602 28599 sgd_solver.cpp:106] Iteration 33200, lr = 0.0001
I0814 15:28:36.882360 28599 solver.cpp:229] Iteration 33250, loss = 0.16571
I0814 15:28:37.034847 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0955584
I0814 15:28:37.034903 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.18123
I0814 15:28:37.034920 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0828553
I0814 15:28:37.034931 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0828553 (* 1 = 0.0828553 loss)
I0814 15:28:37.034940 28599 sgd_solver.cpp:106] Iteration 33250, lr = 0.0001
I0814 15:29:10.254087 28599 solver.cpp:229] Iteration 33300, loss = 0.117592
I0814 15:29:10.404515 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0737043
I0814 15:29:10.404570 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.138936
I0814 15:29:10.404588 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.058796
I0814 15:29:10.404619 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.058796 (* 1 = 0.058796 loss)
I0814 15:29:10.404629 28599 sgd_solver.cpp:106] Iteration 33300, lr = 0.0001
I0814 15:29:43.653861 28599 solver.cpp:229] Iteration 33350, loss = 0.109553
I0814 15:29:43.803485 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0629434
I0814 15:29:43.803544 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122513
I0814 15:29:43.803568 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0547765
I0814 15:29:43.803580 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0547765 (* 1 = 0.0547765 loss)
I0814 15:29:43.803589 28599 sgd_solver.cpp:106] Iteration 33350, lr = 0.0001
I0814 15:30:16.950003 28599 solver.cpp:229] Iteration 33400, loss = 0.220242
I0814 15:30:17.099021 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.118478
I0814 15:30:17.099071 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.226055
I0814 15:30:17.099093 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.110121
I0814 15:30:17.099112 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.110121 (* 1 = 0.110121 loss)
I0814 15:30:17.099118 28599 sgd_solver.cpp:106] Iteration 33400, lr = 0.0001
I0814 15:30:50.304648 28599 solver.cpp:229] Iteration 33450, loss = 0.187477
I0814 15:30:50.453460 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.107386
I0814 15:30:50.453519 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.204345
I0814 15:30:50.453538 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0937385
I0814 15:30:50.453548 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0937385 (* 1 = 0.0937385 loss)
I0814 15:30:50.453559 28599 sgd_solver.cpp:106] Iteration 33450, lr = 0.0001
I0814 15:31:23.549260 28599 solver.cpp:229] Iteration 33500, loss = 0.11279
I0814 15:31:23.698456 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0521645
I0814 15:31:23.698508 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0973769
I0814 15:31:23.698526 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0563951
I0814 15:31:23.698536 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0563951 (* 1 = 0.0563951 loss)
I0814 15:31:23.698544 28599 sgd_solver.cpp:106] Iteration 33500, lr = 0.0001
I0814 15:31:57.157351 28599 solver.cpp:229] Iteration 33550, loss = 0.141105
I0814 15:31:57.306471 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0780647
I0814 15:31:57.306540 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.14866
I0814 15:31:57.306561 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0705525
I0814 15:31:57.306572 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0705525 (* 1 = 0.0705525 loss)
I0814 15:31:57.306581 28599 sgd_solver.cpp:106] Iteration 33550, lr = 0.0001
I0814 15:32:29.966852 28599 solver.cpp:229] Iteration 33600, loss = 0.174915
I0814 15:32:30.116602 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.10065
I0814 15:32:30.116652 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.17696
I0814 15:32:30.116669 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0874575
I0814 15:32:30.116680 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0874575 (* 1 = 0.0874575 loss)
I0814 15:32:30.116689 28599 sgd_solver.cpp:106] Iteration 33600, lr = 0.0001
I0814 15:33:03.016469 28599 solver.cpp:229] Iteration 33650, loss = 0.167777
I0814 15:33:03.165488 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0980921
I0814 15:33:03.165541 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.187854
I0814 15:33:03.165560 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0838888
I0814 15:33:03.165570 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0838888 (* 1 = 0.0838888 loss)
I0814 15:33:03.165577 28599 sgd_solver.cpp:106] Iteration 33650, lr = 0.0001
I0814 15:33:36.329571 28599 solver.cpp:229] Iteration 33700, loss = 0.116228
I0814 15:33:36.478653 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0749854
I0814 15:33:36.478708 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.145071
I0814 15:33:36.478726 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0581142
I0814 15:33:36.478735 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0581142 (* 1 = 0.0581142 loss)
I0814 15:33:36.478744 28599 sgd_solver.cpp:106] Iteration 33700, lr = 0.0001
I0814 15:34:09.589092 28599 solver.cpp:229] Iteration 33750, loss = 0.242448
I0814 15:34:09.738596 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.144224
I0814 15:34:09.738664 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.276207
I0814 15:34:09.738696 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.121224
I0814 15:34:09.738711 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.121224 (* 1 = 0.121224 loss)
I0814 15:34:09.738730 28599 sgd_solver.cpp:106] Iteration 33750, lr = 0.0001
I0814 15:34:43.880352 28599 solver.cpp:229] Iteration 33800, loss = 0.173616
I0814 15:34:44.029165 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.10059
I0814 15:34:44.029228 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.191855
I0814 15:34:44.029248 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0868081
I0814 15:34:44.029261 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0868081 (* 1 = 0.0868081 loss)
I0814 15:34:44.029270 28599 sgd_solver.cpp:106] Iteration 33800, lr = 0.0001
I0814 15:35:18.401361 28599 solver.cpp:229] Iteration 33850, loss = 0.126186
I0814 15:35:18.550268 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0761801
I0814 15:35:18.550382 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.142456
I0814 15:35:18.550424 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0630931
I0814 15:35:18.550451 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0630931 (* 1 = 0.0630931 loss)
I0814 15:35:18.550468 28599 sgd_solver.cpp:106] Iteration 33850, lr = 0.0001
I0814 15:35:50.896798 28599 solver.cpp:229] Iteration 33900, loss = 0.237351
I0814 15:35:51.046162 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.155011
I0814 15:35:51.046214 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.294323
I0814 15:35:51.046252 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.118676
I0814 15:35:51.046262 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.118676 (* 1 = 0.118676 loss)
I0814 15:35:51.046272 28599 sgd_solver.cpp:106] Iteration 33900, lr = 0.0001
I0814 15:36:23.907467 28599 solver.cpp:229] Iteration 33950, loss = 0.160785
I0814 15:36:24.056212 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0863432
I0814 15:36:24.056264 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.162943
I0814 15:36:24.056288 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0803926
I0814 15:36:24.056306 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0803926 (* 1 = 0.0803926 loss)
I0814 15:36:24.056313 28599 sgd_solver.cpp:106] Iteration 33950, lr = 0.0001
I0814 15:36:57.083629 28599 solver.cpp:229] Iteration 34000, loss = 0.178076
I0814 15:36:57.233021 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.101837
I0814 15:36:57.233070 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.176762
I0814 15:36:57.233088 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0890383
I0814 15:36:57.233098 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0890383 (* 1 = 0.0890383 loss)
I0814 15:36:57.233106 28599 sgd_solver.cpp:106] Iteration 34000, lr = 0.0001
I0814 15:37:30.385992 28599 solver.cpp:229] Iteration 34050, loss = 0.0760018
I0814 15:37:30.536366 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0469578
I0814 15:37:30.536432 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0920837
I0814 15:37:30.536454 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0380011
I0814 15:37:30.536473 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0380011 (* 1 = 0.0380011 loss)
I0814 15:37:30.536484 28599 sgd_solver.cpp:106] Iteration 34050, lr = 0.0001
I0814 15:38:03.694844 28599 solver.cpp:229] Iteration 34100, loss = 0.125787
I0814 15:38:03.843763 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0755422
I0814 15:38:03.843830 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.14218
I0814 15:38:03.843852 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0628935
I0814 15:38:03.843868 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0628935 (* 1 = 0.0628935 loss)
I0814 15:38:03.843880 28599 sgd_solver.cpp:106] Iteration 34100, lr = 0.0001
I0814 15:38:37.738111 28599 solver.cpp:229] Iteration 34150, loss = 0.217012
I0814 15:38:37.887105 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.124147
I0814 15:38:37.887163 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.240788
I0814 15:38:37.887182 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.108506
I0814 15:38:37.887193 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.108506 (* 1 = 0.108506 loss)
I0814 15:38:37.887202 28599 sgd_solver.cpp:106] Iteration 34150, lr = 0.0001
I0814 15:39:11.162398 28599 solver.cpp:229] Iteration 34200, loss = 0.126725
I0814 15:39:11.311583 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0789709
I0814 15:39:11.311635 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.152874
I0814 15:39:11.311652 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0633627
I0814 15:39:11.311663 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0633627 (* 1 = 0.0633627 loss)
I0814 15:39:11.311672 28599 sgd_solver.cpp:106] Iteration 34200, lr = 0.0001
I0814 15:39:45.391656 28599 solver.cpp:229] Iteration 34250, loss = 0.180321
I0814 15:39:45.540472 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.114329
I0814 15:39:45.540537 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.216088
I0814 15:39:45.540557 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0901606
I0814 15:39:45.540571 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0901606 (* 1 = 0.0901606 loss)
I0814 15:39:45.540583 28599 sgd_solver.cpp:106] Iteration 34250, lr = 0.0001
I0814 15:40:19.531817 28599 solver.cpp:229] Iteration 34300, loss = 0.109103
I0814 15:40:19.680986 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.07305
I0814 15:40:19.681047 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.138455
I0814 15:40:19.681068 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0545516
I0814 15:40:19.681084 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0545516 (* 1 = 0.0545516 loss)
I0814 15:40:19.681095 28599 sgd_solver.cpp:106] Iteration 34300, lr = 0.0001
I0814 15:40:53.302561 28599 solver.cpp:229] Iteration 34350, loss = 0.184817
I0814 15:40:53.451503 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.102356
I0814 15:40:53.451555 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.18753
I0814 15:40:53.451578 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0924086
I0814 15:40:53.451598 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0924086 (* 1 = 0.0924086 loss)
I0814 15:40:53.451606 28599 sgd_solver.cpp:106] Iteration 34350, lr = 0.0001
I0814 15:41:25.925984 28599 solver.cpp:229] Iteration 34400, loss = 0.107983
I0814 15:41:26.074693 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0624268
I0814 15:41:26.074795 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.120952
I0814 15:41:26.074831 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0539916
I0814 15:41:26.074853 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0539916 (* 1 = 0.0539916 loss)
I0814 15:41:26.074872 28599 sgd_solver.cpp:106] Iteration 34400, lr = 0.0001
I0814 15:41:59.586294 28599 solver.cpp:229] Iteration 34450, loss = 0.157184
I0814 15:41:59.735018 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.101265
I0814 15:41:59.735085 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.187493
I0814 15:41:59.735107 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0785924
I0814 15:41:59.735122 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0785924 (* 1 = 0.0785924 loss)
I0814 15:41:59.735133 28599 sgd_solver.cpp:106] Iteration 34450, lr = 0.0001
I0814 15:42:32.786764 28599 solver.cpp:229] Iteration 34500, loss = 0.139618
I0814 15:42:32.935662 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0873482
I0814 15:42:32.935729 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.165889
I0814 15:42:32.935755 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0698091
I0814 15:42:32.935770 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0698091 (* 1 = 0.0698091 loss)
I0814 15:42:32.935780 28599 sgd_solver.cpp:106] Iteration 34500, lr = 0.0001
I0814 15:43:06.275337 28599 solver.cpp:229] Iteration 34550, loss = 0.18933
I0814 15:43:06.424180 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.108759
I0814 15:43:06.424234 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.203179
I0814 15:43:06.424250 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0946651
I0814 15:43:06.424260 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0946651 (* 1 = 0.0946651 loss)
I0814 15:43:06.424268 28599 sgd_solver.cpp:106] Iteration 34550, lr = 0.0001
I0814 15:43:39.286130 28599 solver.cpp:229] Iteration 34600, loss = 0.245485
I0814 15:43:39.435156 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.156429
I0814 15:43:39.435220 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.308753
I0814 15:43:39.435241 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.122743
I0814 15:43:39.435258 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.122743 (* 1 = 0.122743 loss)
I0814 15:43:39.435269 28599 sgd_solver.cpp:106] Iteration 34600, lr = 0.0001
I0814 15:44:12.097779 28599 solver.cpp:229] Iteration 34650, loss = 0.102945
I0814 15:44:12.247062 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0544847
I0814 15:44:12.247112 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.105107
I0814 15:44:12.247125 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0514724
I0814 15:44:12.247148 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0514724 (* 1 = 0.0514724 loss)
I0814 15:44:12.247156 28599 sgd_solver.cpp:106] Iteration 34650, lr = 0.0001
I0814 15:44:45.478842 28599 solver.cpp:229] Iteration 34700, loss = 0.147338
I0814 15:44:45.628149 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0950497
I0814 15:44:45.628201 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.183297
I0814 15:44:45.628218 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0736692
I0814 15:44:45.628229 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0736692 (* 1 = 0.0736692 loss)
I0814 15:44:45.628238 28599 sgd_solver.cpp:106] Iteration 34700, lr = 0.0001
I0814 15:45:18.791491 28599 solver.cpp:229] Iteration 34750, loss = 0.100519
I0814 15:45:18.951373 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.059938
I0814 15:45:18.951437 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.115472
I0814 15:45:18.951458 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0502597
I0814 15:45:18.951470 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0502597 (* 1 = 0.0502597 loss)
I0814 15:45:18.951479 28599 sgd_solver.cpp:106] Iteration 34750, lr = 0.0001
I0814 15:45:52.765722 28599 solver.cpp:229] Iteration 34800, loss = 0.122422
I0814 15:45:52.916054 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0707868
I0814 15:45:52.916147 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.133674
I0814 15:45:52.916179 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0612112
I0814 15:45:52.916199 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0612112 (* 1 = 0.0612112 loss)
I0814 15:45:52.916213 28599 sgd_solver.cpp:106] Iteration 34800, lr = 0.0001
I0814 15:46:26.109143 28599 solver.cpp:229] Iteration 34850, loss = 0.110915
I0814 15:46:26.259387 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.057593
I0814 15:46:26.259441 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.111567
I0814 15:46:26.259460 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0554578
I0814 15:46:26.259474 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0554578 (* 1 = 0.0554578 loss)
I0814 15:46:26.259481 28599 sgd_solver.cpp:106] Iteration 34850, lr = 0.0001
I0814 15:46:59.511631 28599 solver.cpp:229] Iteration 34900, loss = 0.0928592
I0814 15:46:59.660069 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0585523
I0814 15:46:59.660140 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.109846
I0814 15:46:59.660162 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0464298
I0814 15:46:59.660178 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0464298 (* 1 = 0.0464298 loss)
I0814 15:46:59.660189 28599 sgd_solver.cpp:106] Iteration 34900, lr = 0.0001
I0814 15:47:32.344180 28599 solver.cpp:229] Iteration 34950, loss = 0.064175
I0814 15:47:32.501513 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0418311
I0814 15:47:32.501641 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0802861
I0814 15:47:32.501749 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0320877
I0814 15:47:32.501780 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0320877 (* 1 = 0.0320877 loss)
I0814 15:47:32.501801 28599 sgd_solver.cpp:106] Iteration 34950, lr = 0.0001
I0814 15:48:05.812525 28599 solver.cpp:229] Iteration 35000, loss = 0.130377
I0814 15:48:05.970800 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.067946
I0814 15:48:05.970875 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.127278
I0814 15:48:05.970903 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0651886
I0814 15:48:05.970919 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0651886 (* 1 = 0.0651886 loss)
I0814 15:48:05.970932 28599 sgd_solver.cpp:106] Iteration 35000, lr = 0.0001
I0814 15:48:38.643263 28599 solver.cpp:229] Iteration 35050, loss = 0.125823
I0814 15:48:38.793293 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.078465
I0814 15:48:38.793347 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.148274
I0814 15:48:38.793362 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0629116
I0814 15:48:38.793371 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0629116 (* 1 = 0.0629116 loss)
I0814 15:48:38.793378 28599 sgd_solver.cpp:106] Iteration 35050, lr = 0.0001
I0814 15:49:11.525193 28599 solver.cpp:229] Iteration 35100, loss = 0.180583
I0814 15:49:11.677109 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.119516
I0814 15:49:11.677191 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.22946
I0814 15:49:11.677224 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0902917
I0814 15:49:11.677243 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0902917 (* 1 = 0.0902917 loss)
I0814 15:49:11.677255 28599 sgd_solver.cpp:106] Iteration 35100, lr = 0.0001
I0814 15:49:44.927322 28599 solver.cpp:229] Iteration 35150, loss = 0.124134
I0814 15:49:45.079113 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0810495
I0814 15:49:45.079176 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.156062
I0814 15:49:45.079196 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0620673
I0814 15:49:45.079211 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0620673 (* 1 = 0.0620673 loss)
I0814 15:49:45.079222 28599 sgd_solver.cpp:106] Iteration 35150, lr = 0.0001
I0814 15:50:18.063390 28599 solver.cpp:229] Iteration 35200, loss = 0.11603
I0814 15:50:18.212035 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0611105
I0814 15:50:18.212091 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.119829
I0814 15:50:18.212107 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0580151
I0814 15:50:18.212139 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0580151 (* 1 = 0.0580151 loss)
I0814 15:50:18.212148 28599 sgd_solver.cpp:106] Iteration 35200, lr = 0.0001
I0814 15:50:51.531301 28599 solver.cpp:229] Iteration 35250, loss = 0.115975
I0814 15:50:51.681082 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0546137
I0814 15:50:51.681138 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.102711
I0814 15:50:51.681154 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0579874
I0814 15:50:51.681162 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0579874 (* 1 = 0.0579874 loss)
I0814 15:50:51.681169 28599 sgd_solver.cpp:106] Iteration 35250, lr = 0.0001
I0814 15:51:24.375299 28599 solver.cpp:229] Iteration 35300, loss = 0.108972
I0814 15:51:24.524416 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0675683
I0814 15:51:24.524479 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.128759
I0814 15:51:24.524502 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.054486
I0814 15:51:24.524515 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.054486 (* 1 = 0.054486 loss)
I0814 15:51:24.524530 28599 sgd_solver.cpp:106] Iteration 35300, lr = 0.0001
I0814 15:51:58.730633 28599 solver.cpp:229] Iteration 35350, loss = 0.22462
I0814 15:51:58.880228 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.122338
I0814 15:51:58.880308 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.236482
I0814 15:51:58.880336 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11231
I0814 15:51:58.880365 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11231 (* 1 = 0.11231 loss)
I0814 15:51:58.880384 28599 sgd_solver.cpp:106] Iteration 35350, lr = 0.0001
I0814 15:52:32.610891 28599 solver.cpp:229] Iteration 35400, loss = 0.109901
I0814 15:52:32.763197 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0691399
I0814 15:52:32.763259 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129981
I0814 15:52:32.763283 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0549508
I0814 15:52:32.763296 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0549508 (* 1 = 0.0549508 loss)
I0814 15:52:32.763304 28599 sgd_solver.cpp:106] Iteration 35400, lr = 0.0001
I0814 15:53:07.339479 28599 solver.cpp:229] Iteration 35450, loss = 0.11394
I0814 15:53:07.493681 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0662162
I0814 15:53:07.493753 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122795
I0814 15:53:07.493780 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0569702
I0814 15:53:07.493800 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0569702 (* 1 = 0.0569702 loss)
I0814 15:53:07.493813 28599 sgd_solver.cpp:106] Iteration 35450, lr = 0.0001
I0814 15:53:43.876057 28599 solver.cpp:229] Iteration 35500, loss = 0.116728
I0814 15:53:44.031812 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0591744
I0814 15:53:44.031910 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.114267
I0814 15:53:44.031949 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0583642
I0814 15:53:44.031975 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0583642 (* 1 = 0.0583642 loss)
I0814 15:53:44.031994 28599 sgd_solver.cpp:106] Iteration 35500, lr = 0.0001
I0814 15:54:18.097522 28599 solver.cpp:229] Iteration 35550, loss = 0.126679
I0814 15:54:18.244858 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.06844
I0814 15:54:18.244972 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.130747
I0814 15:54:18.245024 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0633394
I0814 15:54:18.245059 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0633394 (* 1 = 0.0633394 loss)
I0814 15:54:18.245085 28599 sgd_solver.cpp:106] Iteration 35550, lr = 0.0001
I0814 15:54:54.210368 28599 solver.cpp:229] Iteration 35600, loss = 0.196823
I0814 15:54:54.359454 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.127349
I0814 15:54:54.359534 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.238616
I0814 15:54:54.359563 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0984117
I0814 15:54:54.359582 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0984117 (* 1 = 0.0984117 loss)
I0814 15:54:54.359596 28599 sgd_solver.cpp:106] Iteration 35600, lr = 0.0001
I0814 15:55:29.305649 28599 solver.cpp:229] Iteration 35650, loss = 0.201819
I0814 15:55:29.454324 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.114998
I0814 15:55:29.454393 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.218695
I0814 15:55:29.454421 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.10091
I0814 15:55:29.454438 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.10091 (* 1 = 0.10091 loss)
I0814 15:55:29.454450 28599 sgd_solver.cpp:106] Iteration 35650, lr = 0.0001
I0814 15:56:03.821465 28599 solver.cpp:229] Iteration 35700, loss = 0.151592
I0814 15:56:03.969105 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0856274
I0814 15:56:03.969202 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.163321
I0814 15:56:03.969235 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0757962
I0814 15:56:03.969259 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0757962 (* 1 = 0.0757962 loss)
I0814 15:56:03.969275 28599 sgd_solver.cpp:106] Iteration 35700, lr = 0.0001
I0814 15:56:38.329922 28599 solver.cpp:229] Iteration 35750, loss = 0.143406
I0814 15:56:38.477192 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0881377
I0814 15:56:38.477264 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.158907
I0814 15:56:38.477289 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.071703
I0814 15:56:38.477308 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.071703 (* 1 = 0.071703 loss)
I0814 15:56:38.477320 28599 sgd_solver.cpp:106] Iteration 35750, lr = 0.0001
I0814 15:57:13.456583 28599 solver.cpp:229] Iteration 35800, loss = 0.0936016
I0814 15:57:13.605854 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0542349
I0814 15:57:13.605914 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.102678
I0814 15:57:13.605939 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0468009
I0814 15:57:13.605954 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0468009 (* 1 = 0.0468009 loss)
I0814 15:57:13.605967 28599 sgd_solver.cpp:106] Iteration 35800, lr = 0.0001
I0814 15:57:48.354038 28599 solver.cpp:229] Iteration 35850, loss = 0.210074
I0814 15:57:48.501266 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.130016
I0814 15:57:48.501343 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.244616
I0814 15:57:48.501376 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.105037
I0814 15:57:48.501396 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.105037 (* 1 = 0.105037 loss)
I0814 15:57:48.501411 28599 sgd_solver.cpp:106] Iteration 35850, lr = 0.0001
I0814 15:58:23.384090 28599 solver.cpp:229] Iteration 35900, loss = 0.150698
I0814 15:58:23.533000 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0934596
I0814 15:58:23.533052 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.184959
I0814 15:58:23.533069 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0753489
I0814 15:58:23.533079 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0753489 (* 1 = 0.0753489 loss)
I0814 15:58:23.533087 28599 sgd_solver.cpp:106] Iteration 35900, lr = 0.0001
I0814 15:58:56.142964 28599 solver.cpp:229] Iteration 35950, loss = 0.165323
I0814 15:58:56.291502 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.104173
I0814 15:58:56.291560 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.195265
I0814 15:58:56.291580 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0826617
I0814 15:58:56.291596 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0826617 (* 1 = 0.0826617 loss)
I0814 15:58:56.291606 28599 sgd_solver.cpp:106] Iteration 35950, lr = 0.0001
I0814 15:59:29.630152 28599 solver.cpp:229] Iteration 36000, loss = 0.142301
I0814 15:59:29.781940 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0871551
I0814 15:59:29.781999 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.166473
I0814 15:59:29.782016 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0711508
I0814 15:59:29.782027 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0711508 (* 1 = 0.0711508 loss)
I0814 15:59:29.782035 28599 sgd_solver.cpp:106] Iteration 36000, lr = 0.0001
I0814 16:00:02.150640 28599 solver.cpp:229] Iteration 36050, loss = 0.103663
I0814 16:00:02.299727 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0559523
I0814 16:00:02.299782 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.105607
I0814 16:00:02.299799 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0518315
I0814 16:00:02.299809 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0518315 (* 1 = 0.0518315 loss)
I0814 16:00:02.299818 28599 sgd_solver.cpp:106] Iteration 36050, lr = 0.0001
I0814 16:00:35.256108 28599 solver.cpp:229] Iteration 36100, loss = 0.155124
I0814 16:00:35.405381 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0955809
I0814 16:00:35.405448 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.185963
I0814 16:00:35.405472 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0775623
I0814 16:00:35.405488 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0775623 (* 1 = 0.0775623 loss)
I0814 16:00:35.405499 28599 sgd_solver.cpp:106] Iteration 36100, lr = 0.0001
I0814 16:01:08.466940 28599 solver.cpp:229] Iteration 36150, loss = 0.155953
I0814 16:01:08.620728 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0869312
I0814 16:01:08.620792 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.169261
I0814 16:01:08.620813 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0779768
I0814 16:01:08.620827 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0779768 (* 1 = 0.0779768 loss)
I0814 16:01:08.620836 28599 sgd_solver.cpp:106] Iteration 36150, lr = 0.0001
I0814 16:01:42.389305 28599 solver.cpp:229] Iteration 36200, loss = 0.141422
I0814 16:01:42.540146 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0817053
I0814 16:01:42.540207 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.148725
I0814 16:01:42.540228 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0707113
I0814 16:01:42.540240 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0707113 (* 1 = 0.0707113 loss)
I0814 16:01:42.540248 28599 sgd_solver.cpp:106] Iteration 36200, lr = 0.0001
I0814 16:02:14.915138 28599 solver.cpp:229] Iteration 36250, loss = 0.118063
I0814 16:02:15.066913 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0747403
I0814 16:02:15.066972 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.137559
I0814 16:02:15.066987 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0590316
I0814 16:02:15.066995 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0590316 (* 1 = 0.0590316 loss)
I0814 16:02:15.067004 28599 sgd_solver.cpp:106] Iteration 36250, lr = 0.0001
I0814 16:02:47.798552 28599 solver.cpp:229] Iteration 36300, loss = 0.174797
I0814 16:02:47.953519 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.105858
I0814 16:02:47.953572 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.181317
I0814 16:02:47.953598 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0873987
I0814 16:02:47.953616 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0873987 (* 1 = 0.0873987 loss)
I0814 16:02:47.953625 28599 sgd_solver.cpp:106] Iteration 36300, lr = 0.0001
I0814 16:03:21.427284 28599 solver.cpp:229] Iteration 36350, loss = 0.0355966
I0814 16:03:21.578797 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0204946
I0814 16:03:21.578856 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0399924
I0814 16:03:21.578872 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0177983
I0814 16:03:21.578883 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0177983 (* 1 = 0.0177983 loss)
I0814 16:03:21.578892 28599 sgd_solver.cpp:106] Iteration 36350, lr = 0.0001
I0814 16:03:54.257766 28599 solver.cpp:229] Iteration 36400, loss = 0.130712
I0814 16:03:54.409503 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0709548
I0814 16:03:54.409560 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.138596
I0814 16:03:54.409579 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.065356
I0814 16:03:54.409590 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.065356 (* 1 = 0.065356 loss)
I0814 16:03:54.409597 28599 sgd_solver.cpp:106] Iteration 36400, lr = 0.0001
I0814 16:04:27.713747 28599 solver.cpp:229] Iteration 36450, loss = 0.101379
I0814 16:04:27.865265 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0651652
I0814 16:04:27.865327 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.122005
I0814 16:04:27.865348 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0506895
I0814 16:04:27.865361 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0506895 (* 1 = 0.0506895 loss)
I0814 16:04:27.865371 28599 sgd_solver.cpp:106] Iteration 36450, lr = 0.0001
I0814 16:05:00.472132 28599 solver.cpp:229] Iteration 36500, loss = 0.157049
I0814 16:05:00.624027 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0757053
I0814 16:05:00.624145 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.143942
I0814 16:05:00.624162 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0785245
I0814 16:05:00.624171 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0785245 (* 1 = 0.0785245 loss)
I0814 16:05:00.624183 28599 sgd_solver.cpp:106] Iteration 36500, lr = 0.0001
I0814 16:05:32.874265 28599 solver.cpp:229] Iteration 36550, loss = 0.173312
I0814 16:05:33.025115 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.113982
I0814 16:05:33.025168 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.212542
I0814 16:05:33.025183 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0866558
I0814 16:05:33.025194 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0866558 (* 1 = 0.0866558 loss)
I0814 16:05:33.025202 28599 sgd_solver.cpp:106] Iteration 36550, lr = 0.0001
I0814 16:06:06.667898 28599 solver.cpp:229] Iteration 36600, loss = 0.0625047
I0814 16:06:06.819363 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0398438
I0814 16:06:06.819425 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0763756
I0814 16:06:06.819445 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0312524
I0814 16:06:06.819489 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0312524 (* 1 = 0.0312524 loss)
I0814 16:06:06.819501 28599 sgd_solver.cpp:106] Iteration 36600, lr = 0.0001
I0814 16:06:39.809505 28599 solver.cpp:229] Iteration 36650, loss = 0.112853
I0814 16:06:39.960438 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0682127
I0814 16:06:39.960503 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.130639
I0814 16:06:39.960525 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0564264
I0814 16:06:39.960539 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0564264 (* 1 = 0.0564264 loss)
I0814 16:06:39.960548 28599 sgd_solver.cpp:106] Iteration 36650, lr = 0.0001
I0814 16:07:13.003360 28599 solver.cpp:229] Iteration 36700, loss = 0.102711
I0814 16:07:13.155184 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0657539
I0814 16:07:13.155241 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.124764
I0814 16:07:13.155259 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0513557
I0814 16:07:13.155273 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0513557 (* 1 = 0.0513557 loss)
I0814 16:07:13.155283 28599 sgd_solver.cpp:106] Iteration 36700, lr = 0.0001
I0814 16:07:46.178376 28599 solver.cpp:229] Iteration 36750, loss = 0.125072
I0814 16:07:46.329694 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0622607
I0814 16:07:46.329754 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.118162
I0814 16:07:46.329777 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0625362
I0814 16:07:46.329789 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0625362 (* 1 = 0.0625362 loss)
I0814 16:07:46.329800 28599 sgd_solver.cpp:106] Iteration 36750, lr = 0.0001
I0814 16:08:19.341464 28599 solver.cpp:229] Iteration 36800, loss = 0.177794
I0814 16:08:19.493497 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.109242
I0814 16:08:19.493556 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.207728
I0814 16:08:19.493573 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0888971
I0814 16:08:19.493588 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0888971 (* 1 = 0.0888971 loss)
I0814 16:08:19.493597 28599 sgd_solver.cpp:106] Iteration 36800, lr = 0.0001
I0814 16:08:52.636801 28599 solver.cpp:229] Iteration 36850, loss = 0.122357
I0814 16:08:52.787698 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.068052
I0814 16:08:52.787763 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129837
I0814 16:08:52.787788 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0611786
I0814 16:08:52.787804 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0611786 (* 1 = 0.0611786 loss)
I0814 16:08:52.787816 28599 sgd_solver.cpp:106] Iteration 36850, lr = 0.0001
I0814 16:09:26.074108 28599 solver.cpp:229] Iteration 36900, loss = 0.091065
I0814 16:09:26.226214 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0530029
I0814 16:09:26.226276 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.103649
I0814 16:09:26.226294 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0455326
I0814 16:09:26.226305 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0455326 (* 1 = 0.0455326 loss)
I0814 16:09:26.226315 28599 sgd_solver.cpp:106] Iteration 36900, lr = 0.0001
I0814 16:09:59.142513 28599 solver.cpp:229] Iteration 36950, loss = 0.119508
I0814 16:09:59.293277 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0699746
I0814 16:09:59.293354 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.13466
I0814 16:09:59.293378 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0597538
I0814 16:09:59.293393 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0597538 (* 1 = 0.0597538 loss)
I0814 16:09:59.293406 28599 sgd_solver.cpp:106] Iteration 36950, lr = 0.0001
I0814 16:10:32.064635 28599 solver.cpp:229] Iteration 37000, loss = 0.141106
I0814 16:10:32.233054 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0778584
I0814 16:10:32.233120 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.142584
I0814 16:10:32.233144 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0705529
I0814 16:10:32.233158 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0705529 (* 1 = 0.0705529 loss)
I0814 16:10:32.233170 28599 sgd_solver.cpp:106] Iteration 37000, lr = 0.0001
I0814 16:11:05.271077 28599 solver.cpp:229] Iteration 37050, loss = 0.0772315
I0814 16:11:05.424075 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.037083
I0814 16:11:05.424144 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0702641
I0814 16:11:05.424167 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0386158
I0814 16:11:05.424183 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0386158 (* 1 = 0.0386158 loss)
I0814 16:11:05.424193 28599 sgd_solver.cpp:106] Iteration 37050, lr = 0.0001
I0814 16:11:38.902792 28599 solver.cpp:229] Iteration 37100, loss = 0.159
I0814 16:11:39.053748 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0999244
I0814 16:11:39.053805 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.190448
I0814 16:11:39.053827 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0795002
I0814 16:11:39.053839 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0795002 (* 1 = 0.0795002 loss)
I0814 16:11:39.053848 28599 sgd_solver.cpp:106] Iteration 37100, lr = 0.0001
I0814 16:12:12.257699 28599 solver.cpp:229] Iteration 37150, loss = 0.356988
I0814 16:12:12.412760 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.213793
I0814 16:12:12.412827 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.400044
I0814 16:12:12.412850 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.178494
I0814 16:12:12.412865 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.178494 (* 1 = 0.178494 loss)
I0814 16:12:12.412875 28599 sgd_solver.cpp:106] Iteration 37150, lr = 0.0001
I0814 16:12:45.450841 28599 solver.cpp:229] Iteration 37200, loss = 0.215142
I0814 16:12:45.601639 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.135443
I0814 16:12:45.601709 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.25831
I0814 16:12:45.601733 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.107571
I0814 16:12:45.601752 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.107571 (* 1 = 0.107571 loss)
I0814 16:12:45.601764 28599 sgd_solver.cpp:106] Iteration 37200, lr = 0.0001
I0814 16:13:18.589584 28599 solver.cpp:229] Iteration 37250, loss = 0.0752513
I0814 16:13:18.741127 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0473139
I0814 16:13:18.741179 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0905879
I0814 16:13:18.741197 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0376257
I0814 16:13:18.741207 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0376257 (* 1 = 0.0376257 loss)
I0814 16:13:18.741215 28599 sgd_solver.cpp:106] Iteration 37250, lr = 0.0001
I0814 16:13:51.706413 28599 solver.cpp:229] Iteration 37300, loss = 0.131538
I0814 16:13:51.857904 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0750587
I0814 16:13:51.857960 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.140749
I0814 16:13:51.857975 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.065769
I0814 16:13:51.857983 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.065769 (* 1 = 0.065769 loss)
I0814 16:13:51.857990 28599 sgd_solver.cpp:106] Iteration 37300, lr = 0.0001
I0814 16:14:24.736543 28599 solver.cpp:229] Iteration 37350, loss = 0.0987174
I0814 16:14:24.892293 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0597457
I0814 16:14:24.892360 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.117178
I0814 16:14:24.892380 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0493587
I0814 16:14:24.892395 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0493587 (* 1 = 0.0493587 loss)
I0814 16:14:24.892403 28599 sgd_solver.cpp:106] Iteration 37350, lr = 0.0001
I0814 16:14:58.408339 28599 solver.cpp:229] Iteration 37400, loss = 0.176838
I0814 16:14:58.559774 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0962153
I0814 16:14:58.559845 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.182914
I0814 16:14:58.559875 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0884191
I0814 16:14:58.559891 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0884191 (* 1 = 0.0884191 loss)
I0814 16:14:58.559903 28599 sgd_solver.cpp:106] Iteration 37400, lr = 0.0001
I0814 16:15:31.791455 28599 solver.cpp:229] Iteration 37450, loss = 0.144319
I0814 16:15:31.944841 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0921467
I0814 16:15:31.944908 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.173293
I0814 16:15:31.944927 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0721596
I0814 16:15:31.944942 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0721596 (* 1 = 0.0721596 loss)
I0814 16:15:31.944952 28599 sgd_solver.cpp:106] Iteration 37450, lr = 0.0001
I0814 16:16:05.753968 28599 solver.cpp:229] Iteration 37500, loss = 0.116859
I0814 16:16:05.905094 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0707271
I0814 16:16:05.905161 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.129554
I0814 16:16:05.905182 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0584295
I0814 16:16:05.905202 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0584295 (* 1 = 0.0584295 loss)
I0814 16:16:05.905213 28599 sgd_solver.cpp:106] Iteration 37500, lr = 0.0001
I0814 16:16:39.122133 28599 solver.cpp:229] Iteration 37550, loss = 0.277053
I0814 16:16:39.276630 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.161316
I0814 16:16:39.276695 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.313249
I0814 16:16:39.276715 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.138527
I0814 16:16:39.276731 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.138527 (* 1 = 0.138527 loss)
I0814 16:16:39.276741 28599 sgd_solver.cpp:106] Iteration 37550, lr = 0.0001
I0814 16:17:12.721907 28599 solver.cpp:229] Iteration 37600, loss = 0.23078
I0814 16:17:12.873075 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.141285
I0814 16:17:12.873138 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.2701
I0814 16:17:12.873162 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.11539
I0814 16:17:12.873175 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.11539 (* 1 = 0.11539 loss)
I0814 16:17:12.873186 28599 sgd_solver.cpp:106] Iteration 37600, lr = 0.0001
I0814 16:17:46.541584 28599 solver.cpp:229] Iteration 37650, loss = 0.0919321
I0814 16:17:46.692420 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.053047
I0814 16:17:46.692487 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.103258
I0814 16:17:46.692507 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0459661
I0814 16:17:46.692520 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0459661 (* 1 = 0.0459661 loss)
I0814 16:17:46.692529 28599 sgd_solver.cpp:106] Iteration 37650, lr = 0.0001
I0814 16:18:20.074074 28599 solver.cpp:229] Iteration 37700, loss = 0.17952
I0814 16:18:20.225055 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.114241
I0814 16:18:20.225203 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.221253
I0814 16:18:20.225325 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0897598
I0814 16:18:20.225361 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0897598 (* 1 = 0.0897598 loss)
I0814 16:18:20.225384 28599 sgd_solver.cpp:106] Iteration 37700, lr = 0.0001
I0814 16:18:53.990483 28599 solver.cpp:229] Iteration 37750, loss = 0.0982537
I0814 16:18:54.141099 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0629957
I0814 16:18:54.141156 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.120041
I0814 16:18:54.141176 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0491269
I0814 16:18:54.141191 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0491269 (* 1 = 0.0491269 loss)
I0814 16:18:54.141199 28599 sgd_solver.cpp:106] Iteration 37750, lr = 0.0001
I0814 16:19:28.054090 28599 solver.cpp:229] Iteration 37800, loss = 0.206314
I0814 16:19:28.202137 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.131611
I0814 16:19:28.202194 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.255159
I0814 16:19:28.202219 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.103157
I0814 16:19:28.202265 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.103157 (* 1 = 0.103157 loss)
I0814 16:19:28.202277 28599 sgd_solver.cpp:106] Iteration 37800, lr = 0.0001
I0814 16:20:01.568519 28599 solver.cpp:229] Iteration 37850, loss = 0.132022
I0814 16:20:01.717032 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0850656
I0814 16:20:01.717099 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.157622
I0814 16:20:01.717120 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0660113
I0814 16:20:01.717136 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0660113 (* 1 = 0.0660113 loss)
I0814 16:20:01.717147 28599 sgd_solver.cpp:106] Iteration 37850, lr = 0.0001
I0814 16:20:35.060288 28599 solver.cpp:229] Iteration 37900, loss = 0.192969
I0814 16:20:35.208561 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.124419
I0814 16:20:35.208624 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.236614
I0814 16:20:35.208644 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0964845
I0814 16:20:35.208659 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0964845 (* 1 = 0.0964845 loss)
I0814 16:20:35.208668 28599 sgd_solver.cpp:106] Iteration 37900, lr = 0.0001
I0814 16:21:08.264458 28599 solver.cpp:229] Iteration 37950, loss = 0.161049
I0814 16:21:08.415284 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.095451
I0814 16:21:08.415355 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.185565
I0814 16:21:08.415375 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0805247
I0814 16:21:08.415386 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0805247 (* 1 = 0.0805247 loss)
I0814 16:21:08.415396 28599 sgd_solver.cpp:106] Iteration 37950, lr = 0.0001
I0814 16:21:41.993506 28599 solver.cpp:229] Iteration 38000, loss = 0.0739019
I0814 16:21:42.144300 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.040709
I0814 16:21:42.144361 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.0793216
I0814 16:21:42.144381 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.036951
I0814 16:21:42.144394 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.036951 (* 1 = 0.036951 loss)
I0814 16:21:42.144404 28599 sgd_solver.cpp:106] Iteration 38000, lr = 0.0001
I0814 16:22:15.639730 28599 solver.cpp:229] Iteration 38050, loss = 0.121312
I0814 16:22:15.807927 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0735004
I0814 16:22:15.807986 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.138373
I0814 16:22:15.808043 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.060656
I0814 16:22:15.808061 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.060656 (* 1 = 0.060656 loss)
I0814 16:22:15.808071 28599 sgd_solver.cpp:106] Iteration 38050, lr = 0.0001
I0814 16:22:49.656150 28599 solver.cpp:229] Iteration 38100, loss = 0.12497
I0814 16:22:49.821580 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0779197
I0814 16:22:49.821647 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.14553
I0814 16:22:49.821671 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0624851
I0814 16:22:49.821686 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0624851 (* 1 = 0.0624851 loss)
I0814 16:22:49.821696 28599 sgd_solver.cpp:106] Iteration 38100, lr = 0.0001
I0814 16:23:23.830054 28599 solver.cpp:229] Iteration 38150, loss = 0.13471
I0814 16:23:23.993820 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0781038
I0814 16:23:23.993927 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.152997
I0814 16:23:23.993952 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0673549
I0814 16:23:23.993965 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0673549 (* 1 = 0.0673549 loss)
I0814 16:23:23.993976 28599 sgd_solver.cpp:106] Iteration 38150, lr = 0.0001
I0814 16:23:57.513092 28599 solver.cpp:229] Iteration 38200, loss = 0.241492
I0814 16:23:57.665616 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.137234
I0814 16:23:57.665685 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.264014
I0814 16:23:57.665710 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.120746
I0814 16:23:57.665726 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.120746 (* 1 = 0.120746 loss)
I0814 16:23:57.665736 28599 sgd_solver.cpp:106] Iteration 38200, lr = 0.0001
I0814 16:24:31.061276 28599 solver.cpp:229] Iteration 38250, loss = 0.126478
I0814 16:24:31.217023 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.079657
I0814 16:24:31.217084 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.150166
I0814 16:24:31.217106 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0632392
I0814 16:24:31.217120 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0632392 (* 1 = 0.0632392 loss)
I0814 16:24:31.217130 28599 sgd_solver.cpp:106] Iteration 38250, lr = 0.0001
I0814 16:25:04.654700 28599 solver.cpp:229] Iteration 38300, loss = 0.227108
I0814 16:25:04.817454 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.139285
I0814 16:25:04.817523 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.262196
I0814 16:25:04.817543 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.113554
I0814 16:25:04.817558 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.113554 (* 1 = 0.113554 loss)
I0814 16:25:04.817569 28599 sgd_solver.cpp:106] Iteration 38300, lr = 0.0001
I0814 16:25:38.728992 28599 solver.cpp:229] Iteration 38350, loss = 0.152687
I0814 16:25:38.881526 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0884316
I0814 16:25:38.881592 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.166545
I0814 16:25:38.881654 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0763437
I0814 16:25:38.881673 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0763437 (* 1 = 0.0763437 loss)
I0814 16:25:38.881685 28599 sgd_solver.cpp:106] Iteration 38350, lr = 0.0001
I0814 16:26:12.400609 28599 solver.cpp:229] Iteration 38400, loss = 0.0888514
I0814 16:26:12.551558 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.0573131
I0814 16:26:12.551625 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.10848
I0814 16:26:12.551645 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0444258
I0814 16:26:12.551658 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0444258 (* 1 = 0.0444258 loss)
I0814 16:26:12.551668 28599 sgd_solver.cpp:106] Iteration 38400, lr = 0.0001
I0814 16:26:46.119235 28599 solver.cpp:229] Iteration 38450, loss = 0.164107
I0814 16:26:46.271715 28599 solver.cpp:245]     Train net output #0: down_up_half_loss_real = 0.104958
I0814 16:26:46.271783 28599 solver.cpp:245]     Train net output #1: down_up_loss_real = 0.19504
I0814 16:26:46.271811 28599 solver.cpp:245]     Train net output #2: zoom_disp_loss0_real = 0.0820534
I0814 16:26:46.271826 28599 solver.cpp:245]     Train net output #3: zoom_disp_loss0_zoom_disp_loss0_0_split_0 = 0.0820534 (* 1 = 0.0820534 loss)
I0814 16:26:46.271836 28599 sgd_solver.cpp:106] Iteration 38450, lr = 0.0001
I0814 16:27:19.418604 28599 solver.cpp:456] Snapshotting to binary proto file norm_-0.01_iter_38500.caffemodel
I0814 16:27:19.457370 28599 sgd_solver.cpp:273] Snapshotting solver state to binary proto file norm_-0.01_iter_38500.solverstate
I0814 16:27:19.486421 28599 solver.cpp:338] Iteration 38500, Testing net (#0)
I0814 16:27:19.486451 28599 net.cpp:694] Ignoring source layer CustomData1
I0814 16:27:19.486456 28599 net.cpp:694] Ignoring source layer blob0_CustomData1_0_split
I0814 16:27:19.486460 28599 net.cpp:694] Ignoring source layer blob1_CustomData1_1_split
I0814 16:27:19.486464 28599 net.cpp:694] Ignoring source layer blob2_CustomData1_2_split
I0814 16:27:19.486470 28599 net.cpp:694] Ignoring source layer DummyData1
I0814 16:27:19.486474 28599 net.cpp:694] Ignoring source layer blob9_DummyData1_0_split
